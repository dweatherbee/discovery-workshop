<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"><title>Complete 4-Day Course</title><link rel="icon" type="image/x-icon" href="favicon.ico"><link rel="stylesheet" href="assets/reveal.js/dist/reset.css"><link rel="stylesheet" href="assets/reveal.js/dist/reveal.css"><link rel="stylesheet" href="assets/theme/cockroachlabs-light.css" id="theme"><!--This CSS is generated by the Asciidoctor reveal.js converter to further integrate AsciiDoc's existing semantic with reveal.js--><style type="text/css">.reveal div.right {
  float: right
}

/* source blocks */
.reveal .listingblock.stretch > .content {
  height: 100%
}

.reveal .listingblock.stretch > .content > pre {
  height: 100%
}

.reveal .listingblock.stretch > .content > pre > code {
  height: 100%;
  max-height: 100%
}

/* auto-animate feature */
/* hide the scrollbar when auto-animating source blocks */
.reveal pre[data-auto-animate-target] {
  overflow: hidden;
}

.reveal pre[data-auto-animate-target] code {
  overflow: hidden;
}

/* add a min width to avoid horizontal shift on line numbers */
code.hljs .hljs-ln-line.hljs-ln-n {
  min-width: 1.25em;
}

/* tables */
table {
  border-collapse: collapse;
  border-spacing: 0
}

table {
  margin-bottom: 1.25em;
  border: solid 1px #dedede
}

table thead tr th, table thead tr td, table tfoot tr th, table tfoot tr td {
  padding: .5em .625em .625em;
  font-size: inherit;
  text-align: left
}

table tr th, table tr td {
  padding: .5625em .625em;
  font-size: inherit
}

table thead tr th, table tfoot tr th, table tbody tr td, table tr td, table tfoot tr td {
  display: table-cell;
  line-height: 1.6
}

td.tableblock > .content {
  margin-bottom: 1.25em
}

td.tableblock > .content > :last-child {
  margin-bottom: -1.25em
}

table.tableblock, th.tableblock, td.tableblock {
  border: 0 solid #dedede
}

table.grid-all > thead > tr > .tableblock, table.grid-all > tbody > tr > .tableblock {
  border-width: 0 1px 1px 0
}

table.grid-all > tfoot > tr > .tableblock {
  border-width: 1px 1px 0 0
}

table.grid-cols > * > tr > .tableblock {
  border-width: 0 1px 0 0
}

table.grid-rows > thead > tr > .tableblock, table.grid-rows > tbody > tr > .tableblock {
  border-width: 0 0 1px
}

table.grid-rows > tfoot > tr > .tableblock {
  border-width: 1px 0 0
}

table.grid-all > * > tr > .tableblock:last-child, table.grid-cols > * > tr > .tableblock:last-child {
  border-right-width: 0
}

table.grid-all > tbody > tr:last-child > .tableblock, table.grid-all > thead:last-child > tr > .tableblock, table.grid-rows > tbody > tr:last-child > .tableblock, table.grid-rows > thead:last-child > tr > .tableblock {
  border-bottom-width: 0
}

table.frame-all {
  border-width: 1px
}

table.frame-sides {
  border-width: 0 1px
}

table.frame-topbot, table.frame-ends {
  border-width: 1px 0
}

.reveal table th.halign-left, .reveal table td.halign-left {
  text-align: left
}

.reveal table th.halign-right, .reveal table td.halign-right {
  text-align: right
}

.reveal table th.halign-center, .reveal table td.halign-center {
  text-align: center
}

.reveal table th.valign-top, .reveal table td.valign-top {
  vertical-align: top
}

.reveal table th.valign-bottom, .reveal table td.valign-bottom {
  vertical-align: bottom
}

.reveal table th.valign-middle, .reveal table td.valign-middle {
  vertical-align: middle
}

table thead th, table tfoot th {
  font-weight: bold
}

tbody tr th {
  display: table-cell;
  line-height: 1.6
}

tbody tr th, tbody tr th p, tfoot tr th, tfoot tr th p {
  font-weight: bold
}

thead {
  display: table-header-group
}

.reveal table.grid-none th, .reveal table.grid-none td {
  border-bottom: 0 !important
}

/* kbd macro */
kbd {
  font-family: "Droid Sans Mono", "DejaVu Sans Mono", monospace;
  display: inline-block;
  color: rgba(0, 0, 0, .8);
  font-size: .65em;
  line-height: 1.45;
  background: #f7f7f7;
  border: 1px solid #ccc;
  -webkit-border-radius: 3px;
  border-radius: 3px;
  -webkit-box-shadow: 0 1px 0 rgba(0, 0, 0, .2), 0 0 0 .1em white inset;
  box-shadow: 0 1px 0 rgba(0, 0, 0, .2), 0 0 0 .1em #fff inset;
  margin: 0 .15em;
  padding: .2em .5em;
  vertical-align: middle;
  position: relative;
  top: -.1em;
  white-space: nowrap
}

.keyseq kbd:first-child {
  margin-left: 0
}

.keyseq kbd:last-child {
  margin-right: 0
}

/* callouts */
.conum[data-value] {
  display: inline-block;
  color: #fff !important;
  background: rgba(0, 0, 0, .8);
  -webkit-border-radius: 50%;
  border-radius: 50%;
  text-align: center;
  font-size: .75em;
  width: 1.67em;
  height: 1.67em;
  line-height: 1.67em;
  font-family: "Open Sans", "DejaVu Sans", sans-serif;
  font-style: normal;
  font-weight: bold
}

.conum[data-value] * {
  color: #fff !important
}

.conum[data-value] + b {
  display: none
}

.conum[data-value]:after {
  content: attr(data-value)
}

pre .conum[data-value] {
  position: relative;
  top: -.125em
}

b.conum * {
  color: inherit !important
}

.conum:not([data-value]):empty {
  display: none
}

/* Callout list */
.hdlist > table, .colist > table {
  border: 0;
  background: none
}

.hdlist > table > tbody > tr, .colist > table > tbody > tr {
  background: none
}

td.hdlist1, td.hdlist2 {
  vertical-align: top;
  padding: 0 .625em
}

td.hdlist1 {
  font-weight: bold;
  padding-bottom: 1.25em
}

/* Disabled from Asciidoctor CSS because it caused callout list to go under the
 * source listing when .stretch is applied (see #335)
 * .literalblock+.colist,.listingblock+.colist{margin-top:-.5em} */
.colist td:not([class]):first-child {
  padding: .4em .75em 0;
  line-height: 1;
  vertical-align: top
}

.colist td:not([class]):first-child img {
  max-width: none
}

.colist td:not([class]):last-child {
  padding: .25em 0
}

/* Override Asciidoctor CSS that causes issues with reveal.js features */
.reveal .hljs table {
  border: 0
}

/* Callout list rows would have a bottom border with some reveal.js themes (see #335) */
.reveal .colist > table th, .reveal .colist > table td {
  border-bottom: 0
}

/* Fixes line height with Highlight.js source listing when linenums enabled (see #331) */
.reveal .hljs table thead tr th, .reveal .hljs table tfoot tr th, .reveal .hljs table tbody tr td, .reveal .hljs table tr td, .reveal .hljs table tfoot tr td {
  line-height: inherit
}

/* Columns layout */
.columns .slide-content {
  display: flex;
}

.columns.wrap .slide-content {
  flex-wrap: wrap;
}

.columns.is-vcentered .slide-content {
  align-items: center;
}

.columns .slide-content > .column {
  display: block;
  flex-basis: 0;
  flex-grow: 1;
  flex-shrink: 1;
}

.columns .slide-content > .column > * {
  padding: .75rem;
}

/* See #353 */
.columns.wrap .slide-content > .column {
  flex-basis: auto;
}

.columns .slide-content > .column.is-full {
  flex: none;
  width: 100%;
}

.columns .slide-content > .column.is-four-fifths {
  flex: none;
  width: 80%;
}

.columns .slide-content > .column.is-three-quarters {
  flex: none;
  width: 75%;
}

.columns .slide-content > .column.is-two-thirds {
  flex: none;
  width: 66.6666%;
}

.columns .slide-content > .column.is-three-fifths {
  flex: none;
  width: 60%;
}

.columns .slide-content > .column.is-half {
  flex: none;
  width: 50%;
}

.columns .slide-content > .column.is-two-fifths {
  flex: none;
  width: 40%;
}

.columns .slide-content > .column.is-one-third {
  flex: none;
  width: 33.3333%;
}

.columns .slide-content > .column.is-one-quarter {
  flex: none;
  width: 25%;
}

.columns .slide-content > .column.is-one-fifth {
  flex: none;
  width: 20%;
}

.columns .slide-content > .column.has-text-left {
  text-align: left;
}

.columns .slide-content > .column.has-text-justified {
  text-align: justify;
}

.columns .slide-content > .column.has-text-right {
  text-align: right;
}

.columns .slide-content > .column.has-text-left {
  text-align: left;
}

.columns .slide-content > .column.has-text-justified {
  text-align: justify;
}

.columns .slide-content > .column.has-text-right {
  text-align: right;
}

.text-left {
  text-align: left !important
}

.text-right {
  text-align: right !important
}

.text-center {
  text-align: center !important
}

.text-justify {
  text-align: justify !important
}

.footnotes {
  border-top: 1px solid rgba(0, 0, 0, 0.2);
  padding: 0.5em 0 0 0;
  font-size: 0.65em;
  margin-top: 4em;
}

.byline {
  font-size:.8em
}
ul.byline {
  list-style-type: none;
}
ul.byline li + li {
  margin-top: 0.25em;
}
</style><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/v4-shims.min.css">
        <a href="index.html" id="cockroachDBLogo" style="background: url(assets/images/App_icon.svg);
                            position: absolute;
                            background-repeat: no-repeat;
                            z-index: 1000;
                            bottom: 10px;
                            left: 10px;
                            width: 50px;
                            height: 60px;">
        </a></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Complete 4-Day Course</h1></section><section><section id="_workshop_kickoff_agenda_and_introduction"><h2>Workshop Kickoff, Agenda and Introduction</h2></section><section id="_workshop_overview"><h2>Workshop Overview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Four day workshop provides senior, mid-level and junior non-technical business professionals the skills to:</p><div class="ulist"><ul><li><p>Identify and contribute innovative ideas and drastically improve business processes</p></li><li><p>Analyze workflows and identify automation opportunities driven by Agentic AI</p></li><li><p>Deliver meaningful business value from Agentic AI intensified workflows</p></li></ul></div></li></ul></div></div></section><section id="_workshop_prerequisites"><h2>Workshop Prerequisites</h2><div class="slide-content"><div class="ulist"><ul><li><p>No coding or technical AI knowledge required</p></li><li><p>Basic computer literacy and familiarity with business applications</p></li><li><p>Understanding of your organization&#8217;s business processes and workflows</p></li><li><p>Ability to identify challenges and inefficiencies in current business operations</p></li><li><p>Open mindset toward innovation and process transformation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>This workshop has been specifically designed for business professionals without technical backgrounds. You do not need any coding experience or technical AI knowledge to participate effectively.</p></div>
<div class="paragraph"><p>Basic computer literacy is sufficient for this workshop. If you&#8217;re comfortable using business applications like Microsoft Office, collaboration tools, or enterprise systems relevant to your role, you&#8217;ll have the technical foundation needed to generally understand the Integrail AI Studio platform.</p></div>
<div class="paragraph"><p>What&#8217;s most valuable is your business knowledge—specifically your understanding of your organization&#8217;s processes and workflows. Your expertise in how work currently gets done in your department or function will be essential for identifying automation opportunities. This workshop will help you apply that knowledge to discover where AI agents can add the most value.</p></div>
<div class="paragraph"><p>The ability to identify challenges and inefficiencies in current operations is also important. Participants who can recognize bottlenecks, repetitive tasks, error-prone processes, or areas where employees spend significant time on low-value activities will gain the most from our sessions.</p></div>
<div class="paragraph"><p>Finally, an open mindset toward innovation and process transformation will enhance your experience. We&#8217;ll be exploring new approaches to work that may challenge existing assumptions about what tasks require human involvement. Your willingness to reimagine processes—not just automate them as they exist today—will be key to unlocking the full potential of agentic AI.</p></div>
<div class="paragraph"><p>Remember, this workshop focuses on the business application of AI technology, not its technical implementation. Our goal is to empower you to identify opportunities, design solutions, and plan implementations—all without requiring you to understand the underlying technical complexities.</p></div></aside></div></section><section id="_agenda_day_1"><h2>Agenda Day 1</h2><div class="slide-content"><div class="olist arabic text-left"><ol class="arabic"><li><p>Kickoff &amp; Introductions</p></li><li><p>What is AI?</p></li><li><p>What is Agentic AI?</p></li><li><p>Intro to Studio &amp; Build an Agent</p></li></ol></div></div></section><section id="_agenda_day_2"><h2>Agenda Day 2</h2><div class="slide-content"><div class="olist arabic text-left"><ol class="arabic"><li><p>How To ID Processes for Agentic AI</p></li><li><p>ID Departmental Challenges</p></li><li><p>Breakout Groups ID Processes Solving Dept Challenges</p></li></ol></div></div></section><section id="_agenda_day_3"><h2>Agenda Day 3</h2><div class="slide-content"><div class="olist arabic text-left"><ol class="arabic"><li><p>Mini Business Cases</p></li><li><p>Breakout Design Session I</p></li><li><p>Breakout Design Session II</p></li><li><p>Breakout Design Session III</p></li></ol></div></div></section><section id="_agenda_day_4"><h2>Agenda Day 4</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Full Day Break</p></li></ul></div></div></section><section id="_agenda_day_5"><h2>Agenda Day 5</h2><div class="slide-content"><div class="olist arabic text-left"><ol class="arabic"><li><p>Rank Business Cases and Awards</p></li><li><p>Way Forward &amp; Closing</p></li><li><p>1/2 Day Break</p></li></ol></div></div></section><section id="_logistics"><h2>Logistics</h2><div class="slide-content"><div class="ulist"><ul><li><p>9am - 5pm</p></li><li><p>Lunch</p></li><li><p>Morning and Afternoon Breaks</p></li><li><p>Facilities</p></li></ul></div></div></section><section id="_introductions"><h2>Introductions</h2><div class="slide-content"><div class="ulist"><ul><li><p>Integrail AI Instructor(s)</p></li><li><p>Your Name and Role</p></li><li><p>Do you have experience with AI?</p></li><li><p>Do you have experience with Agentic AI?</p></li><li><p>Do you meet the course prerequisites? (SQL/DDL and programming skills?)</p></li><li><p><em>What do you expect/hope to get from this workshop?</em>
== AI Fundamentals for Business Professionals</p></li></ul></div>
<div class="paragraph"><p>Understanding the Value of AI in Business</p></div></div></section><section id="_overview"><h2>Overview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Focus on AI fundamentals with emphasis on large language models (LLMs)</p></li><li><p>Provide conceptual understanding of LLMs without requiring programming knowledge</p><div class="ulist"><ul><li><p>LLMs are a foundational part of Agentic AI</p></li></ul></div></li><li><p>Prepare participants to identify Agentic AI opportunities in their business processes</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Welcome to our training module on AI Fundamentals for Business Professionals. This session is designed specifically for business professionals without technical backgrounds who want to understand how AI, particularly large language models, can create value in business contexts.</p></div>
<div class="paragraph"><p>We&#8217;ll explore the fundamentals of AI with a focus on practical business applications rather than technical implementation details. You don&#8217;t need any programming knowledge to benefit from this training - we&#8217;ll focus on concepts and applications rather than code.</p></div>
<div class="paragraph"><p>This module serves as the foundation for the remainder of our workshop, where you&#8217;ll identify business processes that could benefit from AI agent automation. The concepts we cover today will directly inform your ability to recognize valuable AI opportunities in your specific business context.</p></div>
<div class="paragraph"><p>By the end of this session, you&#8217;ll have a solid conceptual understanding of modern AI systems, particularly large language models, and be able to evaluate their potential applications in your work. This knowledge will prepare you for the more applied sessions in the coming days.</p></div></aside></div></section><section id="_learning_objectives"><h2>Learning Objectives</h2><div class="slide-content"><div class="ulist"><ul><li><p>Explain the evolution and key concepts of modern AI systems</p></li><li><p>Differentiate between types of LLMs and their capabilities</p></li><li><p>Evaluate potential business applications for AI technologies</p></li><li><p>Formulate effective prompts for different types of LLMs</p></li><li><p>Assess when to use different AI enhancement techniques like RAG and tools</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s review what we aim to accomplish in this session. By the end of this training, you&#8217;ll be able to:</p></div>
<div class="paragraph"><p>First, explain the evolution and key concepts of modern AI systems. You&#8217;ll understand how AI has developed over time and grasp the fundamental concepts that drive today&#8217;s AI technologies, particularly large language models.</p></div>
<div class="paragraph"><p>Second, differentiate between types of LLMs and their capabilities. You&#8217;ll learn about different approaches to language models, including probabilistic models like GPT-4o and reasoning-based models like Claude 3 Opus, and understand when each might be most appropriate for different business needs.</p></div>
<div class="paragraph"><p>Third, evaluate potential business applications for AI technologies. You&#8217;ll develop the ability to identify opportunities where AI can create value in your specific business context, focusing on practical applications rather than theoretical possibilities.</p></div>
<div class="paragraph"><p>Fourth, formulate effective prompts for different types of LLMs. You&#8217;ll learn the basics of prompt engineering - how to effectively communicate with AI systems to get the results you need for business tasks.</p></div>
<div class="paragraph"><p>Finally, assess when to use different AI enhancement techniques. You&#8217;ll understand approaches like Retrieval-Augmented Generation (RAG) and tools that extend AI capabilities, and be able to determine when these techniques might add value to your AI implementations.</p></div>
<div class="paragraph"><p>These objectives align with Bloom&#8217;s Taxonomy of learning, progressing from basic understanding to more complex application and evaluation skills. This structured approach ensures you&#8217;ll develop practical knowledge you can apply immediately in your business context.</p></div></aside></div></section></section>
<section><section id="_the_evolution_of_ai"><h2>The Evolution of AI</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>We&#8217;ll begin our exploration of AI fundamentals by looking at how artificial intelligence has evolved over time. Understanding this history provides important context for appreciating the capabilities and limitations of today&#8217;s AI systems, particularly large language models.</p></div>
<div class="paragraph"><p>In this module, we&#8217;ll trace the development of AI from early rule-based systems through the AI winters and into the current era of machine learning and deep learning. We&#8217;ll pay particular attention to the emergence of large language models, which represent a significant advancement in AI capabilities with broad business applications.</p></div>
<div class="paragraph"><p>This historical perspective will help you understand why today&#8217;s AI systems are fundamentally different from previous generations of technology and why they&#8217;re creating unprecedented opportunities for business transformation.</p></div></aside></div></section><section id="_brief_history_of_ai"><h2>Brief History of AI</h2><div class="slide-content"><div class="ulist"><ul><li><p>Early AI (1950s-1980s): Rule-based systems with explicit programming</p></li><li><p>AI Winters (Late 1970s, Late 1980s-Early 1990s): Periods of reduced funding and interest due to unmet expectations</p></li><li><p>Machine Learning Revolution (1990s-2000s): Systems that learn from data rather than explicit rules</p></li><li><p>Deep Learning Breakthrough (2012-2018): Neural networks with multiple layers enabling complex pattern recognition</p></li><li><p>Current Era (2019-Present): Foundation models trained on massive datasets with broad capabilities</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>IMAGE TODO - Timeline with Milestones: Create a horizontal timeline showing the progression of AI development with key milestones marked. Use different colors for each era and include small icons representing the dominant technology of each period (e.g., flowcharts for rule-based systems, neural network diagrams for deep learning).  This slide may need to be split into two.</p></div>
<div class="paragraph"><p>The history of artificial intelligence spans several decades, with distinct phases that have shaped today&#8217;s landscape:</p></div>
<div class="paragraph"><p>Early AI, from roughly the 1950s through the 1980s, focused primarily on rule-based systems that relied on explicit programming. These systems followed logical rules created by human experts to solve specific problems. Examples included expert systems for medical diagnosis or chess-playing programs that used predefined strategies. While impressive for their time, these systems were limited by their inability to handle exceptions or learn from experience.</p></div>
<div class="paragraph"><p>The field experienced several "AI Winters" - periods of reduced funding and interest when the technology failed to meet inflated expectations. These occurred notably in the late 1970s and again in the late 1980s/early 1990s. During these periods, many believed AI had fundamental limitations that would prevent it from achieving its promised potential.</p></div>
<div class="paragraph"><p>The Machine Learning Revolution marked a significant shift in approach. Rather than programming explicit rules, systems were designed to learn patterns from data. This approach proved more flexible and scalable, enabling AI to tackle problems that were too complex for rule-based approaches. Statistical methods and algorithms like support vector machines and random forests demonstrated impressive capabilities in specific domains.</p></div>
<div class="paragraph"><p>The Deep Learning Breakthrough, beginning around 2012, represented another fundamental advancement. Neural networks with multiple layers (hence "deep" learning) demonstrated unprecedented abilities in pattern recognition tasks. The key innovation was the ability to automatically learn hierarchical features from data, eliminating the need for human feature engineering. This approach dramatically improved performance in image recognition, speech processing, and eventually language understanding.</p></div>
<div class="paragraph"><p>The Current Era is characterized by foundation models - large AI systems trained on massive datasets that can be adapted to a wide range of tasks. These models, particularly large language models like GPT-4, Claude, and Gemini, demonstrate broad capabilities across domains without task-specific training. They represent a shift from specialized AI systems to general-purpose technologies with applications across virtually every business function.</p></div>
<div class="paragraph"><p>This evolution from narrow, rule-based systems to flexible, learning-based approaches has dramatically expanded the potential applications of AI in business contexts. Understanding this trajectory helps explain why today&#8217;s AI capabilities represent such a significant opportunity for business transformation.</p></div></aside></div></section><section id="_the_rise_of_large_language_models"><h2>The Rise of Large Language Models</h2><div class="slide-content"><div class="ulist"><ul><li><p>Word embeddings: Representing words as mathematical vectors</p></li><li><p>Transformer architecture (2017): Revolutionary approach enabling parallel processing of text</p></li><li><p>Key breakthroughs: BERT (2018), GPT series (2018-present), Claude, Gemini</p></li><li><p>Scaling laws (2020-2023): Performance improvements correlate with model size and training data</p></li><li><p>Emergent capabilities (2022-present): Advanced reasoning and problem-solving appearing at scale</p></li><li><p>Test Time Compute scaling (2023-present): Performance improvements through increased inference computation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>IMAGE TODO - Timeline with Milestones: may also use a timeline plus Split Into two.</p></div>
<div class="paragraph"><p>Large Language Models (LLMs) represent one of the most significant developments in artificial intelligence, with a trajectory of rapid advancement over the past decade:</p></div>
<div class="paragraph"><p>Word embeddings marked an important early step in natural language processing. These techniques, like Word2Vec (2013) and GloVe (2014), represented words as mathematical vectors in a way that captured semantic relationships. Words with similar meanings would be positioned close together in this mathematical space. This approach allowed algorithms to understand relationships between words, but still had limitations in understanding context.</p></div>
<div class="paragraph"><p>The Transformer architecture, introduced in the 2017 paper "Attention is All You Need," represented a revolutionary approach to processing text. Unlike previous sequential models, transformers could process all words in a text simultaneously, using a mechanism called "attention" to weigh the importance of different words in relation to each other. This parallel processing capability enabled much more efficient training on larger datasets.</p></div>
<div class="paragraph"><p>Key breakthroughs followed rapidly. BERT (Bidirectional Encoder Representations from Transformers), released by Google in 2018, demonstrated unprecedented performance on language understanding tasks. The GPT (Generative Pre-trained Transformer) series from OpenAI, beginning in 2018 and continuing through GPT-4 and beyond, showed increasingly impressive text generation capabilities. Other models like Claude from Anthropic and Gemini from Google have further advanced the field.</p></div>
<div class="paragraph"><p>Scaling laws have emerged as a crucial insight in LLM development. Researchers discovered that performance improvements correlate predictably with increases in model size (number of parameters) and training data volume. This finding led to a race to build ever-larger models, with sizes increasing from millions to billions and now trillions of parameters.</p></div>
<div class="paragraph"><p>Perhaps most surprisingly, emergent capabilities have appeared as models reached certain scale thresholds. Advanced reasoning, problem-solving, and even coding abilities weren&#8217;t explicitly programmed but emerged as models grew larger and were trained on more diverse data. These emergent capabilities have dramatically expanded the potential business applications of LLMs.</p></div>
<div class="paragraph"><p>Test Time Compute scaling represents one of the newest frontiers in LLM advancement. Research from 2023 onward has demonstrated that model performance can be significantly improved not just by increasing model size or training data, but by allocating more computational resources during inference (when the model is actually generating responses). Techniques like speculative decoding, tree-of-thought reasoning, and self-consistency sampling allow models to explore multiple reasoning paths or potential responses before selecting the best one. This approach effectively trades inference speed for quality, enabling even existing models to achieve better performance on complex reasoning tasks without retraining. For businesses, this means that model capabilities can continue to improve through algorithmic innovations even without building larger models, potentially offering more cost-effective paths to enhanced AI performance.</p></div>
<div class="paragraph"><p>The rapid evolution of LLMs has transformed them from academic curiosities to powerful business tools in just a few years. Understanding this trajectory helps explain their current capabilities and limitations, as well as their potential future development.</p></div></aside></div></section><section id="_current_ai_landscape_major_platforms"><h2>Current AI Landscape: Major Platforms</h2><div class="slide-content"><div class="ulist"><ul><li><p>Major commercial platforms: OpenAI (GPT-4o), Anthropic (Claude), Google (Gemini), Mistral, Meta (Llama)</p></li><li><p>Open-source alternatives: Llama 3 (Meta), Mistral Large, Falcon (TII), Deepseek, Mixtral 8x7B, BLOOM, Pythia, Stable LM</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Today&#8217;s AI landscape is characterized by rapid innovation, increasing accessibility, and a growing focus on business applications:</p></div>
<div class="paragraph"><p>Major commercial platforms have emerged as leaders in the development and deployment of large language models. OpenAI&#8217;s GPT series, particularly GPT-4o, offers state-of-the-art capabilities across text, image, and audio modalities. Anthropic&#8217;s Claude models emphasize safety and helpfulness. Google&#8217;s Gemini combines language capabilities with multimodal understanding. Newer entrants like Mistral AI and Meta&#8217;s Llama models are also gaining significant traction. These platforms typically offer API access, allowing businesses to integrate their capabilities without managing the underlying infrastructure.</p></div>
<div class="paragraph"><p>Open-source alternatives have created a parallel ecosystem of freely available models that can be downloaded, modified, and deployed by organizations with the technical resources to do so. Meta&#8217;s Llama 3 series (ranging from 8B to 70B parameters) has become one of the most widely adopted open-source models, offering performance competitive with many commercial options. Mistral AI has released several high-quality open models, including Mistral Large and the innovative Mixtral 8x7B which uses a mixture-of-experts architecture. The Technology Innovation Institute&#8217;s Falcon models (7B, 40B, and 180B versions) have shown impressive capabilities for their size. Deepseek&#8217;s models, particularly Deepseek Coder, excel at programming tasks. Other notable open-source models include BLOOM (a multilingual model developed by over 1,000 researchers), Pythia (a family of models designed for interpretability research), and Stable LM from Stability AI. This open-source movement has accelerated innovation and reduced costs, though these models often require more technical expertise to implement effectively.</p></div></aside></div></section><section id="_current_ai_landscape_trends"><h2>Current AI Landscape: Trends</h2><div class="slide-content"><div class="ulist"><ul><li><p>Enterprise AI integration: Increasing focus on business-specific implementations</p></li><li><p>Democratization of access: API-based services making AI capabilities widely available</p></li><li><p>Specialized vs. general-purpose systems: Trend toward adaptable foundation models</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Enterprise AI integration has become a major focus, with organizations moving beyond experimentation to implement AI capabilities in core business processes. This shift is driving demand for industry-specific models, enterprise-grade security and compliance features, and seamless integration with existing business systems. Companies like Microsoft, Salesforce, and IBM are positioning themselves as enablers of this enterprise AI transformation.</p></div>
<div class="paragraph"><p>The democratization of access represents another key trend. API-based services have made sophisticated AI capabilities available to organizations of all sizes without requiring specialized AI expertise. This accessibility has dramatically expanded the potential user base and use cases for AI technologies. No-code and low-code platforms are further reducing barriers to entry.</p></div>
<div class="paragraph"><p>The industry is seeing a shift from specialized to general-purpose systems. Rather than building custom AI models for each specific task, organizations are increasingly leveraging foundation models that can be adapted to a wide range of applications through techniques like fine-tuning and prompt engineering. This approach reduces development time and cost while maintaining high performance.</p></div>
<div class="paragraph"><p>Understanding this landscape is crucial for business professionals seeking to leverage AI effectively. The rapid pace of innovation means new capabilities are constantly emerging, while increasing accessibility makes implementation more feasible than ever before. This combination creates unprecedented opportunities for business transformation across virtually every industry and function.</p></div></aside></div></section></section>
<section><section id="_understanding_large_language_models"><h2>Understanding Large Language Models</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this module, we&#8217;ll develop a deeper understanding of Large Language Models (LLMs) - what they are, how they work at a high level, and the different approaches to their development. This understanding is essential for evaluating their potential applications in your business.</p></div>
<div class="paragraph"><p>We&#8217;ll explore the fundamental capabilities of LLMs and how they process and generate language. We&#8217;ll then examine different types of LLMs, particularly the distinction between probabilistic models like GPT-4o and reasoning-based models like Claude 3 Opus or GPT-4o with specific prompting techniques.</p></div>
<div class="paragraph"><p>By understanding these different approaches and their respective strengths and limitations, you&#8217;ll be better equipped to determine which type of LLM might be most appropriate for different business applications. This knowledge will directly inform your ability to identify and evaluate AI opportunities in your organization.</p></div></aside></div></section><section id="_what_are_llms_core_mechanics"><h2>What are LLMs? Core Mechanics</h2><div class="slide-content"><div class="ulist"><ul><li><p>AI systems trained on vast text datasets to understand and generate human language</p></li><li><p>Process information by breaking text into tokens (word parts) and analyzing patterns</p></li><li><p>Predict likely next words/tokens based on patterns learned during training</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Large Language Models (LLMs) are a type of artificial intelligence system specifically designed to understand and generate human language. Let&#8217;s explore their fundamental characteristics:</p></div>
<div class="paragraph"><p>LLMs are trained on vast text datasets, often comprising hundreds of billions of words from sources like books, articles, websites, code repositories, and other text-based content. This extensive training allows them to learn the patterns, structures, and relationships in human language across diverse topics and domains. The largest models have effectively "read" more text than any human could in multiple lifetimes.</p></div>
<div class="paragraph"><p>These models process information by breaking text into tokens, which are essentially word parts or complete words. For example, the word "understanding" might be broken into tokens like "under" and "standing." The model analyzes patterns in how these tokens appear in relation to each other across its training data. This tokenization approach allows the model to handle words it hasn&#8217;t explicitly seen before by recognizing their component parts.</p></div>
<div class="paragraph"><p>At their core, LLMs predict likely next words or tokens based on the patterns they&#8217;ve learned. When given a prompt or partial text, they calculate probabilities for what might come next based on similar patterns in their training data. This predictive capability is what enables them to generate coherent and contextually appropriate text that continues from any starting point.</p></div></aside></div></section><section id="_understanding_tokenization"><h2>Understanding Tokenization</h2><div class="slide-content"><div class="ulist"><ul><li><p>Tokens are the basic units LLMs process - can be words, parts of words, or punctuation</p></li><li><p>Examples:</p><div class="ulist"><ul><li><p>"Artificial" → "Art" + "ificial"</p></li><li><p>"intelligence" → "intel" + "ligence"</p></li><li><p>"doesn&#8217;t" → "doesn" + "'t"</p></li></ul></div></li><li><p>Most models use 1,000-100,000 unique tokens in their vocabulary</p></li><li><p>Efficient compression of language into machine-readable units</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Tokenization is a fundamental process that converts human text into a format that LLMs can process. Understanding how tokenization works helps explain both the capabilities and limitations of these models:</p></div>
<div class="paragraph"><p>Tokens represent the basic units that LLMs process. Unlike traditional NLP systems that might work with whole words, LLMs break text down into subword units. These tokens can be complete words, parts of words, or even individual characters and punctuation marks. This approach allows the model to handle a virtually unlimited vocabulary by combining token pieces.</p></div>
<div class="paragraph"><p>The tokenization process follows specific patterns based on the frequency of character combinations in the training data. Common words like "the" or "and" typically get their own tokens, while less common words are split into multiple tokens. For example, "tokenization" might be broken into "token" + "ization" because these parts appear frequently in other words.</p></div>
<div class="paragraph"><p>Different LLM systems use different tokenization approaches. GPT models use a method called Byte-Pair Encoding (BPE), while some other models use WordPiece or SentencePiece tokenizers. Regardless of the specific method, all modern LLMs use some form of subword tokenization.</p></div>
<div class="paragraph"><p>The size of a model&#8217;s token vocabulary typically ranges from about 1,000 to 100,000 unique tokens. This vocabulary represents the building blocks the model uses to understand and generate all text. The specific tokens in this vocabulary are determined during the pre-training process based on the frequency of character patterns in the training data.</p></div>
<div class="paragraph"><p>Tokenization has important practical implications. When using LLMs, inputs are counted in tokens, not words or characters. This affects usage costs for commercial APIs and context window limitations. As a rule of thumb, one word typically corresponds to about 1.3-1.5 tokens in English, though this varies widely depending on the specific text.</p></div>
<div class="paragraph"><p>Understanding tokenization helps explain why LLMs sometimes struggle with very rare words, made-up terms, or specialized technical vocabulary. If a word must be broken into many small token pieces, the model may have difficulty maintaining coherence across those pieces during processing.</p></div></aside></div></section><section id="_prediction_mechanism_probability_distribution"><h2>Prediction Mechanism: Probability Distribution</h2><div class="slide-content"><div class="ulist"><ul><li><p>LLMs function as next-token prediction engines</p></li><li><p>For input: "The capital of France is&#8230;&#8203;"</p></li><li><p>Model calculates probability distribution across entire vocabulary:</p><div class="ulist"><ul><li><p>"Paris": 92%</p></li><li><p>"Lyon": 2%</p></li><li><p>"located": 1%</p></li><li><p>[thousands of other possibilities with lower probabilities]</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>At their core, LLMs operate through a surprisingly simple mechanism: they predict the next token in a sequence based on all the tokens that came before it. This fundamental prediction capability is what enables all their more complex behaviors:</p></div>
<div class="paragraph"><p>When an LLM receives input text, it processes this text token by token, building an internal representation of the context. This representation captures patterns, relationships, and semantic meanings from the input.</p></div>
<div class="paragraph"><p>For each position in the sequence, the model calculates a probability distribution across its entire vocabulary of tokens. This distribution represents the model&#8217;s prediction of how likely each possible token is to appear next in the sequence. For example, given the prompt "The capital of France is," the model might assign a 92% probability to "Paris," a 2% probability to "Lyon," a 1% probability to "located," and distribute the remaining 5% across thousands of other tokens.</p></div>
<div class="paragraph"><p>The model then selects a token from this probability distribution. In the simplest case, it selects the highest probability token (a process called "greedy decoding"). However, most implementations use more sophisticated sampling methods that introduce controlled randomness to generate more diverse and interesting outputs.</p></div></aside></div></section><section id="_prediction_mechanism_temperature_generation"><h2>Prediction Mechanism: Temperature &amp; Generation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Temperature setting controls randomness in token selection:</p><div class="ulist"><ul><li><p>Low temp (0.1-0.5): More predictable, focused outputs</p></li><li><p>Medium temp (0.6-0.8): Balanced creativity &amp; accuracy</p></li><li><p>High temp (0.9-1.0+): More creative, diverse outputs</p></li></ul></div></li><li><p>Each selected token becomes part of context for next prediction</p><div class="ulist"><ul><li><p>This iterative process continues until completion</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The "temperature" setting that many LLM interfaces provide controls the randomness in the token selection process. This parameter fundamentally alters how the model generates text:</p></div>
<div class="paragraph"><p>At low temperatures (typically 0.1-0.5), the model strongly favors high-probability tokens. This results in more predictable, focused, and often more factually accurate outputs. Low temperatures are ideal for tasks requiring precision, such as answering factual questions or generating code.</p></div>
<div class="paragraph"><p>At medium temperatures (around 0.6-0.8), the model strikes a balance between selecting high-probability tokens and occasionally choosing less likely options. This creates outputs with a good balance of coherence and creativity, making it suitable for many general-purpose applications.</p></div>
<div class="paragraph"><p>At high temperatures (0.9 and above), the model is much more likely to select lower-probability tokens. This produces more diverse, creative, and sometimes surprising outputs, but with increased risk of incoherence or factual errors. High temperatures work well for creative writing, brainstorming, or generating varied alternatives.</p></div>
<div class="paragraph"><p>Once a token is selected, it&#8217;s added to the sequence, and the process repeats. The model now calculates a new probability distribution for the next position, taking into account the newly added token. This iterative process continues until the model generates a stopping token or reaches a predefined length limit.</p></div>
<div class="paragraph"><p>What&#8217;s remarkable is that this relatively simple prediction mechanism, when scaled up with billions of parameters and trained on vast datasets, enables the complex capabilities we observe in modern LLMs. The model isn&#8217;t explicitly programmed to answer questions, write essays, or solve problems—it&#8217;s simply predicting what tokens are likely to come next in a given context. The emergent behaviors we value arise from this fundamental prediction capability.</p></div></aside></div></section><section id="_llm_capabilities_and_applications"><h2>LLM Capabilities and Applications</h2><div class="slide-content"><div class="ulist"><ul><li><p>LLMs demonstrate capabilities in writing, summarizing, answering questions, and reasoning</p></li><li><p>Represent a general-purpose technology with applications across business functions</p></li><li><p>All complex behaviors emerge from the simple next-token prediction mechanism</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Modern LLMs demonstrate remarkable capabilities across a range of language tasks. They can write essays, stories, or business documents; summarize lengthy content; answer questions based on their training data; translate between languages; and even perform reasoning tasks that require multiple steps of logical thinking. These capabilities emerge from their statistical understanding of language patterns rather than explicit programming.</p></div>
<div class="paragraph"><p>From a business perspective, LLMs represent a general-purpose technology with applications across virtually every function and industry. Unlike previous AI systems that were designed for specific narrow tasks, LLMs can be applied to a wide range of language-related challenges through appropriate prompting or fine-tuning. This flexibility makes them particularly valuable as a business tool.</p></div>
<div class="paragraph"><p>Understanding LLMs as pattern-recognition systems trained on language data helps explain both their impressive capabilities and their limitations. They don&#8217;t "understand" text in the human sense but have learned statistical patterns that allow them to mimic understanding in ways that are increasingly useful for business applications.</p></div>
<div class="paragraph"><p>What makes LLMs particularly remarkable is that all these diverse capabilities—from writing marketing copy to analyzing financial data to generating computer code—emerge from the same fundamental next-token prediction mechanism. The model&#8217;s ability to perform such varied tasks comes from the patterns it learned during training rather than from task-specific programming. This explains why the same model can switch between different types of tasks simply based on how it&#8217;s prompted.</p></div></aside></div></section><section><div class="slide-content"><div class="paragraph h4-style"><p>There are two types of LLMs: Probabilistic and Chain-of-Thought Reasoning</p></div></div></section><section id="_probabilistic_llms"><h2>Probabilistic LLMs</h2><div class="slide-content"><div class="ulist"><ul><li><p>Trained to predict the next token based on statistical patterns in training data</p></li><li><p>Generate text by repeatedly predicting the most likely next word/token</p></li><li><p>Process is purely statistical - no explicit rules about grammar, facts, or reasoning</p></li><li><p>Each prediction influenced by the entire context provided so far</p></li><li><p>Examples: OpenAI&#8217;s GPT-4o, Anthropic&#8217;s Claude 3.5, Meta&#8217;s Llama2</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Probabilistic LLMs like GPT-4o represent the most common approach to language model development. Let&#8217;s explore how they work and their business implications:</p></div>
<div class="paragraph"><p>These models are trained through a process called "next token prediction." During training, the model is shown vast amounts of text and learns to predict what word or token is likely to come next in any given sequence. This training objective is purely statistical - the model learns patterns of word co-occurrence across billions of examples without explicit rules about grammar, facts, or reasoning.</p></div>
<div class="paragraph"><p>When generating text, probabilistic LLMs work by repeatedly predicting the most likely next word or token based on what they&#8217;ve already generated. Each prediction is influenced by the entire context provided so far. The model calculates probability distributions across its entire vocabulary (often 100,000+ tokens) and selects from these possibilities. This process continues word by word until the response is complete.</p></div></aside></div></section><section id="_probabilistic_strengths_limitations"><h2>Probabilistic: Strengths &amp; Limitations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Strengths:</p><div class="ulist"><ul><li><p>Remarkably fluent, natural-sounding writing</p></li><li><p>Broad knowledge across diverse domains</p></li><li><p>Creative content generation capabilities</p></li></ul></div></li><li><p>Limitations:</p><div class="ulist"><ul><li><p>May "hallucinate" facts that sound plausible but are incorrect</p></li><li><p>Can struggle with complex multi-step reasoning</p></li><li><p>Limited by training data cutoff and potential biases</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The strengths of probabilistic LLMs include remarkably fluent writing that mimics human language patterns, broad knowledge across diverse domains absorbed from their training data, and creative capabilities in generating novel content like stories, marketing copy, or business ideas. They excel at tasks requiring language fluency and general knowledge.</p></div>
<div class="paragraph"><p>However, these models have important limitations. They may "hallucinate" facts that sound plausible but are incorrect, as they&#8217;re optimizing for plausible-sounding text rather than factual accuracy. They can struggle with complex multi-step reasoning, particularly for problems requiring precise logical or mathematical thinking. They&#8217;re also limited by their training data - they don&#8217;t have real-time information beyond their training cutoff and may reflect biases present in that data.</p></div>
<div class="paragraph"><p>Understanding the probabilistic nature of these models helps explain both their impressive capabilities and their limitations. They don&#8217;t "know" facts in the human sense but have learned statistical patterns that allow them to generate text that often contains accurate information. This distinction is important when evaluating their potential applications in business contexts where factual accuracy or reliable reasoning may be critical.</p></div></aside></div></section><section id="_probabilistic_business_applications"><h2>Probabilistic: Business Applications</h2><div class="slide-content"><div class="ulist"><ul><li><p>Content creation: marketing materials, reports, communications</p></li><li><p>Document summarization and information extraction</p></li><li><p>Creative ideation and brainstorming</p></li><li><p>Customer support automation and chatbots</p></li><li><p>Knowledge management and information retrieval</p></li><li><p>Draft generation with human review and refinement</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>From a business perspective, probabilistic LLMs are particularly valuable for a wide range of applications:</p></div>
<div class="paragraph"><p>Content creation is one of the most common use cases, with LLMs helping to generate marketing materials, reports, emails, and other business communications. The models can produce drafts quickly that humans can then review and refine.</p></div>
<div class="paragraph"><p>Document summarization is another powerful application, allowing these models to condense lengthy reports, articles, or transcripts into concise summaries that capture key points. This can dramatically improve information processing efficiency.</p></div>
<div class="paragraph"><p>For creative tasks, LLMs excel at ideation and brainstorming, generating diverse perspectives and approaches that can spark innovation. They can suggest multiple angles on a problem or help develop creative concepts for marketing campaigns.</p></div>
<div class="paragraph"><p>Customer support automation has been revolutionized by these models, which can handle a wide range of customer inquiries with natural-sounding responses. They can be deployed as chatbots or used to assist human agents with response suggestions.</p></div>
<div class="paragraph"><p>Knowledge management applications leverage LLMs' ability to process and retrieve information from large document collections, making organizational knowledge more accessible and useful.</p></div>
<div class="paragraph"><p>The most effective business implementations typically combine LLM capabilities with human oversight, using the models to generate initial drafts or suggestions that humans then review, edit, and approve. This human-in-the-loop approach mitigates the risk of hallucinations or errors while still capturing the efficiency benefits.</p></div></aside></div></section><section id="_chain_of_thought_reasoning_llms"><h2>Chain-of-Thought Reasoning LLMs</h2><div class="slide-content"><div class="ulist"><ul><li><p>Specifically trained or prompted to show explicit reasoning steps</p></li><li><p>Process complex problems by breaking them into logical sequences</p></li><li><p>Examples: OpenAI&#8217;s o1/o3, DeepSeek-R1</p></li><li><p>Example approach for a math problem:</p><div class="ulist"><ul><li><p>Identify relevant variables and formulas</p></li><li><p>Work through calculations step-by-step</p></li><li><p>Verify results before providing final answer</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Chain-of-Thought (CoT) reasoning represents an important advancement in LLM capabilities, particularly evident in models like OpenAI&#8217;s o1 and o3 or when using specific prompting techniques with models like GPT-4:</p></div>
<div class="paragraph"><p>These models are either specifically trained or prompted to show explicit reasoning steps rather than jumping directly to conclusions. When faced with a complex problem, they break it down into a logical sequence of steps, working through the problem methodically much like a human would. This approach dramatically improves performance on tasks requiring multi-step reasoning.</p></div>
<div class="paragraph"><p>The key innovation in CoT models is their ability to process complex problems by breaking them into manageable components. For example, when solving a math problem, the model might first identify the relevant variables, then determine the appropriate formula, perform the calculation step by step, and finally verify the result. This step-by-step approach significantly reduces errors compared to attempting to solve problems in a single step.</p></div>
<div class="paragraph"><p>Chain-of-thought reasoning can be elicited in two primary ways: through specific model training that rewards step-by-step reasoning, or through prompting techniques that explicitly instruct the model to "think step by step" before answering. Both approaches have proven effective at improving performance on complex reasoning tasks.</p></div></aside></div></section><section id="_chain_of_thought_reasoning_strengths_limitations"><h2>Chain-of-Thought Reasoning: Strengths &amp; Limitations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Strengths:</p><div class="ulist"><ul><li><p>Superior mathematical and logical reasoning</p></li><li><p>Transparent decision-making process</p></li><li><p>Reduced error rates on complex problems</p></li><li><p>Self-correction capabilities</p></li></ul></div></li><li><p>Limitations:</p><div class="ulist"><ul><li><p>Higher computational requirements</p></li><li><p>Potentially slower response times</p></li><li><p>Still probabilistic at core - can make reasoning errors</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The strengths of CoT models include superior performance on mathematical reasoning tasks, logical problem-solving that requires multiple steps, and transparent decision-making where the reasoning process is visible and can be verified. This transparency is particularly valuable in business contexts where understanding how a conclusion was reached may be as important as the conclusion itself.</p></div>
<div class="paragraph"><p>A key advantage of chain-of-thought reasoning is the ability to self-correct. By working through problems step-by-step, these models can often identify errors in their own reasoning and revise their approach before arriving at a final answer. This significantly reduces error rates compared to models that attempt to solve problems in a single step.</p></div>
<div class="paragraph"><p>However, these models have limitations. They typically require more computational resources, which can result in higher costs and potentially slower responses compared to standard LLMs. They&#8217;re also still fundamentally probabilistic systems at their core, meaning they can make reasoning errors despite their step-by-step approach. Their performance depends significantly on how problems are presented to them.</p></div>
<div class="paragraph"><p>A key development in this area is the concept of "test-time compute" as a scaling law. Research has shown that allowing models more computation time to think through problems step by step can significantly improve performance, even without increasing model size. This insight suggests that future models may become increasingly capable of complex reasoning tasks simply by allocating more computational resources at inference time.</p></div></aside></div></section><section id="_chain_of_thought_reasoning_business_applications"><h2>Chain-of-Thought Reasoning: Business Applications</h2><div class="slide-content"><div class="ulist"><ul><li><p>Financial analysis and modeling</p></li><li><p>Complex decision support with transparent rationale</p></li><li><p>Process optimization and troubleshooting</p></li><li><p>Risk assessment and scenario planning</p></li><li><p>Educational applications and training</p></li><li><p>Regulatory compliance with documented reasoning</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>From a business perspective, CoT models are particularly valuable for applications requiring complex reasoning and transparency:</p></div>
<div class="paragraph"><p>Financial analysis and modeling benefit greatly from chain-of-thought reasoning, as these models can work through complex calculations while showing their work. This is especially valuable for investment decisions, financial forecasting, and budget planning where stakeholders need to understand the reasoning behind recommendations.</p></div>
<div class="paragraph"><p>For complex decision support, these models can evaluate multiple factors, weigh trade-offs, and provide recommendations with clear rationales. The transparency of their reasoning process builds trust and allows decision-makers to evaluate the quality of the analysis.</p></div>
<div class="paragraph"><p>Process optimization and troubleshooting are enhanced by the models' ability to systematically analyze workflows, identify bottlenecks, and suggest improvements with detailed explanations. This applies to manufacturing processes, supply chain optimization, and service delivery improvements.</p></div>
<div class="paragraph"><p>Risk assessment and scenario planning benefit from the models' ability to methodically work through different scenarios and their implications, helping organizations prepare for various contingencies with well-reasoned strategies.</p></div>
<div class="paragraph"><p>Educational applications leverage these models' ability to explain complex concepts step-by-step, making them valuable tools for training, knowledge transfer, and skill development within organizations.</p></div>
<div class="paragraph"><p>Regulatory compliance is another area where documented reasoning is particularly valuable. When decisions need to be justified to regulators or auditors, having a clear record of the reasoning process provides necessary transparency and accountability.</p></div>
<div class="paragraph"><p>Understanding the capabilities and limitations of CoT reasoning is crucial for identifying business problems where this approach might add significant value compared to standard probabilistic LLMs.</p></div></aside></div></section><section id="_comparing_llm_approaches"><h2>Comparing LLM Approaches</h2><div class="slide-content"><div class="ulist"><ul><li><p>Probabilistic models excel at fluent generation and broad knowledge tasks</p></li><li><p>Reasoning models perform better on complex problem-solving requiring logical steps</p></li><li><p>Cost considerations: Reasoning approaches may require more computational resources</p></li><li><p>Response time: Step-by-step reasoning typically takes longer than direct generation</p></li><li><p>Hybrid approaches often provide the best results for complex business applications</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Understanding the relative strengths and appropriate applications of different LLM approaches is crucial for effective business implementation:</p></div>
<div class="paragraph"><p>Probabilistic models excel at tasks requiring fluent language generation and broad knowledge. They&#8217;re particularly effective for content creation, summarization, creative writing, general question answering, and conversational interfaces. Their strength lies in their ability to generate natural-sounding text across a wide range of topics based on patterns learned from their training data. For many business applications where approximate answers or creative content are sufficient, probabilistic models offer an excellent balance of performance and efficiency.</p></div>
<div class="paragraph"><p>Reasoning models perform significantly better on tasks requiring complex problem-solving with logical steps. They&#8217;re particularly valuable for mathematical calculations, logical deductions, step-by-step analyses, and situations where the reasoning process itself needs to be transparent. Their explicit reasoning approach reduces errors on complex tasks and provides visibility into how conclusions are reached. For business applications where accuracy and verifiability are critical, reasoning-based approaches often justify their additional resource requirements.</p></div>
<div class="paragraph"><p>Cost considerations play an important role in choosing between approaches. Reasoning-based methods typically require more computational resources, which translates to higher costs in cloud-based API implementations. The explicit generation of intermediate reasoning steps means more tokens are generated, directly affecting usage-based pricing models. Organizations need to weigh these additional costs against the value of improved accuracy and transparency for specific use cases.</p></div>
<div class="paragraph"><p>Response time is another important factor. Step-by-step reasoning naturally takes longer than direct generation, as the model works through multiple intermediate steps before reaching a conclusion. For applications where immediate responses are critical, this additional latency may be problematic. However, for complex analytical tasks where accuracy is paramount, the additional time is often a worthwhile tradeoff.</p></div>
<div class="paragraph"><p>In practice, hybrid approaches often provide the best results for complex business applications. Many implementations use probabilistic models for initial content generation or simple queries, then switch to reasoning approaches for complex problems requiring verification or step-by-step analysis. Some systems also implement verification steps where outputs from probabilistic generation are checked using reasoning approaches before being presented to users.</p></div>
<div class="paragraph"><p>The field continues to evolve rapidly, with models increasingly incorporating reasoning capabilities as a standard feature rather than a separate approach. Understanding the appropriate application of different techniques allows organizations to optimize their AI implementations for specific business needs, balancing performance, cost, speed, and accuracy.</p></div></aside></div></section></section>
<section><section id="_enhancing_llm_capabilities"><h2>Enhancing LLM Capabilities</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this module, we&#8217;ll explore techniques that enhance the capabilities of Large Language Models, allowing them to overcome some of their inherent limitations. We&#8217;ll focus particularly on Retrieval-Augmented Generation (RAG) and tools integration, two approaches that significantly expand what LLMs can accomplish in business contexts.</p></div>
<div class="paragraph"><p>Understanding these enhancement techniques is crucial for identifying the full range of potential AI applications in your organization. Many valuable business use cases require capabilities beyond what a standalone LLM can provide, such as access to current information, proprietary data, or specialized functionality.</p></div>
<div class="paragraph"><p>By the end of this module, you&#8217;ll understand how these techniques work at a conceptual level and be able to identify business scenarios where they would add significant value. This knowledge will expand the range of processes you can consider for AI enhancement during the workshop.</p></div></aside></div></section><section id="_retrieval_augmented_generation_rag"><h2>Retrieval-Augmented Generation (RAG)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Combines LLMs with the ability to retrieve and reference specific information</p></li><li><p>Addresses the limitation of LLMs being restricted to their training data</p></li><li><p>Process: Query → Retrieve relevant documents → Incorporate into context → Generate response</p></li><li><p>Enables access to proprietary information, recent data, and specialized knowledge</p></li><li><p>Business applications: Knowledge management, customer support, compliance, research</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Retrieval-Augmented Generation (RAG) represents one of the most important enhancements to LLM capabilities, particularly for business applications:</p></div>
<div class="paragraph"><p>RAG combines the generative capabilities of LLMs with the ability to retrieve and reference specific information from external sources. This hybrid approach leverages the strengths of both technologies - the language understanding and generation abilities of LLMs and the precision and currency of information retrieval systems. The result is a system that can generate responses grounded in specific, retrievable information rather than relying solely on patterns learned during training.</p></div>
<div class="paragraph"><p>This approach directly addresses one of the most significant limitations of standalone LLMs: their restriction to information available in their training data. LLMs don&#8217;t have access to real-time information, proprietary company data, or specialized knowledge unless it was included in their training. RAG overcomes this limitation by allowing the model to access and incorporate external information at the time of response generation.</p></div>
<div class="paragraph"><p>The RAG process typically follows several steps: First, the system analyzes the user query to understand what information is needed. Next, it retrieves relevant documents or data from specified sources such as company databases, knowledge bases, or document repositories. These retrieved documents are then incorporated into the context provided to the LLM. Finally, the LLM generates a response that incorporates both its general language capabilities and the specific information retrieved.</p></div>
<div class="paragraph"><p>From a business perspective, RAG enables access to three critical categories of information: proprietary company data that wouldn&#8217;t be in public training sets (like internal policies, product specifications, or customer records); recent information that postdates the model&#8217;s training cutoff (like current market conditions or updated regulations); and specialized knowledge that might be too niche to be well-represented in general training data (like industry-specific terminology or procedures).</p></div>
<div class="paragraph"><p>RAG is particularly valuable for business applications like knowledge management (creating systems that can answer questions about company-specific information), customer support (providing accurate responses about products, services, and policies), compliance (ensuring responses reflect current regulations and company guidelines), and research (synthesizing information from multiple sources to answer complex questions).</p></div>
<div class="paragraph"><p>The implementation of RAG requires several components: a document storage system, a method for converting documents into a format suitable for retrieval (typically vector embeddings), a retrieval mechanism to find relevant information, and integration with an LLM for response generation. While these technical details are important for implementation, business professionals primarily need to understand the capabilities RAG enables and the types of information it can incorporate.</p></div>
<div class="paragraph"><p>By understanding RAG, you can identify business processes where access to specific information would significantly enhance the value of AI assistance - a crucial consideration when evaluating automation candidates during our workshop.</p></div></aside></div></section><section id="_tools_and_function_calling"><h2>Tools and Function Calling</h2><div class="slide-content"><div class="ulist"><ul><li><p>Enables LLMs to interact with external systems and perform specific actions</p></li><li><p>Examples: Calculators, web search, data analysis, calendar management, CRM updates</p></li><li><p>Process: LLM recognizes need for tool → Formats appropriate request → Tool executes → Results incorporated</p></li><li><p>Extends LLM capabilities beyond text generation to real-world actions</p></li><li><p>Business applications: Workflow automation, data analysis, scheduling, transaction processing</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Tools and function calling represent another crucial enhancement to LLM capabilities, particularly for business applications requiring interaction with external systems:</p></div>
<div class="paragraph"><p>This approach enables LLMs to interact with external systems and perform specific actions beyond text generation. Rather than being limited to producing text based on patterns in their training data, tool-augmented LLMs can recognize when a specific capability is needed and invoke the appropriate external system to perform that function. This dramatically extends what LLMs can accomplish in business contexts.</p></div>
<div class="paragraph"><p>Common examples of tools include calculators for precise mathematical operations, web search for current information, data analysis functions for processing structured data, calendar management for scheduling, CRM systems for customer information updates, and various API-connected services. Each tool provides specialized capabilities that complement the LLM&#8217;s language understanding and generation abilities.</p></div>
<div class="paragraph"><p>The process typically works as follows: The LLM recognizes from the user&#8217;s request or the context of the conversation that a specific tool is needed. It then formats an appropriate request to that tool, including necessary parameters or inputs. The external tool executes the requested function and returns results. Finally, the LLM incorporates these results into its response, often interpreting or explaining them for the user.</p></div>
<div class="paragraph"><p>This capability extends LLMs beyond text generation to enabling real-world actions and accessing real-time information. For example, an LLM with calendar access can not only discuss scheduling concepts but actually check availability and create appointments. One with calculation tools can perform precise financial analyses rather than approximating calculations based on training patterns.</p></div>
<div class="paragraph"><p>From a business perspective, tool integration is particularly valuable for applications like workflow automation (triggering actions across multiple systems), data analysis (performing calculations and generating insights from business data), scheduling and coordination (managing calendars and resources), and transaction processing (initiating and confirming business transactions in external systems).</p></div>
<div class="paragraph"><p>The implementation of tool integration requires defining the available tools, their parameters, and how they should be invoked. The LLM needs to be instructed or fine-tuned to recognize when tools are appropriate and how to format requests correctly. While these technical details are important for implementation, business professionals primarily need to understand what capabilities tool integration enables and what systems might be connected.</p></div>
<div class="paragraph"><p>By understanding tool integration, you can identify business processes where interaction with external systems would significantly enhance the value of AI assistance - another crucial consideration when evaluating automation candidates during our workshop.</p></div></aside></div></section><section id="_combining_approaches_for_business_solutions"><h2>Combining Approaches for Business Solutions</h2><div class="slide-content"><div class="ulist"><ul><li><p>Most effective business implementations combine multiple enhancement techniques</p></li><li><p>RAG provides knowledge grounding while tools enable actions</p></li><li><p>Integration considerations: Data security, system access, performance requirements</p></li><li><p>Cost-benefit analysis: Enhanced capabilities vs. implementation complexity</p></li><li><p>Example: Customer service agent with product knowledge (RAG) and order processing abilities (tools)</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In practice, the most effective business implementations of AI often combine multiple enhancement techniques to create comprehensive solutions:</p></div>
<div class="paragraph"><p>Combining approaches allows organizations to address multiple limitations of standalone LLMs simultaneously. For example, RAG provides knowledge grounding by giving the LLM access to specific information, while tools enable actions by connecting the LLM to external systems. Together, these enhancements create AI systems that both know more and can do more than basic LLMs.</p></div>
<div class="paragraph"><p>A typical combined implementation might include: an LLM for natural language understanding and generation; RAG capabilities to incorporate company-specific information; tool connections to relevant business systems; and potentially chain-of-thought reasoning for complex decision processes. Each component addresses specific requirements of the business process being automated.</p></div>
<div class="paragraph"><p>When considering such implementations, several integration considerations become important. Data security is paramount when connecting AI systems to proprietary information or business systems. System access must be carefully managed to ensure the AI has appropriate permissions while maintaining security boundaries. Performance requirements must be evaluated, as each enhancement adds some latency to the overall system response time.</p></div>
<div class="paragraph"><p>Cost-benefit analysis becomes more complex with combined approaches. Enhanced capabilities must be weighed against increased implementation complexity and operational costs. Not every business process requires the full suite of enhancements - the appropriate combination should be determined based on specific requirements and expected value.</p></div>
<div class="paragraph"><p>As a concrete example, consider a customer service AI agent. It might use RAG to access product specifications, pricing information, and company policies; tool connections to check inventory, process orders, and update customer records; and chain-of-thought reasoning to troubleshoot complex customer issues. This combination creates a comprehensive solution that can handle a wide range of customer interactions with minimal human intervention.</p></div>
<div class="paragraph"><p>For business professionals evaluating AI opportunities, understanding these combined approaches is valuable even without technical implementation knowledge. It allows you to envision more comprehensive solutions and identify processes where multiple enhancements might create significant value. During our workshop, we&#8217;ll explore how different combinations of capabilities might address specific business needs in your organization.</p></div>
<div class="paragraph"><p>The field continues to evolve rapidly, with new enhancement techniques emerging regularly. The fundamental principle remains consistent: identifying the specific limitations of basic LLMs that affect your use case and applying the appropriate enhancements to address those limitations.</p></div></aside></div></section></section>
<section><section id="_effective_prompting_strategies"><h2>Effective Prompting Strategies</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this module, we&#8217;ll explore the art and science of effective prompting - how to communicate with AI systems to get the results you need. Prompt engineering is a crucial skill for business professionals working with AI, as it directly affects the quality and usefulness of AI outputs.</p></div>
<div class="paragraph"><p>We&#8217;ll examine the fundamentals of prompt design and explore specific strategies for different types of LLMs. We&#8217;ll also compare approaches and discuss how to adapt your prompting techniques based on the specific task and model you&#8217;re working with.</p></div>
<div class="paragraph"><p>By the end of this module, you&#8217;ll understand how to construct effective prompts for different business scenarios and be able to optimize your interactions with AI systems to achieve better outcomes. This practical knowledge will be immediately applicable as you begin working with AI tools.</p></div></aside></div></section><section id="_prompting_probabilistic_llms_gpt_4o"><h2>Prompting Probabilistic LLMs (GPT-4o)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Be specific and explicit about desired outcomes and formats</p></li><li><p>Provide relevant context to overcome knowledge limitations</p></li><li><p>Use examples (few-shot learning) to demonstrate expected outputs</p></li><li><p>Manage token limitations by focusing on essential information</p></li><li><p>Business strategy: Start broad, then refine based on initial responses</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Probabilistic LLMs like GPT-4o require specific prompting strategies to maximize their effectiveness for business applications:</p></div>
<div class="paragraph"><p>Being specific and explicit about desired outcomes is particularly important with probabilistic models. Rather than assuming the model will intuit your needs, clearly state what you want, including the format, level of detail, tone, and any specific elements you require. For example, instead of asking "Tell me about our quarterly results," specify "Analyze our Q2 financial results in a 5-bullet executive summary highlighting year-over-year trends in revenue, expenses, and profit margins."</p></div>
<div class="paragraph"><p>Providing relevant context helps overcome the knowledge limitations inherent in these models. Remember that probabilistic LLMs don&#8217;t have access to information beyond their training data unless you provide it. For business applications, this often means including specific facts, figures, or background information in your prompt. For example, when asking for analysis of a business situation, include the key data points the model needs to consider.</p></div>
<div class="paragraph"><p>Using examples, often called few-shot learning, is a powerful technique for demonstrating expected outputs. By showing the model one or more examples of the type of response you want, you provide a pattern it can follow. This approach is particularly effective for specialized formats or when you need consistency across multiple outputs. For instance, if you need product descriptions in a specific format, provide an example or two in your prompt.</p></div>
<div class="paragraph"><p>Managing token limitations is important since all LLMs have context windows that limit how much text they can process at once. Focus on providing essential information rather than exhaustive details. For business applications, this might mean summarizing background information rather than including complete documents, or linking to reference materials rather than pasting their entire contents.</p></div>
<div class="paragraph"><p>From a business strategy perspective, an effective approach is to start with broader prompts and then refine based on initial responses. Begin with a general request to see what the model produces, then iterate with more specific guidance based on what&#8217;s missing or needs improvement. This iterative approach often yields better results than trying to craft the perfect prompt on the first attempt.</p></div>
<div class="paragraph"><p>Business-specific prompting strategies might include using industry terminology to improve relevance, specifying the intended audience for the output (e.g., "Write this for C-level executives"), and including company-specific context that might not be in the model&#8217;s training data. These adaptations help tailor generic LLM capabilities to your specific business needs.</p></div>
<div class="paragraph"><p>Remember that probabilistic LLMs are particularly good at generating creative content, summarizing information, and producing natural-sounding language. Your prompting strategy should leverage these strengths while providing sufficient guidance to overcome limitations in factual precision or complex reasoning.</p></div></aside></div></section><section id="_prompting_chain_of_thought_llms_o1o3"><h2>Prompting Chain-of-Thought LLMs (o1/o3)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Explicitly request step-by-step reasoning in your prompts</p></li><li><p>Structure complex problems with clear intermediate steps</p></li><li><p>Encourage the model to "think aloud" before concluding</p></li><li><p>Implement verification steps to check reasoning validity</p></li><li><p>Business strategy: Break complex problems into logical sequences</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Chain-of-Thought (CoT) LLMs like OpenAI&#8217;s o1/o3 or Claude 3 Opus require different prompting strategies to fully leverage their reasoning capabilities:</p></div>
<div class="paragraph"><p>Explicitly requesting step-by-step reasoning is the foundation of effective CoT prompting. Unlike with standard LLMs where you might ask directly for a conclusion, with reasoning-focused models you should specifically ask the model to work through the problem methodically. For example, instead of "What&#8217;s the optimal inventory level?" try "Please think through the optimal inventory level step by step, considering our lead times, demand variability, and storage costs."</p></div>
<div class="paragraph"><p>Structuring complex problems with clear intermediate steps helps guide the model&#8217;s reasoning process. Break down multi-part problems into a logical sequence and ask the model to address each component in order. This approach is particularly effective for complex business analyses or decision-making scenarios. For instance, when evaluating a potential market entry, you might structure the prompt to first analyze market size, then competition, then regulatory considerations, and finally potential profitability.</p></div>
<div class="paragraph"><p>Encouraging the model to "think aloud" leverages the model&#8217;s ability to reason through problems verbally. Phrases like "Let&#8217;s think about this step by step" or "Let&#8217;s work through this methodically" signal to the model that you want to see its reasoning process, not just its conclusion. This approach is valuable when the reasoning itself provides insights or when you need to verify the model&#8217;s approach to a problem.</p></div>
<div class="paragraph"><p>Implementing verification steps improves accuracy by asking the model to check its own work. After the model provides a solution, prompt it to verify the answer by working backward, using a different method, or checking for common errors. For example, after a financial calculation, you might ask "Please verify this result by using an alternative calculation method and check for any potential errors in your reasoning."</p></div>
<div class="paragraph"><p>From a business strategy perspective, the key is breaking complex problems into logical sequences that the model can work through methodically. This approach is particularly valuable for financial analyses, strategic decisions, risk assessments, and other business scenarios where the reasoning process is as important as the conclusion.</p></div>
<div class="paragraph"><p>When working with CoT models in business contexts, it&#8217;s often valuable to combine reasoning requests with specific business frameworks or methodologies relevant to your industry. For example, you might ask the model to apply a specific strategic framework like Porter&#8217;s Five Forces or a standard financial analysis methodology to ensure the reasoning follows established business practices.</p></div>
<div class="paragraph"><p>The explicit reasoning capabilities of these models make them particularly valuable for explaining complex concepts to stakeholders, documenting decision processes for compliance purposes, and building confidence in AI-assisted business decisions through transparent reasoning.</p></div></aside></div></section></section>
<section id="_what_is_agentic_ai"><h2>What is Agentic AI?</h2></section>
<section><section id="_introduction_to_ai_agents"><h2>Introduction to AI Agents</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>Welcome to our introduction to AI Agents. This one-hour session serves as a foundation for the three-day workshop that follows, where we&#8217;ll focus on discovering business processes that are candidates for automation using AI agents. Today, we&#8217;ll explore what AI agents are, how they differ from traditional automation, and why they represent a significant advancement in how we approach business process automation. The concepts we cover today will be applied directly in our upcoming workshop as you identify and evaluate opportunities for implementing AI agents in your organization.</p></div></aside></div></section><section id="_why_ai_agents_matter"><h2>Why AI Agents Matter</h2><div class="slide-content"><div class="ulist"><ul><li><p>Traditional automation approaches have reached their limitations</p></li><li><p>Business demands increasingly require intelligence, not just process execution</p></li><li><p>AI agents represent a paradigm shift in how work gets accomplished</p></li><li><p>Organizations adopting AI agents realize exponential business value</p></li><li><p>Properly implemented AI agents can automate 70-90% of previously human-only tasks</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>We&#8217;re beginning our session by addressing why AI agents matter in today&#8217;s business landscape. Traditional automation approaches like robotic process automation (RPA) have been valuable but are now reaching their inherent limitations. These systems follow predefined rules without the ability to adapt to new situations or make contextual decisions.</p></div>
<div class="paragraph"><p>Today&#8217;s business challenges require more than just executing processes—they demand intelligence and adaptability. AI agents represent a fundamental shift in automation, moving from simple task execution to autonomous decision-making and outcome delivery. This shift enables organizations to automate processes that previously required human judgment.</p></div>
<div class="paragraph"><p>Organizations that have successfully implemented AI agents are seeing exponential returns on their investments. Unlike linear improvements from traditional automation, AI agents can handle increasingly complex tasks and learn from experience, delivering compounding value over time.</p></div>
<div class="paragraph"><p>Research and real-world implementations have shown that properly designed AI agents can automate 70-90% of tasks within jobs that were previously considered too complex for automation. This doesn&#8217;t necessarily mean replacing jobs, but rather augmenting human workers by handling routine aspects of their work, allowing them to focus on higher-value activities.</p></div>
<div class="paragraph"><p>Understanding AI agents is crucial for maintaining competitiveness in an increasingly AI-driven business environment. The organizations that effectively identify and implement AI agent opportunities will gain significant advantages in operational efficiency, cost reduction, and service delivery.</p></div></aside></div></section></section>
<section><section id="_from_rpa_to_ai_agents"><h2>From RPA to AI Agents</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this section, we&#8217;ll explore the evolution from traditional Robotic Process Automation (RPA) to modern AI agents. Understanding this transition is crucial as it demonstrates why AI agents represent such a significant advancement in business process automation. We&#8217;ll examine the key differences between these approaches and highlight why AI agents are better suited to address the complex, adaptive challenges faced by modern organizations.</p></div></aside></div></section><section id="_rpa_vs_ai_agents_key_differences"><h2>RPA vs. AI Agents: Key Differences</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Process Execution</strong>: Rule-based automation vs. autonomous delivery</p></li><li><p><strong>Adaptability</strong>: Fixed parameters vs. contextual understanding</p></li><li><p><strong>Focus</strong>: Process-centered vs. outcome-driven</p></li><li><p><strong>Decision-making</strong>: Limited or none vs. autonomous judgment</p></li><li><p><strong>Learning</strong>: Static capabilities vs. continuous improvement</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s examine the fundamental differences between traditional Robotic Process Automation (RPA) and AI agents:</p></div>
<div class="paragraph"><p>Process Execution: RPA systems excel at automating well-defined, repetitive processes through rule-based programming. They follow precise instructions without deviation. In contrast, AI agents can autonomously execute tasks by understanding the desired outcome and determining the best approach to achieve it, even when conditions change.</p></div>
<div class="paragraph"><p>Adaptability: RPA operates within fixed parameters and struggles when encountering unexpected scenarios or variations in data or processes. AI agents can understand context and adapt their approach accordingly, making them resilient to changes in the environment or inputs.</p></div>
<div class="paragraph"><p>Focus: RPA is process-centered, focusing on executing predefined steps in a workflow without regard to the ultimate business outcome. AI agents are outcome-driven, prioritizing the achievement of business objectives even if that requires adjusting their approach or making decisions not explicitly programmed.</p></div>
<div class="paragraph"><p>Decision-making: RPA systems have limited or no decision-making capabilities beyond simple conditional logic. AI agents can make autonomous judgments based on available information, weigh options, and choose the most appropriate course of action to achieve the desired outcome.</p></div>
<div class="paragraph"><p>Learning: RPA remains static unless manually reprogrammed, with no ability to improve performance based on experience. AI agents can continuously learn from interactions, feedback, and outcomes, becoming more effective and efficient over time without requiring explicit reprogramming.</p></div>
<div class="paragraph"><p>These differences highlight why AI agents represent a significant advancement over traditional automation approaches, particularly for complex, variable processes that require adaptability, decision-making, and continuous improvement.</p></div></aside></div></section><section id="_evolution_of_ai_assistance"><h2>Evolution of AI Assistance</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>AI Assistants</strong>: Chat interactions to accelerate and simplify tasks</p></li><li><p><strong>AI Agents</strong>: Pre-packaged prompts and simple workflows for repeatable tasks</p></li><li><p><strong>AI Workers</strong>: Complex workflows and reasoning for specific outcome delivery</p></li><li><p>Evolution represents increasing autonomy and complexity of handled tasks</p></li><li><p>Business value increases exponentially with sophistication of AI implementation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The field of AI assistance has evolved through distinct stages, each representing increasing levels of autonomy and capability:</p></div>
<div class="paragraph"><p>AI Assistants represent the first wave of AI assistance most people have experienced. These systems, like basic chatbots and virtual assistants, help accelerate and simplify tasks through conversational interfaces. They respond to direct queries and commands but generally require continuous human guidance. While valuable for information retrieval and simple task execution, they lack true autonomy.</p></div>
<div class="paragraph"><p>AI Agents mark the next stage of evolution, employing pre-packaged prompts and simple workflows to complete repeatable tasks with minimal supervision. These systems can handle routine processes end-to-end, requiring human intervention only for exceptions or approvals. They represent a significant step toward automation of knowledge work.</p></div>
<div class="paragraph"><p>AI Workers represent the most advanced implementation, utilizing complex workflows and sophisticated reasoning to deliver specific outcomes with minimal human input. These systems can understand context, make judgments, and adapt their approach to achieve desired results. They can perform complete business functions previously reserved for human knowledge workers.</p></div>
<div class="paragraph"><p>This evolution represents a continuum of increasing autonomy and capability to handle complex tasks. As we move from assistants to workers, the AI takes on more responsibility and requires less human oversight and intervention.</p></div>
<div class="paragraph"><p>Importantly, business value increases exponentially as organizations implement more sophisticated AI solutions. While assistants provide valuable efficiency gains, true transformation comes from implementing autonomous AI workers that can handle complex knowledge work independently.</p></div>
<div class="paragraph"><p>Understanding where your organization currently stands in this evolution and identifying opportunities to advance to the next level will be a key focus of our upcoming workshop sessions.</p></div></aside></div></section></section>
<section><section id="_ai_knowledge_workers"><h2>AI Knowledge Workers</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this section, we&#8217;ll explore how AI workers function as knowledge workers, capable of performing complex cognitive tasks previously reserved for humans. Understanding the capabilities of AI knowledge workers is essential for identifying which business processes are suitable candidates for automation through AI agents.</p></div></aside></div></section><section id="_ai_workers_as_knowledge_workers"><h2>AI Workers as Knowledge Workers</h2><div class="slide-content"><div class="ulist"><ul><li><p>AI workers can perform cognitive tasks previously reserved for humans</p></li><li><p>They analyze, understand, decide, execute, and learn like human knowledge workers</p></li><li><p>AI workers combine LLMs with structured prompts, workflows, and knowledge bases</p></li><li><p>Significant advancements in AI capabilities enable complex knowledge work automation</p></li><li><p>AI workers can now handle tasks requiring judgment, analysis, and adaptation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>AI workers represent a fundamental shift in automation capabilities, now able to perform cognitive tasks that were previously exclusive to human knowledge workers:</p></div>
<div class="paragraph"><p>AI workers mirror the cognitive processes of human knowledge workers, employing a similar workflow of analyzing information, understanding context, making decisions, executing actions, and learning from outcomes. This mirrors how human professionals approach complex tasks.</p></div>
<div class="paragraph"><p>The technical foundation of AI workers combines large language models (LLMs) with structured prompts, well-defined workflows, and access to knowledge bases. This integration creates systems capable of sophisticated reasoning and task execution beyond simple rule following.</p></div>
<div class="paragraph"><p>Recent advancements in AI capabilities, particularly in language understanding, reasoning, and context awareness, have enabled this shift. Models have progressed from simple pattern recognition to complex reasoning that can be applied to knowledge work.</p></div>
<div class="paragraph"><p>Modern AI workers can now handle tasks requiring judgment, analysis, and adaptation—activities that traditional automation could not address. They can interpret ambiguous information, weigh options based on multiple factors, and adjust their approach as circumstances change.</p></div>
<div class="paragraph"><p>This evolution has profound implications for how organizations approach automation. Processes previously considered too complex or nuanced for automation can now be candidates for AI worker implementation, expanding the automation frontier into knowledge work domains.</p></div>
<div class="paragraph"><p>As we move forward in our workshop, we&#8217;ll explore how to identify which knowledge work processes in your organization might be suitable for AI worker implementation, focusing on tasks that require analysis, understanding, decision-making, execution, and continuous improvement.</p></div></aside></div></section><section id="_core_capabilities_of_ai_knowledge_workers"><h2>Core Capabilities of AI Knowledge Workers</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Analyze</strong>: Navigate, read, calculate, extract information</p></li><li><p><strong>Understand</strong>: Interpret, summarize, compare data and contexts</p></li><li><p><strong>Decide</strong>: Weigh options, make judgments, choose appropriate actions</p></li><li><p><strong>Execute</strong>: Click, write, post, convert, update information and systems</p></li><li><p><strong>Learn</strong>: Adapt approaches, improve performance over time</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s examine the five core capabilities that enable AI workers to function effectively as knowledge workers:</p></div>
<div class="paragraph"><p>Analyze: AI workers excel at information processing, capable of navigating complex data environments, reading various document formats, calculating numerical values, and extracting relevant information from diverse sources. These analytical capabilities allow AI workers to gather and process the inputs needed for effective task completion, similar to how human workers collect and review information.</p></div>
<div class="paragraph"><p>Understand: Beyond mere data collection, AI workers can interpret information in context, summarize key points from extensive content, and compare different data points to identify patterns, relationships, and inconsistencies. This contextual understanding is crucial for meaningful automation that goes beyond simple data processing to genuine comprehension.</p></div>
<div class="paragraph"><p>Decide: AI workers can weigh multiple options based on various criteria, make judgments about the best course of action, and choose appropriate responses to situations—even when facing ambiguity or incomplete information. This decision-making capability allows AI workers to handle complex tasks that require evaluating trade-offs and selecting optimal approaches.</p></div>
<div class="paragraph"><p>Execute: After analyzing, understanding, and deciding, AI workers can take concrete actions such as clicking through interfaces, writing content, posting information, converting files between formats, and updating records in various systems. This execution capability enables AI workers to produce tangible outcomes and interact with existing digital infrastructure.</p></div>
<div class="paragraph"><p>Learn: Perhaps most importantly, AI workers can adapt their approaches based on feedback and outcomes, continuously improving their performance over time without explicit reprogramming. This learning capability ensures that AI workers become increasingly valuable assets, growing more efficient and effective through experience.</p></div>
<div class="paragraph"><p>Together, these capabilities form the foundation of AI knowledge work, enabling systems to perform complex tasks previously reserved for human professionals. When identifying processes for potential AI worker implementation, consider whether the process requires these five capabilities and to what degree the AI worker can fulfill them.</p></div></aside></div></section></section>
<section><section id="_ai_agent_architecture"><h2>AI Agent Architecture</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this section, we&#8217;ll explore the architecture that makes AI agents possible. Understanding the components and their relationships is essential for implementing effective AI agent solutions and evaluating potential automation candidates during our upcoming workshop.</p></div></aside></div></section><section id="_core_components_of_an_ai_agent_architecture"><h2>Core Components of an AI Agent Architecture</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Instructions</strong>: Prompts and guidance that define the task and objectives</p></li><li><p><strong>Knowledge</strong>: Data and insights relevant to completing the task</p></li><li><p><strong>Actions</strong>: Capabilities to interact with systems and execute operations</p></li><li><p><strong>Large Language Model(s)</strong>: The reasoning engine that drives understanding and decisions</p></li><li><p><strong>Integration Layer</strong>: Connects the agent to enterprise systems and data sources</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s examine the core components that comprise an effective AI agent architecture:</p></div>
<div class="paragraph"><p>Instructions form the foundation of any AI agent implementation, providing clear prompts and guidance that define what the agent should accomplish and how it should approach the task. Well-crafted instructions are critical for ensuring the agent understands its objectives, constraints, and success criteria. These can range from simple prompts to complex decision trees that guide the agent&#8217;s behavior in different scenarios.</p></div>
<div class="paragraph"><p>Knowledge represents the information resources available to the agent, including enterprise data, domain-specific insights, and contextual information relevant to completing assigned tasks. This knowledge can be provided through various mechanisms such as retrieval-augmented generation (RAG), structured databases, or direct context inclusion. The quality and relevance of this knowledge significantly impact the agent&#8217;s effectiveness.</p></div>
<div class="paragraph"><p>Actions define what the agent can actually do—its capabilities to interact with systems, manipulate data, generate content, and execute operations. These actions can include API calls to enterprise systems, browser automation for web interfaces, content generation capabilities, and communication channels. The scope of possible actions determines what tasks the agent can autonomously complete.</p></div>
<div class="paragraph"><p>Large Language Model(s) serve as the reasoning engine that powers the agent, enabling it to understand instructions, interpret knowledge, make decisions, and determine appropriate actions. The LLM&#8217;s capabilities in comprehension, reasoning, and generation directly influence the agent&#8217;s overall effectiveness and the complexity of tasks it can handle.</p></div>
<div class="paragraph"><p>Integration Layer provides the connections between the agent and enterprise systems, enabling seamless interaction with existing infrastructure, data sources, and workflows. This layer translates between the agent&#8217;s operations and the specific requirements of various systems, allowing the agent to work within the organization&#8217;s digital ecosystem.</p></div>
<div class="paragraph"><p>Understanding these components and how they interact is essential for designing effective AI agent solutions. During our workshop, we&#8217;ll explore how to evaluate and select each component based on your specific business requirements and automation objectives.</p></div></aside></div></section><section id="_ai_agent_architecture_diagram"><h2>AI Agent Architecture Diagram</h2><div class="slide-content"><div class="ulist"><ul><li><p>AI Workers interact with enterprise systems through universal API connectors or browser automation</p></li><li><p>Enterprise Knowledge Engine provides context and factual information</p></li><li><p>AI Knowledge/Actions power the agent&#8217;s capabilities to analyze, understand, decide, execute, and learn</p></li><li><p>Instructions guide the agent&#8217;s behavior and define objectives</p></li><li><p>Architecture enables controlled autonomous operation with appropriate guardrails</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The AI Agent Architecture Diagram illustrates how the various components interact to enable autonomous task completion:</p></div>
<div class="paragraph"><p>At the core of the architecture are the AI Worker capabilities we discussed earlier—the abilities to analyze, understand, decide, execute, and learn. These capabilities are enabled by the combination of large language models, structured workflows, and integration with enterprise systems.</p></div>
<div class="paragraph"><p>The Universal API Connector or Human-like Browser Mode represents the integration layer that allows AI agents to interact with enterprise systems. The API connector provides direct system integration, while browser automation enables interaction with web interfaces just as human users would. This flexibility allows agents to work with both modern API-enabled systems and legacy applications without dedicated integrations.</p></div>
<div class="paragraph"><p>The Enterprise Knowledge Engine serves as the agent&#8217;s information foundation, providing access to organizational data, documents, policies, and other knowledge resources. This component ensures the agent has the necessary context and factual information to make appropriate decisions and execute tasks correctly.</p></div>
<div class="paragraph"><p>Instructions provide the guidance and parameters that direct the agent&#8217;s behavior, defining what it should accomplish and how it should operate. These instructions can include specific prompts, workflow definitions, decision criteria, and success metrics.</p></div>
<div class="paragraph"><p>The entire architecture is designed to enable controlled autonomous operation, with appropriate guardrails ensuring the agent works within defined boundaries and according to organizational policies and standards.</p></div>
<div class="paragraph"><p>This architectural approach provides both flexibility and control—allowing organizations to implement AI agents across diverse use cases while maintaining appropriate oversight and governance. During our workshop, we&#8217;ll explore how this architecture can be applied to specific business processes in your organization.</p></div></aside></div></section></section>
<section><section id="_trusting_ai_workers"><h2>Trusting AI Workers</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this section, we&#8217;ll address one of the most critical aspects of implementing AI agents: establishing trust in autonomous AI systems. Understanding the requirements for building trust is essential for successful adoption and effective implementation of AI agents in your organization.</p></div></aside></div></section><section id="_requirements_for_trusting_autonomous_ai"><h2>Requirements for Trusting Autonomous AI</h2><div class="slide-content"><div class="ulist"><ul><li><p>AI needs comprehensive understanding of organizational knowledge and standards</p></li><li><p>Systems must provide visibility into AI reasoning and information sources</p></li><li><p>Organizations need control mechanisms to define and enforce behavioral boundaries</p></li><li><p>AI must respect existing permissions and security protocols</p></li><li><p>Continuous monitoring and feedback loops ensure appropriate operation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>For AI workers to be trusted with autonomous operation, several key requirements must be satisfied:</p></div>
<div class="paragraph"><p>Comprehensive understanding of organizational knowledge and standards is essential for AI workers to operate appropriately within your business context. This means the AI must have access to relevant policies, procedures, best practices, and domain-specific information. Without this understanding, AI workers may make decisions or take actions that don&#8217;t align with organizational expectations.</p></div>
<div class="paragraph"><p>Visibility into AI reasoning and information sources provides the transparency needed for oversight and accountability. Unlike "black box" AI systems, effective AI workers should provide clear reasoning for their decisions and actions, including what information they considered and how they reached their conclusions. This visibility builds trust by making the AI&#8217;s operations understandable and auditable.</p></div>
<div class="paragraph"><p>Control mechanisms allow organizations to define and enforce behavioral boundaries for AI workers. These guardrails ensure that AI systems operate within acceptable parameters and don&#8217;t take actions that could be harmful or counter to business objectives. Control mechanisms might include approval workflows for certain actions, restrictions on system access, or explicit prohibition of specific operations.</p></div>
<div class="paragraph"><p>Respecting existing permissions and security protocols ensures that AI workers don&#8217;t circumvent established security measures or access information they shouldn&#8217;t. AI systems should operate within the same security framework as human employees, with appropriate authentication, authorization, and audit trails.</p></div>
<div class="paragraph"><p>Continuous monitoring and feedback loops provide ongoing assurance that AI workers are performing as expected and allow for correction when necessary. This includes tracking performance metrics, reviewing outcomes, and incorporating human feedback to improve future operations.</p></div>
<div class="paragraph"><p>These requirements highlight that implementing trusted AI workers isn&#8217;t just about technical capabilities—it&#8217;s equally about governance, oversight, and appropriate integration with existing organizational processes and controls. During our workshop, we&#8217;ll explore practical approaches to addressing these requirements in your specific context.</p></div></aside></div></section><section id="_building_trust_through_knowledge_and_control"><h2>Building Trust Through Knowledge and Control</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Knowledge Integration</strong>: Connected to systems of record and sources of truth</p></li><li><p><strong>Context Access</strong>: Has the right information accessible during task execution</p></li><li><p><strong>Fact Understanding</strong>: Comprehends canonical information and organizational knowledge</p></li><li><p><strong>Boundary Definition</strong>: Clear delineation of what the AI can and cannot do</p></li><li><p><strong>Operational Transparency</strong>: Visibility into reasoning and decision processes</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Building trust in AI workers requires a balanced approach focusing on both knowledge integration and operational control:</p></div>
<div class="paragraph"><p>Knowledge Integration ensures AI workers have access to authoritative organizational information by connecting them to systems of record and sources of truth. This integration might include CRM systems, document management platforms, ERP systems, knowledge bases, and other enterprise applications. When AI has access to accurate, up-to-date information, it can make better-informed decisions and provide more reliable outputs.</p></div>
<div class="paragraph"><p>Context Access goes beyond simple system integration to ensure AI workers have the specific information needed for each task. This means implementing mechanisms that retrieve and provide relevant context at the moment it&#8217;s required, whether that&#8217;s customer history during a support interaction, policy details during a compliance review, or product specifications during a sales engagement.</p></div>
<div class="paragraph"><p>Fact Understanding enables AI workers to comprehend and correctly apply canonical information and organizational knowledge. This involves not just accessing facts but understanding their relationships, implications, and appropriate application in different scenarios. Advanced knowledge representation techniques like knowledge graphs can enhance this capability.</p></div>
<div class="paragraph"><p>Boundary Definition provides clear guardrails for AI operation by explicitly defining what the AI can and cannot do. These boundaries might include limitations on financial approval thresholds, restrictions on customer communication channels, or requirements for human approval of certain actions. Clear boundaries help prevent unexpected or inappropriate AI behaviors.</p></div>
<div class="paragraph"><p>Operational Transparency makes AI worker processes understandable and reviewable by providing visibility into reasoning and decision processes. This transparency might include explanations of why specific actions were taken, what information was considered most relevant, and how conflicting priorities were balanced. When stakeholders can see how and why the AI reached its conclusions, their trust in the system increases.</p></div>
<div class="paragraph"><p>By addressing both knowledge capabilities and control mechanisms, organizations can build AI worker implementations that earn trust through demonstrated reliability, appropriate operation, and alignment with organizational values and objectives.</p></div></aside></div></section></section>
<section><section id="_ai_workers_in_action"><h2>AI Workers in Action</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this section, we&#8217;ll explore concrete examples of AI workers successfully implementing automation across different business functions. These examples will help illustrate the practical applications of the concepts we&#8217;ve discussed and provide inspiration for identifying automation candidates in your own organization.</p></div></aside></div></section><section id="_example_customer_support_ai_worker"><h2>Example: Customer Support AI Worker</h2><div class="slide-content"><div class="ulist"><ul><li><p>Handles customer inquiries across multiple channels autonomously</p></li><li><p>Analyzes customer history and current issue to provide context-aware responses</p></li><li><p>Makes decisions about resolution approaches and escalation criteria</p></li><li><p>Executes actions including providing information, updating records, and initiating processes</p></li><li><p>Achieves 80% automated resolution rate with high customer satisfaction</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s examine a specific example of an AI worker implementation in customer support:</p></div>
<div class="paragraph"><p>The Customer Support AI Worker handles inbound customer inquiries across multiple channels, including email, chat, and messaging platforms. Unlike simple chatbots, this AI worker can manage complete support interactions from initial contact through resolution, working autonomously for straightforward issues and collaborating with human agents for complex cases.</p></div>
<div class="paragraph"><p>This AI worker analyzes comprehensive customer context, including purchase history, previous support interactions, product usage patterns, and current account status. This deep contextual understanding enables personalized assistance that addresses the customer&#8217;s specific situation rather than providing generic responses.</p></div>
<div class="paragraph"><p>Based on its analysis, the AI worker makes informed decisions about the most appropriate resolution approach. It determines whether issues can be resolved immediately, require additional information, need escalation to specialists, or warrant proactive offers based on the customer&#8217;s situation. These decisions follow established support protocols while adapting to each unique case.</p></div>
<div class="paragraph"><p>The AI worker executes a range of actions to resolve customer issues, including providing detailed product information, troubleshooting guidance, updating customer records, processing returns or exchanges, scheduling appointments, and initiating workflows in connected systems. This action capability enables end-to-end issue resolution without human intervention in many cases.</p></div>
<div class="paragraph"><p>In real-world implementations, customer support AI workers have achieved impressive results, including 80% automated resolution rates for incoming inquiries, significant reductions in average handling time, consistent 24/7 support coverage, and high customer satisfaction ratings. The most advanced implementations learn from each interaction, continuously improving their performance based on outcomes and feedback.</p></div>
<div class="paragraph"><p>This example demonstrates how AI workers can transform customer support operations by automating routine inquiries, ensuring consistent service quality, reducing waiting times, and freeing human agents to focus on complex cases requiring specialized expertise or emotional intelligence.</p></div></aside></div></section><section id="_example_finance_ai_worker"><h2>Example: Finance AI Worker</h2><div class="slide-content"><div class="ulist"><ul><li><p>Automates invoice processing from receipt through payment approval</p></li><li><p>Validates invoice details against purchase orders, contracts, and receiving records</p></li><li><p>Identifies and resolves discrepancies according to established policies</p></li><li><p>Routes exceptions to appropriate personnel with relevant context</p></li><li><p>Reduces processing time by 70% while improving accuracy and compliance</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s explore another concrete example of an AI worker implementation, this time in finance operations:</p></div>
<div class="paragraph"><p>The Finance AI Worker specializes in automating the invoice processing workflow from initial receipt through payment approval. It handles the complete process for straightforward invoices and manages the coordination of human approvals when required, maintaining end-to-end visibility throughout the workflow.</p></div>
<div class="paragraph"><p>This AI worker performs comprehensive validation of invoice details by cross-referencing information against purchase orders, contracts, receiving records, and vendor master data. It verifies line items, quantities, pricing, payment terms, tax calculations, and other critical elements to ensure accuracy before processing continues.</p></div>
<div class="paragraph"><p>When discrepancies are identified—such as price variances, quantity mismatches, or missing references—the AI worker applies established policies to determine appropriate resolution paths. It can automatically resolve minor variations within approved thresholds, initiate standardized exception processes, or compile relevant information for human review.</p></div>
<div class="paragraph"><p>For invoices requiring approval or exception handling, the AI worker routes information to the appropriate personnel based on organizational hierarchy, approval matrices, and delegation rules. Importantly, it provides complete context including highlighted discrepancies, supporting documentation, and historical patterns to facilitate efficient human decision-making.</p></div>
<div class="paragraph"><p>Organizations implementing finance AI workers have reported significant operational improvements, including 70% reductions in processing time, near-elimination of payment errors, enhanced early payment discount capture, improved vendor satisfaction, and stronger compliance with financial controls and policies.</p></div>
<div class="paragraph"><p>This example illustrates how AI workers can transform finance operations by automating routine processing, ensuring consistent application of policies, identifying potential issues proactively, and focusing human attention on exceptions and strategic decisions rather than repetitive validation tasks.</p></div></aside></div></section><section id="_example_recruitment_ai_worker"><h2>Example: Recruitment AI Worker</h2><div class="slide-content"><div class="ulist"><ul><li><p>Screens candidates against job requirements and organizational fit</p></li><li><p>Reviews resumes, applications, and assessment results</p></li><li><p>Prioritizes candidates based on customizable criteria</p></li><li><p>Schedules interviews and coordinates with hiring teams</p></li><li><p>Accelerates talent acquisition by 10x while improving quality of shortlists</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s examine a third example of an AI worker implementation, focusing on recruitment and talent acquisition:</p></div>
<div class="paragraph"><p>The Recruitment AI Worker streamlines the candidate screening process by evaluating applicants against both explicit job requirements and more nuanced organizational fit criteria. It processes applications as they arrive, ensuring consistent evaluation standards and rapid response times regardless of volume fluctuations.</p></div>
<div class="paragraph"><p>This AI worker conducts comprehensive review of candidate materials including resumes, application forms, assessment results, portfolio samples, and public professional profiles. It extracts relevant qualifications, experiences, and indicators of potential success, creating standardized candidate profiles that facilitate objective comparison.</p></div>
<div class="paragraph"><p>Based on configurable evaluation frameworks, the AI worker prioritizes candidates according to multiple dimensions including technical qualifications, relevant experience, growth potential, diversity considerations, and alignment with organizational values. These prioritization models can be customized for different roles and adjusted based on hiring outcomes to improve future recommendations.</p></div>
<div class="paragraph"><p>For candidates meeting threshold criteria, the AI worker manages the interview scheduling process, coordinating availability among candidates and hiring team members, sending calendar invitations, providing preparation materials, and handling rescheduling needs. This automation eliminates the administrative burden that often delays the hiring process.</p></div>
<div class="paragraph"><p>Organizations implementing recruitment AI workers have achieved remarkable efficiency gains, including 10x faster candidate processing, significant improvements in shortlist quality as measured by hiring manager satisfaction, reduced time-to-hire metrics, and more consistent candidate experiences regardless of application volume.</p></div>
<div class="paragraph"><p>This example demonstrates how AI workers can transform recruitment operations by eliminating processing bottlenecks, ensuring consistent and objective evaluation, reducing administrative overhead, and enabling recruiting teams to focus on high-value activities like candidate engagement and hiring manager partnerships.</p></div></aside></div></section></section>
<section><section id="_identifying_ai_worker_opportunities"><h2>Identifying AI Worker Opportunities</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>This section prepares participants for the upcoming three-day workshop by introducing frameworks for identifying business processes that are strong candidates for AI worker implementation. Understanding these evaluation criteria will enable more effective discovery and prioritization during the workshop activities.</p></div></aside></div></section><section id="_process_decomposition_for_ai_implementation"><h2>Process Decomposition for AI Implementation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Break business processes into atomic components before applying AI</p></li><li><p>Identify pattern-recognition tasks ideal for AI automation</p></li><li><p>Recognize contextual reasoning tasks suitable for hybrid AI-human approaches</p></li><li><p>Reserve creative/ethical judgment tasks for human-led work with AI support</p></li><li><p>Most successful implementations automate 70-90% of tasks within jobs, not entire roles</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>One of the most critical success factors for AI worker implementation is effective process decomposition:</p></div>
<div class="paragraph"><p>Breaking business processes into atomic components before applying AI allows organizations to identify precisely where AI can add the most value. This granular approach avoids the common pitfall of trying to automate entire roles or complex processes all at once. Instead, it focuses on understanding the specific tasks and decisions that comprise each process.</p></div>
<div class="paragraph"><p>Through decomposition, organizations can identify pattern-recognition tasks that are ideal candidates for AI automation. These tasks involve analyzing data, recognizing trends, categorizing information, or making predictions based on historical patterns—all areas where AI excels. Examples include document classification, anomaly detection, sentiment analysis, and trend identification.</p></div>
<div class="paragraph"><p>The decomposition also reveals contextual reasoning tasks that benefit from a hybrid AI-human approach. These tasks require understanding broader context, applying judgment within defined parameters, or making decisions based on multiple factors. In these scenarios, AI can perform initial analysis and provide recommendations, with humans making final decisions or handling edge cases.</p></div>
<div class="paragraph"><p>Finally, process decomposition highlights creative and ethical judgment tasks that should remain primarily human-led, though potentially with AI support. These tasks involve novel problem-solving, ethical considerations, emotional intelligence, or strategic thinking where human judgment remains essential. AI can assist by providing information, generating options, or handling routine aspects of these tasks.</p></div>
<div class="paragraph"><p>The most successful AI implementations focus on automating the 70-90% of tasks within jobs that follow predictable patterns, rather than attempting to replace entire roles. This approach maximizes impact while acknowledging the continuing value of human creativity, judgment, and interpersonal skills.</p></div>
<div class="paragraph"><p>During our upcoming workshop, we&#8217;ll apply this decomposition framework to your specific business processes, identifying the components most suitable for AI worker implementation and designing approaches that effectively combine AI and human capabilities.</p></div></aside></div></section><section id="_evaluating_processes_for_ai_worker_potential"><h2>Evaluating Processes for AI Worker Potential</h2><div class="slide-content"><div class="ulist"><ul><li><p>Assess process volume, frequency, and business impact</p></li><li><p>Evaluate current process stability, standardization, and documentation</p></li><li><p>Consider data availability, quality, and accessibility</p></li><li><p>Identify potential integration points with existing systems</p></li><li><p>Analyze return on investment including both direct and indirect benefits</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>When evaluating business processes for AI worker implementation potential, several key factors should be considered:</p></div>
<div class="paragraph"><p>Process volume, frequency, and business impact help determine whether automation will deliver meaningful results. High-volume, frequently performed processes with significant business impact typically offer the greatest return on AI investment. For example, processes handling thousands of transactions daily that directly affect customer experience or financial outcomes are strong candidates.</p></div>
<div class="paragraph"><p>Current process stability, standardization, and documentation indicate how ready a process is for AI implementation. Well-defined, stable processes with clear documentation are easier to automate effectively. Processes with frequent changes, inconsistent execution, or poor documentation may require standardization before AI implementation.</p></div>
<div class="paragraph"><p>Data availability, quality, and accessibility are critical considerations since AI workers depend on information to function effectively. Processes with structured, digitized data that&#8217;s readily accessible typically offer smoother implementation paths. Data quality issues, paper-based information, or siloed systems may require additional preparation work.</p></div>
<div class="paragraph"><p>Integration points with existing systems determine how smoothly an AI worker can operate within your technological ecosystem. Processes that interact with systems offering modern APIs, structured data formats, or browser-based interfaces are generally easier to automate. Legacy systems with limited integration capabilities may present additional challenges.</p></div>
<div class="paragraph"><p>Return on investment analysis should consider both direct benefits (cost savings, throughput improvements, error reduction) and indirect benefits (improved employee experience, enhanced customer satisfaction, better compliance). Comprehensive ROI evaluation helps prioritize implementation efforts for maximum organizational impact.</p></div>
<div class="paragraph"><p>During our workshop, we&#8217;ll use these evaluation criteria to systematically assess potential automation candidates in your organization, developing a prioritized roadmap based on implementation feasibility and expected business impact.</p></div></aside></div></section></section>
<section><section id="_human_ai_collaboration_models"><h2>Human-AI Collaboration Models</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>This final section explores the future of work through the lens of human-AI collaboration, preparing participants to envision and design effective partnership models during the upcoming workshop.</p></div></aside></div></section><section id="_designing_effective_human_ai_teaming_models"><h2>Designing Effective Human-AI Teaming Models</h2><div class="slide-content"><div class="ulist"><ul><li><p>Focus on symbiotic workflows where humans and AI complement each other</p></li><li><p>Leverage AI for data processing, pattern recognition, and routine decision-making</p></li><li><p>Position humans for complex judgment, creativity, and emotional intelligence</p></li><li><p>Implement continuous learning loops where AI improves through human feedback</p></li><li><p>Design workflows that would be impossible with either humans or AI alone</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The most successful AI implementations focus not on replacement but on creating effective human-AI partnerships:</p></div>
<div class="paragraph"><p>Symbiotic workflows represent the ideal state of human-AI collaboration, where each partner contributes their unique strengths to achieve outcomes neither could accomplish alone. This approach moves beyond simplistic automation to reimagine processes that leverage the complementary capabilities of both humans and AI systems.</p></div>
<div class="paragraph"><p>In effective teaming models, AI handles data-intensive tasks where it excels—processing large volumes of information, identifying patterns across diverse datasets, performing consistent analysis, and making routine decisions based on established criteria. This frees human capacity for higher-value activities while ensuring consistent handling of information-intensive tasks.</p></div>
<div class="paragraph"><p>Humans remain essential for complex judgment requiring nuanced understanding, ethical considerations, novel problem-solving, and emotional intelligence. These uniquely human capabilities ensure appropriate handling of edge cases, ethical dilemmas, innovation opportunities, and sensitive interactions where context and values matter deeply.</p></div>
<div class="paragraph"><p>Continuous learning loops create ever-improving systems by capturing human feedback and incorporating it into AI operation. When humans review AI outputs, provide corrections, or handle exceptions, these interactions become learning opportunities that enhance future AI performance. Over time, the AI requires fewer interventions as it adapts to organizational preferences and edge cases.</p></div>
<div class="paragraph"><p>The most advanced implementations create workflows that would be impossible with either humans or AI working independently. For example, AI might analyze thousands of customer interactions to identify emerging issues and present them to human experts who can quickly develop solutions. Neither partner could effectively perform the complete workflow alone—the AI couldn&#8217;t develop the creative solutions, while humans couldn&#8217;t process the volume of interactions to identify patterns.</p></div>
<div class="paragraph"><p>During our workshop, we&#8217;ll explore how to design these symbiotic teaming models for your specific business context, creating collaboration approaches that maximize the strengths of both human employees and AI workers.</p></div></aside></div></section><section id="_transforming_into_an_ai_first_business"><h2>Transforming into an AI-First Business</h2><div class="slide-content"><div class="ulist"><ul><li><p>Implement ruthless process decomposition before applying AI</p></li><li><p>Position data infrastructure as the organizational foundation</p></li><li><p>Design symbiotic human-AI workflows that leverage complementary strengths</p></li><li><p>Focus on augmenting human capabilities rather than replacing roles</p></li><li><p>Create continuous feedback loops that improve AI performance over time</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>As we conclude our introduction and prepare for the detailed work of our upcoming workshop, let&#8217;s consider what it means to transform into a truly AI-first business:</p></div>
<div class="paragraph"><p>Ruthless process decomposition serves as the foundation for effective AI implementation. Before deploying any AI solution, successful organizations break every business process into its atomic components, identifying which specific tasks are suitable for AI automation, which benefit from hybrid approaches, and which should remain primarily human-led. This granular understanding prevents the common pitfall of attempting to automate entire jobs rather than focusing on the 70-90% of tasks within those jobs that follow predictable patterns.</p></div>
<div class="paragraph"><p>In AI-first organizations, data infrastructure becomes the organizational foundation rather than merely supporting existing structures. These companies architect their entire operations around their data assets, with every decision—from hiring to product development—evaluated based on how it will improve data positioning. This approach recognizes that the quality, accessibility, and organization of data directly determine the effectiveness of AI implementations.</p></div>
<div class="paragraph"><p>Symbiotic human-AI workflows represent the operational model of AI-first businesses. Rather than viewing AI merely as a cost-reduction tool, these organizations create collaborative processes that would be impossible with either humans or AI working independently. These workflows leverage the complementary strengths of both partners—AI&#8217;s ability to process vast amounts of information and identify patterns, combined with human creativity, judgment, and emotional intelligence.</p></div>
<div class="paragraph"><p>The focus on augmenting human capabilities rather than replacing roles distinguishes truly transformative AI implementations. By automating routine aspects of jobs, AI-first organizations enable their employees to focus on higher-value activities that leverage uniquely human strengths. This approach not only improves operational efficiency but also enhances employee satisfaction and engagement by eliminating tedious tasks.</p></div>
<div class="paragraph"><p>Finally, AI-first organizations implement continuous feedback loops that improve AI performance over time. They create systems where humans teach AI through their responses to recommendations, corrections to outputs, and handling of exceptions. This creates a virtuous cycle where the AI system requires fewer interventions over time as it adapts to organizational needs and preferences.</p></div>
<div class="paragraph"><p>During our upcoming workshop, we&#8217;ll explore how to apply these principles to your specific business context, creating a roadmap for transforming into an AI-first organization that leverages the full potential of human-AI collaboration.</p></div></aside></div></section></section>
<section><section id="_summary"><h2>Summary</h2></section><section id="_key_takeaways"><h2>Key Takeaways</h2><div class="slide-content"><div class="ulist"><ul><li><p>AI agents represent a significant advancement over rule-based automation</p></li><li><p>AI workers function as knowledge workers with analyze-understand-decide-execute-learn capabilities</p></li><li><p>Effective implementation requires understanding AI architecture components and their relationships</p></li><li><p>Trust in autonomous AI depends on knowledge integration and appropriate control mechanisms</p></li><li><p>Human-AI teaming models create outcomes impossible for either alone</p></li><li><p>Process decomposition is critical for identifying suitable automation candidates</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>As we conclude our introduction to AI agents, let&#8217;s recap the key concepts we&#8217;ve explored today:</p></div>
<div class="paragraph"><p>AI agents represent a fundamental advancement beyond traditional rule-based automation approaches like RPA. Unlike their predecessors, AI agents can adapt to changing conditions, make autonomous decisions, and focus on outcomes rather than just processes. This shift enables automation of complex knowledge work previously considered too nuanced for traditional approaches.</p></div>
<div class="paragraph"><p>AI workers function as true knowledge workers, with capabilities that mirror human cognitive processes: analyzing information, understanding context, making decisions, executing actions, and learning from experience. This comprehensive capability set allows AI workers to handle sophisticated tasks requiring judgment and adaptation.</p></div>
<div class="paragraph"><p>Implementing effective AI workers requires understanding the core architectural components—instructions, knowledge, actions, language models, and integration layers—and how they work together to enable autonomous task completion. This architectural understanding is essential for designing solutions that deliver on the promise of AI-powered automation.</p></div>
<div class="paragraph"><p>Building trust in autonomous AI systems depends on both comprehensive knowledge integration and appropriate control mechanisms. AI workers must have access to organizational information and context while operating within clearly defined boundaries that ensure alignment with business objectives and values.</p></div>
<div class="paragraph"><p>The most successful implementations focus not on replacing humans but on creating effective human-AI teaming models that leverage the complementary strengths of both partners. These symbiotic workflows enable outcomes that would be impossible for either humans or</p></div></aside></div></section></section>
<section><section id="_quiz_time"><h2>Quiz Time!</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>Now let&#8217;s test our understanding of the key concepts we&#8217;ve covered in today&#8217;s session with a brief quiz. This will help reinforce the important points before we move into our three-day workshop on discovering business processes for AI agent automation.</p></div></aside></div></section><section id="_question_1"><h2>Question 1</h2><div class="slide-content"><div class="paragraph"><p>What is the primary difference between RPA and AI agents?</p></div>
<div class="ulist"><ul><li><p>A) RPA is more expensive to implement than AI agents</p></li><li><p>B) RPA follows rule-based automation while AI agents can adapt and make autonomous decisions</p></li><li><p>C) AI agents can only work with structured data</p></li><li><p>D) RPA is faster at executing tasks than AI agents</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is B. RPA follows rule-based automation while AI agents can adapt and make autonomous decisions. This represents a fundamental difference in capability - RPA executes predefined processes without deviation, while AI agents can understand context, adapt to changing conditions, and autonomously determine the best approach to achieve desired outcomes.</p></div></aside></div></section><section id="_question_2"><h2>Question 2</h2><div class="slide-content"><div class="paragraph"><p>Which of the following represents the most advanced implementation of AI assistance?</p></div>
<div class="ulist"><ul><li><p>A) AI Assistants</p></li><li><p>B) AI Agents</p></li><li><p>C) AI Workers</p></li><li><p>D) AI Chatbots</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is C. AI Workers represent the most advanced implementation, utilizing complex workflows and sophisticated reasoning to deliver specific outcomes with minimal human input. They can understand context, make judgments, and adapt their approach to achieve desired results, performing complete business functions previously reserved for human knowledge workers.</p></div></aside></div></section><section id="_question_3"><h2>Question 3</h2><div class="slide-content"><div class="paragraph"><p>What are the five core capabilities of AI knowledge workers?</p></div>
<div class="ulist"><ul><li><p>A) Plan, Program, Process, Predict, Perform</p></li><li><p>B) Collect, Compute, Create, Communicate, Complete</p></li><li><p>C) Analyze, Understand, Decide, Execute, Learn</p></li><li><p>D) Search, Sort, Summarize, Suggest, Solve</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is C. The five core capabilities of AI knowledge workers are: Analyze (navigate, read, calculate, extract information), Understand (interpret, summarize, compare data and contexts), Decide (weigh options, make judgments, choose appropriate actions), Execute (click, write, post, convert, update information and systems), and Learn (adapt approaches, improve performance over time).</p></div></aside></div></section><section id="_question_4"><h2>Question 4</h2><div class="slide-content"><div class="paragraph"><p>Which of these is NOT a core component of an AI Agent Architecture?</p></div>
<div class="ulist"><ul><li><p>A) Instructions</p></li><li><p>B) Knowledge</p></li><li><p>C) Social intelligence</p></li><li><p>D) Large Language Model(s)</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is C. Social intelligence is not listed as one of the core components of an AI Agent Architecture. The core components we discussed are: Instructions (prompts and guidance), Knowledge (relevant data and insights), Actions (capabilities to interact with systems), Large Language Model(s) (the reasoning engine), and Integration Layer (connections to enterprise systems).</p></div></aside></div></section><section id="_question_5"><h2>Question 5</h2><div class="slide-content"><div class="paragraph"><p>What is a key requirement for trusting autonomous AI in business environments?</p></div>
<div class="ulist"><ul><li><p>A) AI must always operate completely independently without human oversight</p></li><li><p>B) AI needs comprehensive understanding of organizational knowledge and standards</p></li><li><p>C) AI should prioritize speed over accuracy</p></li><li><p>D) AI must be developed in-house rather than using vendor solutions</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is B. AI needs comprehensive understanding of organizational knowledge and standards to be trusted with autonomous operation. This ensures the AI operates appropriately within the business context, following relevant policies, procedures, and best practices. Other requirements include visibility into AI reasoning, control mechanisms, respecting existing permissions, and continuous monitoring.</p></div></aside></div></section><section id="_question_6"><h2>Question 6</h2><div class="slide-content"><div class="paragraph"><p>What percentage of tasks within jobs can typically be automated with properly implemented AI agents?</p></div>
<div class="ulist"><ul><li><p>A) 20-30%</p></li><li><p>B) 40-50%</p></li><li><p>C) 70-90%</p></li><li><p>D) 100%</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is C. Research and real-world implementations have shown that properly designed AI agents can automate 70-90% of tasks within jobs that were previously considered too complex for automation. This doesn&#8217;t necessarily mean replacing jobs entirely, but rather augmenting human workers by handling routine aspects of their work.</p></div></aside></div></section><section id="_question_7"><h2>Question 7</h2><div class="slide-content"><div class="paragraph"><p>What is the recommended approach for identifying AI worker opportunities in business processes?</p></div>
<div class="ulist"><ul><li><p>A) Automate entire departments at once</p></li><li><p>B) Focus only on customer-facing processes</p></li><li><p>C) Break business processes into atomic components before applying AI</p></li><li><p>D) Implement AI only in processes that are already fully digital</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is C. Breaking business processes into atomic components before applying AI is the recommended approach. This process decomposition allows organizations to identify precisely where AI can add the most value, focusing on specific tasks and decisions rather than trying to automate entire roles or complex processes all at once.</p></div></aside></div></section><section id="_question_8"><h2>Question 8</h2><div class="slide-content"><div class="paragraph"><p>In effective human-AI teaming models, what role should humans typically play?</p></div>
<div class="ulist"><ul><li><p>A) Humans should only supervise AI decisions</p></li><li><p>B) Humans should focus on complex judgment, creativity, and emotional intelligence</p></li><li><p>C) Humans should primarily handle data entry and routine tasks</p></li><li><p>D) Humans should be removed from all processes where AI is implemented</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The correct answer is B. In effective human-AI teaming models, humans should focus on complex judgment requiring nuanced understanding, ethical considerations, novel problem-solving, and emotional intelligence. These uniquely human capabilities ensure appropriate handling of edge cases, ethical dilemmas, innovation opportunities, and sensitive interactions where context and values matter deeply.</p></div></aside></div></section></section>
<section><section id="_thank_you"><h2>Thank You!</h2></section><section id="_next_steps"><h2>Next Steps</h2><div class="slide-content"><div class="ulist"><ul><li><p>Tomorrow we begin our three-day workshop on discovering business processes for AI agent automation</p></li><li><p>Please come prepared with specific processes from your department to analyze</p></li><li><p>We&#8217;ll apply the frameworks and concepts from today to identify high-value automation opportunities</p></li><li><p>Questions? Contact the workshop facilitator at [<a href="mailto:facilitator@example.com">facilitator@example.com</a>]</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Thank you for your participation in today&#8217;s introduction to AI agents. This session has laid the groundwork for our upcoming three-day workshop, where we&#8217;ll focus on discovering business processes in your organization that are candidates for automation using AI agents.</p></div>
<div class="paragraph"><p>Please come prepared tomorrow with specific processes from your department that you&#8217;d like to analyze. We&#8217;ll apply the frameworks and concepts we&#8217;ve discussed today to identify high-value automation opportunities tailored to your organization&#8217;s needs.</p></div>
<div class="paragraph"><p>If you have any questions before tomorrow&#8217;s session, please don&#8217;t hesitate to contact the workshop facilitator. We look forward to working with you to explore the transformative potential of AI agents in your business operations.</p></div></aside></div></section></section>
<section><section id="_running_and_monitoring_workloads_in_cockroachdb"><h2>Running and Monitoring Workloads in CockroachDB</h2></section><section id="_why_workload_monitoring_matters"><h2>Why Workload Monitoring Matters</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Ensures optimal performance</p></li><li><p>Identifies potential bottlenecks</p></li><li><p>Facilitates capacity planning</p></li><li><p>Enables proactive maintenance</p></li><li><p>Validates system health</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Workload monitoring is essential for <strong>maintaining and tuning cluster
performance</strong>. It provides real-time visibility into query throughput, resource
usage, and latency. By identifying bottlenecks early, you can scale hardware or
adjust configurations proactively instead of reacting to an outage. This
practice also supports <strong>capacity planning</strong>: knowing when to add nodes or adjust
replication settings to keep up with growth. Finally, consistent monitoring
validates that the system is healthy enough to meet <strong>SLAs</strong> or internal
performance goals.</p></div></aside></div></section><section id="_understanding_workloads"><h2>Understanding Workloads</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Built-in workload generators</p></li><li><p>Configurable parameters</p></li><li><p>Realistic data patterns</p></li><li><p>Performance testing capability</p></li><li><p>Load simulation tools</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB ships with several built-in workload generators (e.g., <code>movr</code>,
<code>bank</code>, <code>tpcc</code>, <code>kv</code>) to help you <strong>simulate real-world usage</strong> and stress test
your cluster. For instance, the <code>movr</code> workload simulates a ride-sharing or
vehicle-sharing application, including multiple tables (users, vehicles, rides)
with realistic transaction patterns. Parameters such as <strong>duration</strong>,
<strong>concurrency</strong>, and <strong>connection strings</strong> can be tailored to mimic actual
deployment scenarios.</p></div>
<div class="paragraph"><p>Example for initializing the <code>movr</code> workload:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-bash hljs" data-noescape="true" data-lang="bash">cockroach workload init movr 'postgresql://root@haproxy:26257?sslmode=disable'</code></pre></div></div>
<div class="paragraph"><p>This creates the <code>movr</code> schema and populates it with seed data. Adjust the
connection string if you’re using TLS or certificate-based authentication.</p></div></aside></div></section><section id="_db_console_overview"><h2>DB Console Overview</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Real-time performance metrics</p></li><li><p>SQL activity monitoring</p></li><li><p>Hardware resource tracking</p></li><li><p>Database size analysis</p></li><li><p>Custom time range selection</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The <strong>DB Console</strong> is the primary monitoring interface for CockroachDB. After
visiting <code><a href="http://&lt;node-address&gt;:8080" class="bare">http://&lt;node-address&gt;:8080</a></code>, you can explore:</p></div>
<div class="ulist"><ul><li><p><strong>Metrics Page:</strong> CPU, memory, disk I/O, and SQL-specific stats.</p></li><li><p><strong>SQL Activity / Statements Page:</strong> Shows queries being executed, their frequency, and latency distribution.</p></li><li><p><strong>Hardware/Nodes Page:</strong> Detailed view of node status, storage usage, and localities.</p></li><li><p><strong>Database/Schema Page:</strong> Summaries of database objects (tables, indexes, schemas) and their sizes.</p></li><li><p><strong>Time Range Filter:</strong> Zoom in on a specific window to investigate spikes or anomalies.</p></li></ul></div>
<div class="paragraph"><p>This console is updated in near real time, making it a powerful tool for diagnosing performance issues under load.</p></div></aside></div></section><section id="_performance_metrics"><h2>Performance Metrics</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>CPU utilization tracking</p></li><li><p>SQL statement analysis</p></li><li><p>Transaction throughput</p></li><li><p>Storage utilization</p></li><li><p>Query performance statistics</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Important performance metrics to watch:</p></div>
<div class="ulist"><ul><li><p><strong>CPU Utilization:</strong> High CPU usage could mean the nodes are under heavy load or queries are computationally expensive.</p></li><li><p><strong>SQL Statement Analysis:</strong> The DB Console can show <strong>top slow statements</strong>, letting you pinpoint queries causing bottlenecks.</p></li><li><p><strong>Transaction Throughput (TPS/QPS):</strong> Track how many transactions or queries per second your cluster can handle.</p></li><li><p><strong>Storage Utilization:</strong> Monitor disk usage to avoid running out of space or encountering slow writes as disks near capacity.</p></li><li><p><strong>Query Latency Stats:</strong> Latency percentile charts (p50, p95, p99) help identify occasional slow queries that may affect user experience.</p></li></ul></div>
<div class="paragraph"><p>Review these metrics regularly or integrate them into external monitoring systems (e.g., Prometheus, Datadog, Grafana) for longer historical views and alerting.</p></div></aside></div></section><section id="_workload_management"><h2>Workload Management</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Controlled load generation</p></li><li><p>Duration configuration</p></li><li><p>Load balancer integration</p></li><li><p>Performance validation</p></li><li><p>Resource utilization monitoring</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Managing workloads involves tailoring parameters to represent realistic usage. You can specify concurrency, duration, or data scale:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-bash hljs" data-noescape="true" data-lang="bash">cockroach workload run movr --duration=1m \
    'postgresql://root@haproxy:26257?sslmode=disable'</code></pre></div></div>
<div class="paragraph"><p>This example runs the <code>movr</code> workload for <strong>1 minute</strong>, passing traffic through an HAProxy load balancer. During the test, you can:</p></div>
<div class="ulist"><ul><li><p><strong>Validate performance</strong> by checking latency and throughput in the DB Console.</p></li><li><p><strong>Monitor Resource Usage</strong> (CPU, memory) to ensure no single node is overloaded.</p></li><li><p><strong>Spot Slow Queries</strong> or potential hotspots in tables or indexes.</p></li><li><p><strong>Confirm Scalability</strong> by adjusting concurrency or adding more nodes to see if the cluster handles increased load effectively.</p></li></ul></div></aside></div></section><section id="_exercise_overview"><h2>Exercise Overview</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Initialize sample workload</p></li><li><p>Configure test parameters</p></li><li><p>Monitor performance metrics</p></li><li><p>Analyze system behavior</p></li><li><p>Validate cluster health</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the upcoming exercise, you’ll:</p></div>
<div class="ulist"><ul><li><p><strong>Initialize</strong> the <code>movr</code> workload in your cluster to create relevant tables and seed data.</p></li><li><p><strong>Run Configurable Tests</strong> using different durations and concurrency settings, possibly via a load balancer like HAProxy.</p></li><li><p><strong>Monitor</strong> the cluster in real-time with the DB Console, focusing on <strong>Statements</strong>, <strong>Metrics</strong>, and <strong>Nodes</strong> pages.</p></li><li><p><strong>Analyze</strong> throughput, latency, and resource usage to understand how your cluster handles various levels of demand.</p></li><li><p><strong>Validate</strong> that the database remains healthy, queries complete successfully, and no severe bottlenecks or errors emerge.</p></li></ul></div>
<div class="paragraph"><p>By the end of this exercise, you’ll have hands-on knowledge of how to run workloads against a CockroachDB cluster and systematically monitor performance and capacity.</p></div></aside></div></section></section>
<section><section id="_garbage_collection_and_ttl_in_cockroachdb"><h2>Garbage Collection and TTL in CockroachDB</h2></section><section id="_why_data_cleanup_matters"><h2>Why Data Cleanup Matters</h2><div class="slide-content"><div class="ulist"><ul><li><p>Prevents unbounded storage growth</p></li><li><p>Maintains consistent performance</p></li><li><p>Supports MVCC functionality</p></li><li><p>Optimizes resource utilization</p></li><li><p>Reduces operational costs</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Garbage collection is a fundamental component of CockroachDB’s architecture. Because the database uses MVCC, multiple versions of the same row can accumulate over time. Without garbage collection, this data would grow indefinitely, affecting query performance and increasing storage costs. GC reclaims storage by removing obsolete row versions once they are no longer needed by active transactions or time-travel queries.</p></div>
<div class="paragraph"><p>Maintaining an optimal GC policy directly impacts:</p></div>
<div class="ulist"><ul><li><p><strong>Performance</strong>: The presence of outdated row versions can slow down queries.</p></li><li><p><strong>Storage Efficiency</strong>: Failing to remove stale versions leads to unnecessary data bloat and costs.</p></li><li><p><strong>System Health</strong>: Proper cleanup ensures system resources (like memory and disk) are effectively utilized.</p></li></ul></div>
<div class="paragraph"><p>For example, if a table receives frequent updates, rows quickly accumulate historical versions. With a well-configured GC process, these old versions will be automatically discarded after a suitable retention period, preserving performance and stability.</p></div></aside></div></section><section id="_multi_version_concurrency_control_mvcc"><h2>Multi-Version Concurrency Control (MVCC)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Maintains multiple versions of data</p></li><li><p>Enables lock-free reads</p></li><li><p>Supports serializable transactions</p></li><li><p>Requires version cleanup</p></li><li><p>Impacts storage utilization</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB uses MVCC to allow simultaneous read and write operations without blocking. When a transaction updates a row, the database creates a new version tagged with the transaction’s commit timestamp, while older versions remain for readers that might need them. This mechanism supports serializable isolation without requiring explicit read locks, improving concurrency.</p></div>
<div class="paragraph"><p>However, each new version consumes storage until it is cleaned up. Over time, these versions accumulate, making regular garbage collection essential to reclaim space from out-of-date rows. If GC is not tuned correctly, tables with frequent writes can become bloated, affecting performance and eventually leading to excessive disk usage.</p></div>
<div class="paragraph"><p><strong>Technical Example:</strong> A long-running analytical transaction may read older row versions while short transactions continue to update the same rows. MVCC ensures the analytical query sees consistent data at its start timestamp. GC will only remove older row versions that are not needed by any active transaction or potential <strong>AS OF SYSTEM TIME</strong> query.</p></div></aside></div></section><section id="_understanding_gc_ttlseconds"><h2>Understanding gc.ttlseconds</h2><div class="slide-content"><div class="ulist"><ul><li><p>Controls retention period for old versions</p></li><li><p>Affects cleanup scheduling</p></li><li><p>Balances storage vs functionality</p></li><li><p>Defaults to 4 hours (14400 seconds)</p></li><li><p>Configurable per database/table</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The <strong>gc.ttlseconds</strong> parameter defines how long outdated row versions are kept before they become eligible for garbage collection. The default of <strong>14400 seconds</strong> (4 hours) accommodates most workloads by providing enough time for long-running queries or backups to reference historical data.</p></div>
<div class="paragraph"><p>You can configure this setting at the table level via zone configurations. For example, to set <strong>gc.ttlseconds</strong> to 7200 (2 hours) for a table named <strong>users</strong>:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE users
CONFIGURE ZONE USING gc.ttlseconds = 7200;

SHOW ZONE CONFIGURATION FOR TABLE users;</code></pre></div></div>
<div class="paragraph"><p>Adjusting this value requires careful consideration of:</p></div>
<div class="ulist"><ul><li><p><strong>Transaction Durations</strong>: Long-running transactions may need older versions for reads.</p></li><li><p><strong>Backup and Recovery Strategy</strong>: Longer retention might be necessary to allow for point-in-time recovery or historical queries.</p></li><li><p><strong>Storage Costs</strong>: Holding onto old versions for too long increases disk usage, so striking a balance is critical.</p></li></ul></div></aside></div></section><section id="_storage_impact_of_database_operations"><h2>Storage Impact of Database Operations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Index creation doubles storage temporarily</p></li><li><p>Deleted data remains until GC</p></li><li><p>DDL operations affect space usage</p></li><li><p>Background cleanup is automatic</p></li><li><p>Monitoring is essential</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Different database operations can significantly affect storage consumption:</p></div>
<div class="ulist"><ul><li><p><strong>Index Creation</strong>: Building a new index involves backfilling data, which temporarily increases storage usage.</p></li><li><p><strong>Delete Operations</strong>: When you delete rows, CockroachDB marks those row versions as deleted, but they are not physically removed until GC. This is why immediate space savings may not be seen right after large deletions.</p></li><li><p><strong>DDL Changes</strong>: Dropping columns or tables leaves behind obsolete metadata and row versions that remain until GC processes them.</p></li></ul></div>
<div class="paragraph"><p>CockroachDB manages these cleanups automatically, but monitoring is crucial to understand if changes in workload or large DDL operations create storage spikes. If you notice sustained high storage usage, verify that GC is running effectively and is appropriately configured.</p></div></aside></div></section><section id="_monitoring_garbage_collection"><h2>Monitoring Garbage Collection</h2><div class="slide-content"><div class="ulist"><ul><li><p>Track storage usage metrics</p></li><li><p>Observe cleanup timing</p></li><li><p>Monitor GC process</p></li><li><p>Verify space reclamation</p></li><li><p>Check system impact</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Monitoring GC behavior helps validate that older versions are being cleaned up in a timely manner. You can track:</p></div>
<div class="ulist"><ul><li><p><strong>DB Console Metrics</strong>: Review storage usage graphs to see whether disk consumption decreases after rows are deleted or after large data modifications.</p></li><li><p><strong>GC-Related Metrics</strong>: Look for specific GC counters (e.g., how many keys are awaiting GC).</p></li><li><p><strong>System Impact</strong>: Observe CPU and memory usage during GC processes to ensure that cleanup jobs do not degrade overall performance.</p></li><li><p><strong>SHOW JOBS Output</strong>: Some GC processes or large-scale schema changes appear as background jobs in CockroachDB. If these jobs hang, older versions might persist.</p></li></ul></div>
<div class="paragraph"><p>Regular reviews of these metrics confirm whether your <strong>gc.ttlseconds</strong> values are appropriate for the workload or need adjustments.</p></div></aside></div></section><section id="_best_practices_for_production"><h2>Best Practices for Production</h2><div class="slide-content"><div class="ulist"><ul><li><p>Maintain default gc.ttlseconds when possible</p></li><li><p>Consider workload patterns</p></li><li><p>Plan for storage fluctuations</p></li><li><p>Monitor cleanup effectiveness</p></li><li><p>Account for backup requirements</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Default settings often balance stability and space efficiency, so it’s advisable to keep the <strong>gc.ttlseconds</strong> at <strong>14400</strong> unless your workload dictates otherwise. Important considerations include:</p></div>
<div class="ulist"><ul><li><p><strong>Workload Profile</strong>: High write throughput or frequent schema changes might require more proactive monitoring or higher TTL to accommodate longer transactions.</p></li><li><p><strong>Backup Windows</strong>: If your backups rely on consistent snapshots, ensure the TTL is sufficient to include all versions needed for those backups.</p></li><li><p><strong>Peak Usage Planning</strong>: During major migrations or index backfills, storage usage may spike. Ensure you have capacity for these temporary surges.</p></li><li><p><strong>Regular Monitoring</strong>: Continuously check that GC is removing old row versions as expected. If you see unexpectedly high disk usage, investigate whether GC or user-defined zone configurations need tuning.</p></li></ul></div></aside></div></section><section id="_summary_2"><h2>Summary</h2><div class="slide-content"><div class="ulist"><ul><li><p>GC maintains storage efficiency</p></li><li><p>MVCC requires version cleanup</p></li><li><p>Settings balance needs</p></li><li><p>Operations impact storage</p></li><li><p>Monitoring ensures effectiveness</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Garbage collection is indispensable for controlling storage usage in CockroachDB’s MVCC model.
By removing obsolete row versions, GC maintains performance and prevents unbounded data growth.</p></div>
<div class="paragraph"><p>The <strong>gc.ttlseconds</strong> configuration governs how long these versions persist, offering a balance between operational needs (long-running queries, backups) and storage efficiency.</p></div>
<div class="paragraph"><p>Understanding and monitoring the GC lifecycle ensures your CockroachDB clusters remain performant and cost-effective.</p></div></aside></div></section><section id="_exercise_preview"><h2>Exercise Preview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Configure GC settings</p></li><li><p>Observe storage impact</p></li><li><p>Create and drop indexes</p></li><li><p>Monitor cleanup process</p></li><li><p>Verify space reclamation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the upcoming exercise, you’ll:</p></div>
<div class="ulist"><ul><li><p>Adjust <strong>gc.ttlseconds</strong> on a specific table and observe how it influences the retention of old row versions.</p></li><li><p>Perform index creation and deletion to monitor how storage usage spikes, then recedes as GC runs.</p></li><li><p>Examine metrics via the DB Console or SQL commands, such as <code>SHOW JOBS</code>, to confirm GC is reclaiming space.</p></li><li><p>Gain practical experience in tuning and validating garbage collection in a real CockroachDB environment.</p></li></ul></div></aside></div></section></section>
<section><section id="_write_intents_in_cockroachdb"><h2>Write Intents in CockroachDB</h2></section><section id="_understanding_write_intents"><h2>Understanding Write Intents</h2><div class="slide-content"><div class="ulist"><ul><li><p>Temporary records created during transactions</p></li><li><p>Essential for MVCC and transaction isolation</p></li><li><p>Impact concurrent access and performance</p></li><li><p>Require cleanup after transaction completion</p></li><li><p>Affect cluster storage utilization</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Write intents are temporary markers placed on data when a transaction modifies rows but has not yet committed. These markers ensure transactional integrity by allowing <strong>other transactions</strong> or <strong>reads</strong> to detect that the data is currently being changed. In CockroachDB, write intents are tied to MVCC, which manages multiple versions of rows.</p></div>
<div class="paragraph"><p>Key points to understand:</p></div>
<div class="ulist"><ul><li><p><strong>Isolation:</strong> Write intents serve to isolate in-flight transactions, preventing others from reading or modifying the data in conflicting ways.</p></li><li><p><strong>Visibility:</strong> Other transactions see that the row is “locked” pending a commit or rollback. Reads may wait or retry if they encounter these intents, depending on the transaction isolation level.</p></li><li><p><strong>Cleanup:</strong> Once a transaction completes (commits or aborts), its write intents are either <strong>promoted</strong> to committed data or <strong>discarded</strong> if the transaction is rolled back.</p></li><li><p><strong>Performance Impact:</strong> Excessive write intents—especially under a high write load—can create contention, lengthen transaction retries, and strain the system’s storage and CPU resources.</p></li></ul></div></aside></div></section><section id="_write_intent_lifecycle"><h2>Write Intent Lifecycle</h2><div class="slide-content"><div class="ulist"><ul><li><p>Created at transaction start</p></li><li><p>Block conflicting operations</p></li><li><p>Persist until transaction commits/aborts</p></li><li><p>Require cleanup resources</p></li><li><p>Impact concurrent transactions</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The lifecycle of a write intent proceeds through distinct phases:</p></div>
<div class="ulist"><ul><li><p><strong>Creation:</strong> As soon as a transaction writes to a row, CockroachDB places a write intent at that row’s latest MVCC version, marking the data as “in flux.”</p></li><li><p><strong>Blocking:</strong> Operations that conflict with the intent (i.e., other writes to the same row) may be forced to wait or retry, ensuring serializable isolation. Lock-free reads can still proceed on older committed versions, though they might see stale data relative to the in-flight transaction.</p></li><li><p><strong>Persistence:</strong> Write intents are durable in the underlying key-value store. If the node crashes, these intents remain, awaiting resolution once the transaction is recovered or timed out.</p></li><li><p><strong>Resolution (Cleanup):</strong> On commit, the intent is “resolved,” promoting the row to a fully committed version. On rollback (abort), the system discards the write. This cleanup process uses additional resources and can impact performance if large numbers of intents need resolution.</p></li><li><p><strong>Concurrency Impact:</strong> While the write intent remains, concurrent transactions can be slowed or retried, potentially increasing overall cluster latency if not managed effectively.</p></li></ul></div></aside></div></section><section id="_impact_of_write_intent_buildup"><h2>Impact of Write Intent Buildup</h2><div class="slide-content"><div class="ulist"><ul><li><p>Increased transaction conflicts</p></li><li><p>Higher storage consumption</p></li><li><p>Extended cleanup overhead</p></li><li><p>Reduced system throughput</p></li><li><p>Degraded query performance</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>A buildup of unresolved write intents can create bottlenecks:</p></div>
<div class="ulist"><ul><li><p><strong>Transaction Conflicts:</strong> Multiple transactions attempting to modify the same rows frequently encounter each other’s intents, increasing the likelihood of restarts or backoffs.</p></li><li><p><strong>Storage Overhead:</strong> Each intent adds an additional version record. A large volume of uncommitted changes spikes storage usage and can put pressure on system resources like memory and disk.</p></li><li><p><strong>Cleanup Costs:</strong> The background process that resolves intents must handle more records, consuming CPU and I/O. Under heavy write loads, this cost can become significant.</p></li><li><p><strong>Performance Degradation:</strong> Longer resolution times mean reads and writes experience delays. If too many transactions produce lingering intents, the cluster’s throughput is reduced.</p></li></ul></div></aside></div></section><section id="_monitoring_write_intents"><h2>Monitoring Write Intents</h2><div class="slide-content"><div class="ulist"><ul><li><p>Track active transactions</p></li><li><p>Measure intent counts</p></li><li><p>Observe cleanup duration</p></li><li><p>Monitor system impact</p></li><li><p>Analyze performance patterns</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Effective monitoring of write intents is vital to ensure healthy cluster operation:</p></div>
<div class="ulist"><ul><li><p><strong>Active Transaction Inspection:</strong> Use queries or the DB Console to see how many transactions are currently open and what they’re modifying.</p></li><li><p><strong>Intent Counts:</strong> Look for metrics that specifically track the number of outstanding write intents or “locks.” High or steadily increasing counts may signal a problem.</p></li><li><p><strong>Cleanup Duration:</strong> Observe how long it takes the system to resolve intents after commit or rollback. Prolonged cleanup times can indicate contention or insufficient resources.</p></li><li><p><strong>System-Wide Metrics:</strong> CPU, memory, and disk I/O usage can all be affected by high-intent workloads. If these metrics spike without corresponding throughput increases, write intents could be at fault.</p></li><li><p><strong>Performance Profiling:</strong> Use logs, traces, or built-in instrumentation to analyze how transactions and batch operations behave under load. Identify patterns, such as frequent retries or slow commits, that point to excessive write intent activity.</p></li></ul></div></aside></div></section><section id="_batch_processing_strategies"><h2>Batch Processing Strategies</h2><div class="slide-content"><div class="ulist"><ul><li><p>Size batches appropriately</p></li><li><p>Control transaction scope</p></li><li><p>Balance throughput vs overhead</p></li><li><p>Allow concurrent operations</p></li><li><p>Minimize cleanup impact</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Batching in CockroachDB means grouping multiple statements into a single transaction. This approach can improve throughput but may lead to a large number of write intents if transactions are too big:</p></div>
<div class="ulist"><ul><li><p><strong>Appropriate Batch Sizing:</strong> If a batch is too large, it may hold onto many write intents for an extended period, increasing conflict potential. If it’s too small, overhead from transaction setup and commit could reduce throughput. Finding a sweet spot is key.</p></li><li><p><strong>Transaction Scope Control:</strong> Make sure each transaction’s changes are related. Combining unrelated operations into the same transaction increases the chance of contention and complicated rollbacks.</p></li><li><p><strong>Throughput vs. Overhead:</strong> Large batches can boost throughput but risk greater conflict and bigger cleanup tasks. Smaller batches reduce concurrency conflicts but might not fully utilize the cluster’s capacity.</p></li><li><p><strong>Concurrency Allowance:</strong> By running multiple moderately sized transactions in parallel, the cluster can better distribute load across nodes, limiting the buildup of any single transaction’s write intents.</p></li></ul></div></aside></div></section><section id="_performance_optimization_tips"><h2>Performance Optimization Tips</h2><div class="slide-content"><div class="ulist"><ul><li><p>Limit transaction size</p></li><li><p>Use appropriate batch sizes</p></li><li><p>Schedule large updates carefully</p></li><li><p>Monitor system metrics</p></li><li><p>Plan for cleanup overhead</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>When designing workloads in CockroachDB, consider:</p></div>
<div class="ulist"><ul><li><p><strong>Limiting Transaction Size:</strong> Smaller, well-defined transactions create fewer write intents, reducing the likelihood of large-scale rollbacks and making cleanup faster.</p></li><li><p><strong>Batch Size Tuning:</strong> Use trial and error and workload analysis to find an optimal batch size. Too large can cause lock contention; too small can hamper throughput.</p></li><li><p><strong>Scheduling Large Updates:</strong> Perform major migrations or bulk updates during off-peak hours to minimize conflict with regular workloads and to allow more time for intent resolution.</p></li><li><p><strong>Metric Monitoring:</strong> Keep an eye on CPU usage, I/O, and the count of open transactions. Spikes can indicate concurrency issues linked to write intents.</p></li><li><p><strong>Cleanup Overhead Planning:</strong> Recognize that cleanup tasks (resolving many write intents) can consume cluster resources. Factor this into your operational plan, ensuring you allocate enough capacity.</p></li></ul></div></aside></div></section><section id="_summary_3"><h2>Summary</h2><div class="slide-content"><div class="ulist"><ul><li><p>Write intents enable transactions</p></li><li><p>Buildup impacts performance</p></li><li><p>Monitoring is essential</p></li><li><p>Batching improves efficiency</p></li><li><p>Size affects cleanup time</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Write intents form the backbone of CockroachDB’s transactional consistency model, but they must be carefully managed to avoid bottlenecks:</p></div>
<div class="ulist"><ul><li><p><strong>Essential Transaction Component:</strong> They preserve isolation across concurrent operations and keep data consistent.</p></li><li><p><strong>Performance Trade-Off:</strong> Accumulated write intents can degrade performance, requiring thoughtful transaction design and cleanup processes.</p></li><li><p><strong>Monitoring &amp; Batching:</strong> By keeping watch on system metrics and using well-tuned batch sizes, you can keep write intents from overwhelming the cluster.</p></li><li><p><strong>Impact on Cleanup:</strong> The more writes left “in limbo,” the more overhead is needed to resolve them, so controlling transaction scope and timing is key to smooth operations.</p></li></ul></div></aside></div></section><section id="_exercise_preview_2"><h2>Exercise Preview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Monitor write intent metrics</p></li><li><p>Compare batch strategies</p></li><li><p>Observe cleanup patterns</p></li><li><p>Measure system impact</p></li><li><p>Optimize update operations</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the upcoming exercise, you will:</p></div>
<div class="ulist"><ul><li><p>Examine the volume of write intents generated by different batch sizes and transactional scopes.</p></li><li><p>Measure system impact by monitoring key metrics (e.g., CPU, memory, open transactions).</p></li><li><p>Assess how quickly CockroachDB cleans up uncommitted writes after transactions complete.</p></li><li><p>Adjust strategies—batch size, scheduling, concurrency—to see how each change affects performance and resource usage.</p></li></ul></div></aside></div></section></section>
<section><section id="_data_distribution_and_rebalancing_in_cockroachdb"><h2>Data Distribution and Rebalancing in CockroachDB</h2></section><section id="_why_data_distribution_matters"><h2>Why Data Distribution Matters</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Ensures balanced workload across nodes</p></li><li><p>Optimizes resource utilization</p></li><li><p>Maintains consistent performance</p></li><li><p>Enables horizontal scalability</p></li><li><p>Supports high availability</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Proper data distribution underpins CockroachDB&#8217;s ability to scale horizontally while maintaining high performance and fault tolerance.
When data is evenly spread across nodes, it prevents single nodes from becoming hot spots under heavy workloads.
This design also ensures that when you add more nodes, the cluster can automatically rebalance data to make use of the new resources.</p></div>
<div class="paragraph"><p>CockroachDB’s approach to replication ensures that if one node fails, data remains accessible on other nodes, supporting high availability.
By distributing data into “ranges” and placing replicas of each range across different nodes, CockroachDB eliminates single points of failure and utilizes each node’s storage and processing capacity more effectively.</p></div></aside></div></section><section id="_understanding_ranges"><h2>Understanding Ranges</h2><div class="slide-content"><div class="paragraph text-left"><p>Ranges are the fundamental unit of data distribution, replicated across nodes. They automatically split when they exceed size limits and can merge when they are small.</p></div>
<div class="paragraph text-left"><p>Default size limits apply (commonly 512MB), and zone configurations let you manage range placement and replication.</p></div>
<aside class="notes"><div class="paragraph"><p>Ranges are contiguous chunks of the keyspace that CockroachDB uses to break down large datasets into more manageable pieces. Each range is a <strong>Raft</strong> replication group, meaning all replicas stay in sync through the Raft consensus protocol. When a range grows beyond the default size limit (512MB, though this can vary by version), it automatically splits into two or more smaller ranges. Conversely, small ranges can merge to maintain efficiency.</p></div>
<div class="paragraph"><p>Zone configurations control how many replicas each range will have and where they should be placed geographically or within specific nodes. This allows fine-grained control over data placement to meet performance, compliance, or fault-tolerance requirements. Because CockroachDB automatically splits and merges ranges, the system continually adapts to changing data distribution needs.</p></div></aside></div></section><section id="_range_distribution_mechanics"><h2>Range Distribution Mechanics</h2><div class="slide-content"><div class="paragraph"><p>CockroachDB offers <strong>automatic</strong> balancing, but also supports manual operations such as <strong>splits</strong> and <strong>replica placement rules</strong>.</p></div>
<div class="paragraph"><p>Each range has a lease holder for reads.
Overall storage is balanced across the cluster.</p></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB automatically balances ranges across nodes to optimize both performance and resource usage. The <strong>allocator</strong> process monitors node capacity, load, and existing replicas to decide which ranges should be moved or split. Some key aspects:</p></div>
<div class="ulist"><ul><li><p><strong>Automatic Balancing:</strong> The cluster automatically rebalances replicas to distribute load and storage usage across all available nodes.</p></li><li><p><strong>Manual Splits:</strong> You can explicitly split a range when you know a key boundary will receive high traffic. For instance:</p></li></ul></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE orders SPLIT AT VALUES ('order100000');</code></pre></div></div>
<div class="paragraph"><p>This can prevent a particular range from becoming a bottleneck if you anticipate large amounts of reads or writes on certain keys.</p></div>
<div class="ulist"><ul><li><p><strong>Replica Placement Rules:</strong> Zone configurations define which nodes or regions can hold replicas. This helps comply with data locality or regulatory constraints.</p></li><li><p><strong>Lease Holder Management:</strong> Each range has a <strong>lease holder</strong> replica that handles read requests and coordinates writes. The system tries to place the lease holder close to the majority of requests, enhancing performance.</p></li><li><p><strong>Balanced Storage:</strong> As data grows, the allocator moves replicas to avoid filling up any single node’s disk or overwhelming its CPU resources.</p></li></ul></div></aside></div></section><section id="_zone_configurations"><h2>Zone Configurations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Control replication factor</p></li><li><p>Define replica constraints</p></li><li><p>Set storage parameters</p></li><li><p>Manage lease preferences</p></li><li><p>Override system defaults</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Zone configurations let you customize the distribution and replication settings for specific databases, tables, or even subsets of data within a table:</p></div>
<div class="ulist"><ul><li><p><strong>Replication Factor:</strong> Specify how many replicas each range should have, such as 3 or 5, balancing redundancy and cost.</p></li><li><p><strong>Replica Constraints:</strong> Place replicas in certain regions or on certain nodes. For example, ensuring data resides in both US and EU regions for compliance or latency reasons.</p></li><li><p><strong>Storage Parameters:</strong> Configure attributes like the <strong>gc.ttlseconds</strong> or other advanced settings to control how quickly old data is removed or how frequently backups might be required.</p></li><li><p><strong>Lease Preferences:</strong> Indicate where the lease holder should reside, which is useful for read-heavy tables that must serve low-latency queries from a specific region.</p></li><li><p><strong>Override System Defaults:</strong> While CockroachDB defaults work well in many cases, zone configs allow you to override them to optimize for your unique workload.</p></li></ul></div>
<div class="paragraph"><p>Example of changing zone config:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER RANGE default
CONFIGURE ZONE USING
  num_replicas = 5,
  constraints = '{+region=us-east: 1, +region=us-west: 1, +region=eu-west: 1}';
SHOW ZONE CONFIGURATION FOR RANGE default;</code></pre></div></div></aside></div></section><section id="_monitoring_distribution"><h2>Monitoring Distribution</h2><div class="slide-content"><div class="paragraph text-left"><p>Monitoring your cluster involves checking:</p></div>
<div class="ulist text-left"><ul><li><p>range statistics</p></li><li><p>replica counts</p></li><li><p>observing storage balance</p></li></ul></div>
<div class="admonitionblock important"><table><tr><td class="icon"><i class="fa fa-exclamation-circle" title="Important"></i></td><td class="content">Ensuring lease distribution is even can improve read performance.</td></tr></table></div></div></section><section><div class="slide-content"><div class="paragraph h4-style"><p>You should also track split and merge activity over time</p></div>
<div class="paragraph h4-style"><p>Excessive splits might indicate hotspots, while merges can consolidate small ranges for better efficiency</p></div>
<aside class="notes"><div class="paragraph"><p>Monitoring distribution is essential to ensure data remains balanced and performance is optimal. Tools and techniques:</p></div>
<div class="ulist"><ul><li><p><strong>DB Console:</strong> Provides a visual overview of ranges, their replicas, and node usage. It also shows if certain nodes are under higher load.</p></li><li><p><strong>Replica Counts:</strong> Track how many replicas each node holds, ensuring the cluster remains balanced.</p></li><li><p><strong>Storage Balance:</strong> Watch for skew in disk usage. If one node’s usage starts to grow faster than others, rebalancing might be needed.</p></li><li><p><strong>Lease Distribution:</strong> Verify that lease holders are appropriately distributed. If a single node holds too many lease holders, it can become a bottleneck for reads.</p></li><li><p><strong>Split/Merge Activity:</strong> Regular splits and merges indicate the cluster is adapting to data volume changes. Excessive splits might mean your data is rapidly growing or receiving heavy writes in a small key space.</p></li></ul></div></aside></div></section><section id="_manual_operations"><h2>Manual Operations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Split ranges explicitly</p></li><li><p>Force rebalancing</p></li><li><p>Adjust zone configs</p></li><li><p>Verify changes</p></li><li><p>Monitor progress</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>While CockroachDB automates most distribution tasks, manual intervention can help tackle specific challenges:</p></div>
<div class="ulist"><ul><li><p><strong>Explicit Split:</strong> When you know a certain key range will become very large or heavily accessed, manually split it to preemptively distribute load.</p></li></ul></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_logs SPLIT AT VALUES ('user5000');</code></pre></div></div>
<div class="ulist"><ul><li><p><strong>Force Rebalancing:</strong> If the system lags behind in redistribution, you can use SQL statements (or certain CLI commands) to expedite rebalancing. However, typically CockroachDB handles this automatically.</p></li><li><p><strong>Zone Config Adjustments:</strong> Adjust constraints, replicas, or lease preferences for special cases, such as storing data in multiple regions or meeting compliance needs.</p></li><li><p><strong>Verification:</strong> Always confirm changes by running <code>SHOW ZONE CONFIGURATION</code> or <code>SHOW RANGES FROM TABLE tablename;</code> to see if your updates took effect.</p></li><li><p><strong>Monitoring Progress:</strong> Watch the DB Console or relevant logs to ensure data rebalancing completes successfully and performance improves as expected.</p></li></ul></div></aside></div></section><section id="_summary_4"><h2>Summary</h2><div class="slide-content"><div class="paragraph text-left"><p>Ranges enable distribution, with <strong>zone configuration</strong>s controlling their behavior and ensuring an even workload.</p></div>
<div class="paragraph text-left"><p>Monitoring the cluster&#8217;s range balances and replica counts is <strong>crucial</strong>.</p></div>
<div class="paragraph text-left"><p>Manual operations let you control splits and rebalancing. Always verify changes and regularly check performance.</p></div>
<aside class="notes"><div class="paragraph"><p>Data distribution in CockroachDB hinges on the <strong>range</strong> concept, with automation driven by zone configurations and an internal allocator.
In most cases, CockroachDB’s defaults and automatic splits/rebalancing keep the system balanced.
However, complex workloads or strict regulatory requirements may demand manual interventions, advanced zone config rules, or frequent monitoring.
By proactively verifying distribution and rebalancing outcomes, you ensure stable performance and robust fault tolerance across your cluster.</p></div></aside></div></section><section id="_exercise_preview_3"><h2>Exercise Preview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Analyze range distribution</p></li><li><p>Configure manual splits</p></li><li><p>Modify zone configurations</p></li><li><p>Monitor distribution changes</p></li><li><p>Verify system behavior</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the upcoming exercise, you will:</p></div>
<div class="ulist"><ul><li><p>Inspect your current range distribution to see how data is split and replicated.</p></li><li><p>Apply manual splits to preemptively distribute large or frequently accessed data segments.</p></li><li><p>Modify zone configurations to customize replication factors and replica placement.</p></li><li><p>Observe changes in the DB Console, confirming that new splits and configs are adopted.</p></li><li><p>Verify the system’s overall behavior to ensure that your adjustments yield improved performance and balanced resource usage.</p></li></ul></div></aside></div></section></section>
<section><section id="_compaction_in_cockroachdb"><h2>Compaction in CockroachDB</h2></section><section id="_why_compaction_matters"><h2>Why Compaction Matters</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Storage performance directly impacts application responsiveness</p></li><li><p>LSM trees balance write speed against read efficiency</p></li><li><p>Unmanaged growth leads to degraded query performance</p></li><li><p>Critical for maintaining consistent database performance</p></li><li><p>Essential skill for database administrators</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Compaction is a fundamental process in CockroachDB that directly affects database performance. The storage layer uses Log-Structured Merge (LSM) trees (via Pebble) to organize data, prioritizing write performance while maintaining read efficiency. Without proper compaction, SST files can accumulate, increasing read amplification and storage costs.</p></div>
<div class="paragraph"><p>Effective compaction merges smaller, fragmented SST files into larger, more organized files, reducing the number of lookups required for queries. This leads to faster read operations and more predictable performance. Monitoring and managing compaction is a key responsibility for database administrators to ensure queries remain efficient as the database grows.</p></div></aside></div></section><section id="_lsm_tree_structure"><h2>LSM Tree Structure</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Data organized in levels L0 through L6</p></li><li><p>New data enters at L0 level first</p></li><li><p>Each level contains sorted string table (SST) files</p></li><li><p>Lower levels hold larger SSTs with older data</p></li><li><p>Compaction merges data downward through levels</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB’s Pebble storage engine implements an LSM tree with levels from L0 to L6. New or recently updated data resides in L0, where writes are quickest but can accumulate many small SST files. Through compaction, these files are merged and moved into lower levels (L1, L2, and so on) which each contain fewer but larger SST files. This tiered approach maintains a balance between efficient writes (at higher levels) and efficient reads (at lower levels).</p></div>
<div class="paragraph"><p>As data progresses through compaction, older or less frequently updated rows move further down the levels. This design also helps CockroachDB handle large volumes of writes without overloading the system, as compaction distributes data more evenly over time.</p></div></aside></div></section><section id="_read_amplification"><h2>Read Amplification</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Occurs when multiple SSTs must be consulted for one read</p></li><li><p>Higher values indicate compaction may be needed</p></li><li><p>Monitored through DB Console&#8217;s Storage dashboard</p></li><li><p>Single-digit values are optimal</p></li><li><p>Can worsen during heavy write workloads</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Read amplification measures how many SST files must be accessed to fulfill a single read request. In CockroachDB, high read amplification often occurs when compaction lags behind ongoing write activity, leaving many small SST files at higher levels.</p></div>
<div class="paragraph"><p>You can monitor read amplification in the DB Console’s Storage dashboard. Ideally, you want this metric to remain in single digits. If you notice it creeping into double digits, it signals that compaction might be overdue. Heavy write workloads, large bulk operations, or frequent updates can exacerbate read amplification if compactions aren’t keeping pace.</p></div></aside></div></section><section id="_write_amplification"><h2>Write Amplification</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Ratio of total values to live records</p></li><li><p>Calculated using range-level statistics</p></li><li><p>Indicates storage efficiency</p></li><li><p>Affected by update frequency</p></li><li><p>Higher values suggest need for compaction</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Write amplification reflects how many times data is written to storage compared
to the number of live records. Frequent updates to the same rows generate
multiple versions across SST files, especially if compactions aren’t performed
promptly. Over time, this inflates storage usage and slows down the system.</p></div>
<div class="paragraph"><p>Range-level metrics in CockroachDB let you calculate approximate write
amplification. For heavily updated tables, or those receiving constant upserts,
you should track this ratio and consider triggering manual compactions if it
grows too high, especially during maintenance windows.</p></div></aside></div></section><section id="_managing_compaction"><h2>Managing Compaction</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Regular monitoring through DB Console</p></li><li><p>Force compaction during maintenance windows</p></li><li><p>Requires temporary node shutdown</p></li><li><p>Performed using cockroach debug compact</p></li><li><p>Results in improved read/write performance</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB automatically compacts data in the background, but there are times when you may want to initiate manual compaction:</p></div>
<div class="olist arabic"><ol class="arabic"><li><p><strong>Performance Degradation</strong>: If read or write amplification has become unacceptably high.</p></li><li><p><strong>Maintenance Windows</strong>: Compaction can be CPU and I/O intensive, so schedule it when the load is minimal.</p></li><li><p><strong>Manual Command</strong>:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-bash hljs" data-noescape="true" data-lang="bash">cockroach debug compact /path/to/store</code></pre></div></div>
<div class="ulist"><ul><li><p>This command must be run when the node is stopped.</p></li><li><p>Point it to the correct store directory for each node.</p></li></ul></div></li></ol></div>
<div class="paragraph"><p>After compaction completes, you’ll often see improved read performance and
reduced write amplification. Although downtime is required, this proactive
maintenance can extend the system’s longevity and sustain consistent performance
as data grows.</p></div></aside></div></section><section id="_exercise_overview_2"><h2>Exercise Overview</h2><div class="slide-content"><div class="ulist text-left"><ul><li><p>Monitor read amplification via DB Console</p></li><li><p>Calculate write amplification using SQL</p></li><li><p>Create additional load to observe metrics</p></li><li><p>Perform manual compaction</p></li><li><p>Observe performance improvements</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the exercise, you will:</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Monitor <strong>read amplification</strong> through the DB Console’s Storage dashboard.</p></li><li><p>Use SQL queries to calculate approximate <strong>write amplification</strong> for specific ranges or tables.</p></li><li><p>Generate additional load (e.g., with INSERTs or UPDATEs) to observe how these metrics change under stress.</p></li><li><p>Perform a <strong>manual compaction</strong> using the <code>cockroach debug compact</code> command on a node&#8217;s store directory.</p></li><li><p>Confirm <strong>performance improvements</strong> by re-checking read and write amplification levels afterward.</p></li></ol></div>
<div class="paragraph"><p>This hands-on activity will illustrate how compaction works in real-world scenarios and how to recognize when intervention is needed.</p></div></aside></div></section></section>
<section><section id="_planned_node_maintenance_in_cockroachdb"><h2>Planned Node Maintenance in CockroachDB</h2></section><section id="_why_node_maintenance_matters"><h2>Why Node Maintenance Matters</h2><div class="slide-content"><div class="ulist"><ul><li><p>Database nodes require regular maintenance for updates, patches, and hardware upgrades</p></li><li><p>Improper maintenance can lead to data unavailability and application downtime</p></li><li><p>CockroachDB provides built-in features for graceful node maintenance</p></li><li><p>Proper procedures ensure continuous service availability</p></li><li><p>Critical for both planned maintenance and emergency scenarios</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Node maintenance is a crucial operational task that every database administrator needs to master.
Whether it&#8217;s for routine OS upgrades, security patches, or hardware replacements, being able to
safely remove a node from the cluster and bring it back online helps ensure uninterrupted
service. CockroachDB&#8217;s shared-nothing, distributed architecture is specifically designed to
tolerate node failures and handle rolling upgrades, provided the correct drain and decommission
procedures are followed.</p></div>
<div class="paragraph"><p>For instance, when planning a rolling upgrade, you typically drain each node in sequence so it
stops accepting new traffic and gracefully transfers active leases and replicas. This prevents
disruptions to ongoing queries. When hardware upgrades are required, draining ensures in-flight
operations complete, minimizing the risk of data inconsistencies or query failures.</p></div></aside></div></section><section id="_types_of_node_maintenance"><h2>Types of Node Maintenance</h2><div class="slide-content"><div class="ulist"><ul><li><p>Temporary maintenance - node will rejoin with existing data store</p></li><li><p>Permanent removal - node decommissioning required</p></li><li><p>Rolling upgrades - systematic cluster-wide maintenance</p></li><li><p>Emergency maintenance - unplanned but controlled shutdown</p></li><li><p>Hardware replacement - permanent node replacement</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Different maintenance scenarios call for different operational procedures. A temporary
maintenance event might be something like upgrading an operating system on a node or performing
minor hardware fixes; in such a scenario, you drain the node, shut it down, and then restart it
with the same data. The node rejoins the cluster as if it had experienced a transient outage.</p></div>
<div class="paragraph"><p>For permanent removal—perhaps for decomissioning a node in a cloud environment or retiring old
hardware—CockroachDB&#8217;s decommissioning process redistributes replicas off that node and frees
the cluster of that node&#8217;s ID. This ensures data replication requirements remain fulfilled.</p></div>
<div class="paragraph"><p>Rolling upgrades refer to upgrading the CockroachDB version one node at a time. CockroachDB&#8217;s
multi-active availability design facilitates these upgrades without a full cluster outage, as
remaining nodes continue serving requests.</p></div>
<div class="paragraph"><p>Emergency maintenance might be triggered by unexpected hardware failures or urgent security
patching. Even in these situations, if you have enough replication and carefully follow the
draining steps, the rest of the cluster remains operational while the problematic node is
addressed.</p></div>
<div class="paragraph"><p>Hardware replacement is similar to permanent removal followed by adding new nodes, ensuring
replication factors are maintained. The method chosen depends on the nature of the event, the
cluster topology, and the service-level objectives.</p></div></aside></div></section><section id="_prerequisites_for_safe_maintenance"><h2>Prerequisites for Safe Maintenance</h2><div class="slide-content"><div class="ulist"><ul><li><p>Load balancer health monitoring</p></li><li><p>Proper cluster settings configuration</p></li><li><p>Sufficient cluster capacity and replication factor</p></li><li><p>Connection and transaction timeout settings</p></li><li><p>Storage capacity for data redistribution</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Before beginning any maintenance tasks, it is vital to confirm that the cluster can handle the
temporary loss of one or more nodes. For example, ensure that you have the recommended
replication factor (usually 3 or 5) so that the cluster remains available if a node is taken
offline.</p></div>
<div class="paragraph"><p>Load balancers (if used) must be configured to reroute traffic away from the node under
maintenance. Typically, health checks can be set to fail open connections or new requests
directed to the node being drained.</p></div>
<div class="paragraph"><p>Cluster settings such as <code>server.shutdown.drain_wait</code> or timeouts for connections and queries
must be tuned so that draining does not abruptly kill active transactions. The cluster also
needs enough capacity and storage overhead to handle any temporary or permanent redistribution
of replicas that might occur.</p></div>
<div class="paragraph"><p>For example, you might run a quick check on under-replicated ranges before maintenance:</p></div>
<div class="listingblock"><div class="content"><pre>&gt; cockroach sql --insecure -e "SHOW RANGES FROM TABLE mydb.mytable"</pre></div></div>
<div class="paragraph"><p>Ensuring minimal under-replicated ranges helps avoid unexpected data unavailability during
maintenance.</p></div></aside></div></section><section id="_node_draining_process"><h2>Node Draining Process</h2><div class="slide-content"><div class="ulist"><ul><li><p>Phase 1: Unready phase - node stops accepting new connections</p></li><li><p>Phase 2: Connection drain - existing connections gracefully close</p></li><li><p>Phase 3: SQL drain - active queries complete or transfer</p></li><li><p>Phase 4: DistSQL drain - distributed queries complete</p></li><li><p>Controlled by specific timeout settings</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB provides a <code>cockroach node drain</code> command (or the equivalent <code>cockroach quit</code> with
the <code>--drain</code> flag) to orchestrate node draining. During:</p></div>
<div class="ulist"><ul><li><p><strong>Phase 1 (Unready phase)</strong>: The node signals the load balancer and cluster that it&#8217;s not
accepting new connections.</p></li><li><p><strong>Phase 2 (Connection drain)</strong>: Existing connections are given a grace period to wrap up;
no new sessions are accepted.</p></li><li><p><strong>Phase 3 (SQL drain)</strong>: In-flight transactions and queries finish execution or fail if they
exceed the configured timeouts.</p></li><li><p><strong>Phase 4 (DistSQL drain)</strong>: Distributed SQL flows that might span multiple nodes are given
time to complete.</p></li></ul></div>
<div class="paragraph"><p>Each phase of draining can be customized with timeouts. For instance:</p></div>
<div class="listingblock"><div class="content"><pre>$ cockroach node drain --certs-dir=certs --drain-wait=1m --insecure 1</pre></div></div>
<div class="paragraph"><p>This command waits up to one minute for the node with ID 1 to drain. Fine-tuning these timeouts
ensures graceful shutdown without stalling the entire cluster.</p></div></aside></div></section><section id="_critical_cluster_settings"><h2>Critical Cluster Settings</h2><div class="slide-content"><div class="ulist"><ul><li><p><code>server.shutdown.drain_wait</code> - maximum drain wait time</p></li><li><p><code>server.shutdown.connection_cancel_period</code> - connection drain period</p></li><li><p><code>server.shutdown.query_wait</code> - query completion time</p></li><li><p><code>--drain-wait</code> flag - overall drain timeout for CLI commands</p></li><li><p><code>server.time_until_store_dead</code> - store retention period</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>These cluster settings govern how CockroachDB handles node shutdown:</p></div>
<div class="ulist"><ul><li><p><code>server.shutdown.drain_wait</code>: The maximum time the server remains in a draining state before
forcing a shutdown. This is especially important to let in-flight queries finish gracefully.</p></li><li><p><code>server.shutdown.connection_cancel_period</code>: The grace period before established connections
are forcibly canceled.</p></li><li><p><code>server.shutdown.query_wait</code>: Defines how long to wait for active queries to complete before
forcibly terminating them.</p></li><li><p><code>--drain-wait</code>: A CLI flag that can override the above settings on a per-command basis. This
is used when running <code>cockroach node drain</code> or <code>cockroach quit --drain</code>.</p></li><li><p><code>server.time_until_store_dead</code>: Controls how long a store (node) can be unresponsive before
the cluster considers it “dead,” which triggers replica rebalancing or recovery.</p></li></ul></div>
<div class="paragraph"><p>Verifying these settings is essential before performing maintenance because incorrect or
misaligned timeouts can abruptly kill critical transactions or prolong maintenance windows
unnecessarily.</p></div></aside></div></section><section id="_monitoring_maintenance_progress"><h2>Monitoring Maintenance Progress</h2><div class="slide-content"><div class="ulist"><ul><li><p>Node status and liveness</p></li><li><p>Range statistics and distribution</p></li><li><p>Under-replicated ranges</p></li><li><p>Capacity usage across nodes</p></li><li><p>Decommissioning progress</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>CockroachDB provides multiple ways to monitor maintenance operations:</p></div>
<div class="ulist"><ul><li><p><strong>Node status and liveness</strong>: Use the Admin UI or the <code>cockroach node status</code> CLI command to
check a node&#8217;s health and see if it&#8217;s live.</p></li><li><p><strong>Range statistics</strong>: Monitoring range count and distribution helps ensure that critical data
remains fully replicated.</p></li><li><p><strong>Under-replicated ranges</strong>: Check the Admin UI or run <code>SHOW UNDERREPLICATED RANGES;</code> in
the SQL shell to identify any data ranges that might be missing enough replicas.</p></li><li><p><strong>Capacity usage</strong>: Monitoring store usage is especially important if data is being
rebalanced from one node to others. A node hitting storage limits can slow or interrupt
rebalancing.</p></li><li><p><strong>Decommissioning progress</strong>: When removing a node permanently, track the decommission status
with <code>cockroach node decommission --status</code> to see how many replicas remain to be
transferred off the node.</p></li></ul></div>
<div class="paragraph"><p>This combination of CLI commands and the Admin UI ensures real-time visibility into the
cluster&#8217;s condition and the progress of node maintenance.</p></div></aside></div></section><section id="_best_practices"><h2>Best Practices</h2><div class="slide-content"><div class="ulist"><ul><li><p>Verify cluster health before maintenance</p></li><li><p>Monitor system during maintenance operations</p></li><li><p>Maintain sufficient node capacity for data redistribution</p></li><li><p>Document maintenance procedures</p></li><li><p>Test procedures in non-production environment</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Following best practices is critical for avoiding unplanned downtime:</p></div>
<div class="ulist"><ul><li><p><strong>Verify cluster health</strong>: Run checks on replication, node statuses, and range distribution
before pulling any node out for maintenance.</p></li><li><p><strong>Monitor system continuously</strong>: Keep an eye on logs, Admin UI metrics, and CLI outputs to
detect anomalies early.</p></li><li><p><strong>Ensure sufficient capacity</strong>: Draining or decommissioning a node redistributes replicas.
Having extra node capacity prevents sudden overloads on remaining nodes.</p></li><li><p><strong>Document maintenance</strong>: Establish internal runbooks detailing the entire sequence of
commands and checks. This allows rapid troubleshooting if something goes wrong.</p></li><li><p><strong>Test in non-production</strong>: Especially for upgrades, a staging or test environment can help
you discover misconfigurations without risking production workloads.</p></li></ul></div></aside></div></section><section id="_exercise_preview_4"><h2>Exercise Preview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Practice both temporary and permanent node maintenance</p></li><li><p>Configure and verify maintenance prerequisites</p></li><li><p>Perform node drain and shutdown procedures</p></li><li><p>Monitor maintenance progress</p></li><li><p>Verify cluster health throughout the process</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the upcoming lab exercise, you&#8217;ll perform hands-on node maintenance in a multi-node cluster.
You&#8217;ll begin by validating key cluster settings like <code>server.shutdown.drain_wait</code> and
<code>server.time_until_store_dead</code>, then proceed to drain a node for a temporary maintenance
scenario. You&#8217;ll observe how queries complete, how connections are closed, and how to bring
the node back into service.</p></div>
<div class="paragraph"><p>Next, you&#8217;ll decommission a node permanently, monitoring how data is relocated and ensuring
there are no under-replicated ranges left. By the end, you&#8217;ll be well-versed in both short-term
maintenance and the full decommission process, preparing you for real-world scenarios.</p></div></aside></div></section></section>
<section><section id="_unplanned_node_outages_in_cockroachdb"><h2>Unplanned Node Outages in CockroachDB</h2></section><section id="_why_understanding_node_outages_matters"><h2>Why Understanding Node Outages Matters</h2><div class="slide-content"><div class="ulist"><ul><li><p>Unplanned outages can occur at any time due to hardware failures, network issues, or system crashes</p></li><li><p>Proper configuration and monitoring are crucial for maintaining data availability</p></li><li><p>Quick response to failures prevents cascading issues and minimizes downtime</p></li><li><p>Understanding failure states helps make informed recovery decisions</p></li><li><p>Well-configured clusters can often self-heal without manual intervention</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Unplanned node outages are a reality in distributed systems. CockroachDB uses a distributed architecture built on the Raft consensus protocol to handle node failures gracefully. However, proper configuration and monitoring are essential to ensure uninterrupted service and data integrity. This module covers how to prepare for, detect, and respond to unexpected node failures.</p></div>
<div class="paragraph"><p>Understanding node outages is critical because:</p></div>
<div class="ulist"><ul><li><p>They can happen without warning and at inconvenient times, such as high-traffic moments or during maintenance windows.</p></li><li><p>Poor configuration, like incorrectly set timeouts or insufficient replication factors, can lead to unnecessary data movement or delayed recovery.</p></li><li><p>Quick, informed responses prevent minor issues—like a temporary network partition—from becoming major problems, such as data unavailability or query failures.</p></li><li><p>Well-configured clusters often handle failures automatically, maintaining consensus on data and offering continuous availability for most workloads.</p></li></ul></div>
<div class="paragraph"><p>Key technical considerations include:</p></div>
<div class="ulist"><ul><li><p>Ensuring an appropriate replication factor for fault tolerance (e.g., using three replicas across three nodes).</p></li><li><p>Monitoring liveness heartbeats, which let the cluster know which nodes are active.</p></li><li><p>Understanding the difference between short-lived outages (like a transient network glitch) versus permanent node failures (like a complete hardware meltdown).</p></li></ul></div></aside></div></section><section id="_key_cluster_settings_for_failure_detection"><h2>Key Cluster Settings for Failure Detection</h2><div class="slide-content"><div class="ulist"><ul><li><p><code>server.time_until_store_dead</code> controls transition from suspect to dead state</p></li><li><p>Default 5-minute wait period allows for temporary failures to resolve</p></li><li><p>Shorter times trigger faster recovery but risk unnecessary data movement</p></li><li><p>Longer times reduce unnecessary rebalancing but delay recovery</p></li><li><p>Setting should align with your infrastructure&#8217;s typical failure patterns</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The <code>server.time_until_store_dead</code> setting is the primary control for how CockroachDB responds to node failures:</p></div>
<div class="ulist"><ul><li><p>When a node stops responding, it enters a "suspect" state, tracked by node liveness heartbeats.</p></li><li><p>The cluster waits for the configured time before declaring the node "dead."</p></li><li><p>During the suspect period, replicas remain in place to avoid unnecessary data movement, which reduces churn if the node comes back quickly.</p></li><li><p>After the dead state is declared, the cluster begins redistributing data to maintain quorum and availability.</p></li></ul></div>
<div class="paragraph"><p>Typical tuning involves balancing the risk of waiting too long (leading to slower recovery) versus reacting too quickly (leading to extra data rebalancing). In many production deployments, the default value (5 minutes) is a good starting point. However, you may need to adjust it based on hardware reliability and network conditions. For example, if your nodes often experience transient network hiccups, you might opt for a slightly longer window.</p></div>
<div class="paragraph"><p>Example of changing <code>server.time_until_store_dead</code> using SQL:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER CLUSTER SETTING server.time_until_store_dead = '3m';</code></pre></div></div>
<div class="paragraph"><p>Example of verifying the current setting:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">SHOW CLUSTER SETTING server.time_until_store_dead;</code></pre></div></div></aside></div></section><section id="_node_states_and_cluster_behavior"><h2>Node States and Cluster Behavior</h2><div class="slide-content"><div class="ulist"><ul><li><p>Nodes transition through states: Live → Suspect → Dead</p></li><li><p>Suspect state preserves data location for quick recovery</p></li><li><p>Dead state triggers automatic data redistribution</p></li><li><p>State transitions influence resource utilization</p></li><li><p>Monitoring tools track state changes and cluster health</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Understanding node states is crucial for effective operations:</p></div>
<div class="paragraph"><p>Live State:</p></div>
<div class="ulist"><ul><li><p>The node is fully functional, actively participating in Raft replication, and servicing queries.</p></li><li><p>Regular heartbeat communication with other nodes confirms its status to the cluster.</p></li></ul></div>
<div class="paragraph"><p>Suspect State:</p></div>
<div class="ulist"><ul><li><p>The node stops responding to heartbeats, but it might be experiencing only a transient failure, such as a momentary network partition.</p></li><li><p>Replicas remain on the node in hopes it will recover quickly, avoiding unnecessary rebalancing operations.</p></li></ul></div>
<div class="paragraph"><p>Dead State:</p></div>
<div class="ulist"><ul><li><p>The node is considered lost permanently after exceeding the <code>server.time_until_store_dead</code> threshold.</p></li><li><p>The cluster will promptly begin rebalancing data to maintain the desired replication factor and quorum.</p></li><li><p>Additional system resources, such as CPU and disk I/O, may be consumed significantly during rebalancing.</p></li></ul></div>
<div class="paragraph"><p>Monitoring the transitions between these states is vital. Operations teams often use the CockroachDB Admin UI, CLI commands, or external monitoring systems (e.g., Prometheus/Grafana) to observe node liveness changes in real time. This ensures swift action when a node experiences persistent issues.</p></div></aside></div></section><section id="_critical_monitoring_metrics"><h2>Critical Monitoring Metrics</h2><div class="slide-content"><div class="ulist"><ul><li><p>Under-replicated ranges indicate recovery needs</p></li><li><p>CPU and disk I/O show resource pressure</p></li><li><p>Available storage capacity affects recovery speed</p></li><li><p>Node liveness heartbeats track cluster health</p></li><li><p>These metrics guide intervention decisions</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Key metrics to monitor during node failures:</p></div>
<div class="paragraph"><p>Store Metrics:</p></div>
<div class="ulist"><ul><li><p>Under-replicated ranges: A direct indicator that the cluster is missing replicas and is in the process of rebalancing or waiting for a node to come back online.</p></li><li><p>Storage capacity: Ensures each node can handle the extra replicas when a node fails. Low available storage can slow down or even block recovery.</p></li></ul></div>
<div class="paragraph"><p>System Metrics:</p></div>
<div class="ulist"><ul><li><p>CPU usage: Elevated usage can occur during rebalancing, compactions, or when queries are rerouted away from the failed node.</p></li><li><p>Disk I/O: Data movement generates additional read/write activity, potentially causing performance impacts.</p></li><li><p>Memory usage: Rebalancing and increased workload can strain memory resources if the cluster is undersized.</p></li><li><p>Heartbeat success: Monitors node liveness. A significant drop indicates possible communication issues, resource exhaustion, or node failures.</p></li></ul></div>
<div class="paragraph"><p>Use these metrics to determine:</p></div>
<div class="ulist"><ul><li><p>Whether the cluster’s self-healing process (automatic rebalancing of replicas) is progressing efficiently.</p></li><li><p>If additional capacity or manual intervention is required to maintain performance.</p></li><li><p>When it is safe to remove or recommission nodes based on the success or failure of recovery steps.</p></li></ul></div></aside></div></section><section id="_recovery_strategies_and_best_practices"><h2>Recovery Strategies and Best Practices</h2><div class="slide-content"><div class="ulist"><ul><li><p>Configure monitoring alerts for early detection</p></li><li><p>Assess resource capacity before recovery actions</p></li><li><p>Add nodes when self-healing is overwhelmed</p></li><li><p>Remove dead nodes only after confirming state</p></li><li><p>Verify full recovery through testing</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Best practices for handling unplanned outages:</p></div>
<div class="paragraph"><p>Preparation:
- Configure appropriate monitoring dashboards and alerts (e.g., alert on under-replicated ranges or node liveness heartbeat failures).
- Understand normal cluster performance metrics to detect anomalies quickly.
- Plan for adequate capacity, ensuring that each node can absorb extra replicas in the event another node fails.
- Document clear recovery procedures to reduce downtime during critical events.</p></div>
<div class="paragraph"><p>Recovery Process:
1. Detect and confirm the failure, using heartbeat metrics and system logs.
2. Monitor self-healing progress by observing under-replicated ranges and cluster logs.
3. Scale out by adding nodes if rebalancing saturates existing resources (e.g., CPU, disk I/O).
4. Remove dead nodes from the cluster once certain they won’t rejoin. In CockroachDB, you might decommission a node via:</p></div>
<div class="paragraph"><p>+</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-bash hljs" data-noescape="true" data-lang="bash">cockroach node decommission &lt;node-id&gt; --host=&lt;host&gt; --port=&lt;port&gt;</code></pre></div></div>
<div class="paragraph"><p>+
5. Verify cluster stability by confirming that under-replicated ranges have returned to normal and that no remaining errors persist.</p></div>
<div class="paragraph"><p>Key considerations:</p></div>
<div class="ulist"><ul><li><p>Balancing quick recovery with overall cluster stability is essential. A cluster that rebalances too aggressively can degrade performance.</p></li><li><p>Proactively monitor resource usage during recovery to avoid compounding issues.</p></li><li><p>Validate that all impacted replicas have been restored to the desired replication factor and that the cluster remains consistent and responsive.</p></li></ul></div></aside></div></section><section id="_summary_5"><h2>Summary</h2><div class="slide-content"><div class="ulist"><ul><li><p>Unplanned outages require balanced recovery strategies</p></li><li><p>Proper configuration prevents unnecessary data movement</p></li><li><p>Monitoring guides intervention decisions</p></li><li><p>Quick response prevents cascading issues</p></li><li><p>Testing ensures complete recovery</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Key takeaways:
1. Configuration impacts recovery behavior: Adjust settings like <code>server.time_until_store_dead</code> for optimal failover detection.
2. Monitoring is essential: Stay ahead of failures with alerts on node liveness, storage capacity, and under-replicated ranges.
3. Quick response prevents bigger problems: A timely reaction can avoid data availability issues and performance degradation.
4. Testing confirms full recovery: Always validate that ranges are fully replicated and that no critical alerts remain.
5. Documentation supports consistent responses: Keep detailed runbooks so operators can confidently follow established procedures.</p></div>
<div class="paragraph"><p>Remember:
- Balance speed with stability when tuning failure detection settings.
- Monitor key metrics (CPU, disk I/O, memory, node liveness) before, during, and after a failure.
- Add capacity when needed to accommodate data rebalance.
- Verify full recovery by ensuring all replicas are re-established.
- Document and review each outage event to improve future response strategies.</p></div></aside></div></section><section id="_exercise_preview_handling_node_failures"><h2>Exercise Preview: Handling Node Failures</h2><div class="slide-content"><div class="ulist"><ul><li><p>Practice configuring failure detection settings</p></li><li><p>Monitor cluster behavior during node failure</p></li><li><p>Implement emergency scale-out procedures</p></li><li><p>Remove dead nodes safely</p></li><li><p>Verify cluster recovery and stability</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The upcoming exercise will provide hands-on experience with node failure handling in CockroachDB:</p></div>
<div class="paragraph"><p>Learning Objectives:
- Configure critical cluster settings (e.g., adjusting <code>server.time_until_store_dead</code>).
- Monitor cluster health (via CockroachDB Admin UI or external monitoring) to detect and evaluate the impact of a node failure.
- Execute emergency scale-out procedures: add one or more nodes to handle rebalancing load.
- Safely remove dead or decommissioned nodes after confirming they cannot rejoin.
- Verify the cluster’s recovery, ensuring replicas are fully up-replicated and performance is stable.</p></div>
<div class="paragraph"><p>You will:
1. Configure <code>server.time_until_store_dead</code> to a value suitable for your environment.
2. Observe cluster behavior and metrics (under-replicated ranges, node liveness).
3. Add nodes if rebalancing or capacity constraints require additional resources.
4. Use <code>cockroach node decommission</code> to remove a permanently failed node.
5. Check under-replicated ranges and metrics to confirm the cluster has stabilized.</p></div></aside></div></section></section>
<section><section id="_cluster_version_management"><h2>Cluster Version Management</h2></section><section id="_why_version_management_matters"><h2>Why Version Management Matters</h2><div class="slide-content"><div class="ulist"><ul><li><p>Ensures access to latest security patches and features</p></li><li><p>Minimizes downtime through rolling upgrades</p></li><li><p>Preserves data consistency during transitions</p></li><li><p>Enables rollback capability when needed</p></li><li><p>Maintains compliance with support requirements</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Version management is a critical operational skill because:
- Security patches protect against vulnerabilities that could compromise data integrity or system availability.
- New features introduce performance enhancements and functionality that can improve operations.
- Rolling upgrades allow updates with minimal disruption to client applications, ensuring continuous availability of services.
- Proper management preserves rollback options, which are especially valuable if issues arise during or after an upgrade.
- Many support agreements require clusters to remain on currently supported versions to receive timely patches and assistance.</p></div>
<div class="paragraph"><p>The upgrade process requires thorough planning and execution to maintain cluster stability. Typical steps include verifying cluster health, performing backups, and completing the upgrade in a node-by-node fashion.</p></div></aside></div></section><section id="_understanding_version_types_and_impact"><h2>Understanding Version Types and Impact</h2><div class="slide-content"><div class="ulist"><ul><li><p>Patch versions (e.g., 24.1.6 → 24.1.8): Bug fixes and security updates</p></li><li><p>Major versions (e.g., 24.1 → 24.3): New features and architectural changes</p></li><li><p>Auto-finalization impacts rollback capability</p></li><li><p>Enterprise licenses may affect upgrade paths</p></li><li><p>Version compatibility affects client applications</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Different version changes have different implications in CockroachDB:</p></div>
<div class="paragraph"><p>Patch Updates:
- Primarily bug fixes and security updates.
- Often address stability and minor performance issues.
- Typically straightforward to implement.
- Generally do not require finalization and involve minimal risk of compatibility issues.</p></div>
<div class="paragraph"><p>Major Updates:
- May include new features, performance improvements, and architectural changes.
- Require careful planning due to changes in cluster behavior or data layout.
- Often need manual finalization to enable newly introduced features.
- Could affect application compatibility if APIs or system behavior change.</p></div>
<div class="paragraph"><p>Auto-finalization:
- When enabled, the cluster automatically finalizes a major upgrade after a certain period.
- Disabling auto-finalization allows administrators to test stability and roll back if necessary (before finalization).</p></div>
<div class="paragraph"><p>Enterprise Licenses:
- Some features (e.g., certain backup/restore options, encryption) may require an active enterprise license.
- Expired or invalid licenses can interrupt upgrade paths for enterprise-only functionalities.</p></div>
<div class="paragraph"><p>Version Compatibility:
- Client drivers or libraries must be compatible with the upgraded cluster version.
- It’s best practice to test upgraded nodes or clusters with staging/QA environments before rolling changes out to production.</p></div></aside></div></section><section id="_critical_pre_upgrade_considerations"><h2>Critical Pre-Upgrade Considerations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Verify cluster health and stability</p></li><li><p>Check enterprise license status</p></li><li><p>Assess available disk space for binary files</p></li><li><p>Review client application compatibility</p></li><li><p>Create backup and rollback plans</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Before starting any upgrade, verify:</p></div>
<div class="paragraph"><p>Health Checks:
- Ensure there are no dead or decommissioning nodes. Upgrading an already unhealthy cluster can exacerbate existing issues.
- Confirm there are no under-replicated ranges by checking Admin UI or running queries like:
  [source,sql]
  ----
  SELECT range_id, replicas FROM crdb_internal.ranges WHERE array_length(replicas, 1) &lt; 3;
  ----
  (Adjust the replication factor as needed for your cluster.)
- Check performance metrics (CPU, RAM, disk I/O) to verify stability and that the cluster can handle additional rebalancing if necessary.
- Make sure sufficient disk space is available on each node to accommodate new binaries and any extra data associated with rebalancing.</p></div>
<div class="paragraph"><p>Planning Requirements:
- A valid CockroachDB enterprise license if enterprise features are in use (for backup/restore, encryption, etc.).
- Confirm that client libraries, ORM frameworks, or custom applications are compatible with the new version.
- Document rollback procedures, which may involve retaining older binaries and disabling auto-finalization.
- Test backup solutions (e.g., <code>cockroach backup</code>) to guarantee you can restore data if needed. A typical command is:
  [source,bash]
  ----
  cockroach backup --host=&lt;host&gt; --port=&lt;port&gt; --user=&lt;username&gt; --insecure \
      --full-backup-destination="nodelocal://1/backup"
  ----</p></div></aside></div></section><section id="_rolling_upgrade_process"><h2>Rolling Upgrade Process</h2><div class="slide-content"><div class="ulist"><ul><li><p>Node-by-node upgrade preserves availability</p></li><li><p>Drain connections before upgrading each node</p></li><li><p>Replace binary and restart service</p></li><li><p>Verify node health before proceeding</p></li><li><p>Monitor cluster stability throughout</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The rolling upgrade process in CockroachDB involves:</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Draining and Stopping a Node:</p><div class="ulist"><ul><li><p>Use the drain feature to gracefully shut down the node:
[source,bash]
----
cockroach node drain &lt;node-id&gt; --host=&lt;host&gt; --port=&lt;port&gt; \
    --insecure --accept-sql-connections=false
----
This allows the node to reject new SQL connections and push ongoing transactions to completion, minimizing disruptions.</p></li></ul></div></li><li><p>Upgrading the Binary:</p><div class="ulist"><ul><li><p>Stop the CockroachDB service (e.g., <code>systemctl stop cockroach</code> or a similar command in your environment).</p></li><li><p>Replace the old binary with the new version (download and install the latest CockroachDB release).</p></li><li><p>Restart the service (e.g., <code>systemctl start cockroach</code>).</p></li></ul></div></li><li><p>Rejoining the Cluster:</p><div class="ulist"><ul><li><p>Confirm that the node rejoins the cluster and that it shows a healthy status in the Admin UI or via CLI:
[source,bash]
----
cockroach node status --host=&lt;host&gt; --port=&lt;port&gt; --insecure
----</p></li><li><p>Check logs (<code>cockroach.log</code>) for any critical errors.</p></li></ul></div></li><li><p>Repeating for Each Node:</p><div class="ulist"><ul><li><p>Proceed one node at a time to maintain quorum and minimize downtime.</p></li><li><p>Verify cluster stability (no under-replicated ranges, no errors in logs) before moving on to the next node.</p></li></ul></div></li></ol></div>
<div class="paragraph"><p>Throughout this process, monitor cluster metrics (CPU, memory, storage, network) to spot potential issues like overloading or performance regressions.</p></div></aside></div></section><section id="_finalization_and_validation"><h2>Finalization and Validation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Auto-finalization can be controlled via settings</p></li><li><p>Manual finalization enables new features</p></li><li><p>Verify consistent versions across nodes</p></li><li><p>Test basic operations post-upgrade</p></li><li><p>Monitor performance metrics</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Post-upgrade steps ensure the cluster is ready for production use:</p></div>
<div class="paragraph"><p>Finalization:
- If auto-finalization is disabled, you must manually finalize the upgrade to enable certain new features:
  [source,sql]
  ----
  SET CLUSTER SETTING version = '24.3';
  ----
- Finalization is irreversible, so confirm all nodes are upgraded and stable before finalizing.
- If <code>auto-finalize</code> is enabled, the cluster will finalize automatically after a grace period. Watch logs and metrics to ensure successful completion.</p></div>
<div class="paragraph"><p>Validation:
- Verify that each node reports the same version:
  [source,sql]
  ----
  SHOW CLUSTER SETTING version;
  ----
- Test basic SQL operations (CRUD) to confirm the cluster is functioning correctly.
- Check performance metrics (latency, throughput, CPU, memory, disk I/O) for any anomalies.
- Confirm application compatibility by performing integration tests on typical transactions or workloads.</p></div></aside></div></section><section id="_rollback_procedures_and_troubleshooting"><h2>Rollback Procedures and Troubleshooting</h2><div class="slide-content"><div class="ulist"><ul><li><p>Pre-finalization rollback preserves options</p></li><li><p>Monitor node rejoin process</p></li><li><p>Check logs for errors</p></li><li><p>Verify client connectivity</p></li><li><p>Document lessons learned</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>When issues arise or if you suspect instability:</p></div>
<div class="paragraph"><p>Rollback Options:
- Rollback is possible only if finalization has not occurred. Keep older binaries accessible to revert nodes.
- Conduct a node-by-node downgrade by draining, stopping, and restarting each node with the previous binary.
- If the cluster is partially upgraded, ensure all nodes are eventually on the same lower version before rejoining.</p></div>
<div class="paragraph"><p>Monitoring and Troubleshooting:
- Carefully watch logs (<code>cockroach.log</code> or syslog) for errors or warnings that indicate version mismatches or node connectivity issues.
- Verify nodes can rejoin the cluster. If a node is stuck, check disk usage, network connectivity, or potential replication conflicts.
- Confirm client connectivity after rollback, ensuring that drivers and applications can still communicate normally with the downgraded cluster.
- Thoroughly document any anomalies, root causes, and remediation steps to improve future upgrade processes.</p></div></aside></div></section><section id="_summary_6"><h2>Summary</h2><div class="slide-content"><div class="ulist"><ul><li><p>Version management maintains cluster health</p></li><li><p>Rolling upgrades minimize disruption</p></li><li><p>Preparation prevents common issues</p></li><li><p>Monitoring ensures success</p></li><li><p>Documentation supports future upgrades</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Key takeaways:
1. Proper planning is essential: Conduct health checks, verify license requirements, and prepare backups.
2. Rolling upgrades maintain availability: Upgrading nodes one by one avoids losing quorum.
3. Monitoring guides decisions: Observe cluster metrics to detect potential stresses or anomalies.
4. Preparation prevents common issues: Understand application compatibility and test thoroughly in staging environments.
5. Documentation improves processes: Keep clear records of upgrade steps, troubleshooting insights, and rollback plans.</p></div>
<div class="paragraph"><p>Remember:
- Test thoroughly in non-production environments.
- Continuously monitor the cluster during and after the upgrade.
- Document each step, including any lessons learned, for future reference.
- Prepare for rollbacks by disabling auto-finalization and retaining older binaries until confident in the new version’s stability.</p></div></aside></div></section><section id="_exercise_preview_version_management"><h2>Exercise Preview: Version Management</h2><div class="slide-content"><div class="ulist"><ul><li><p>Configure auto-finalization settings</p></li><li><p>Perform health checks</p></li><li><p>Execute rolling upgrades</p></li><li><p>Monitor and validate</p></li><li><p>Practice rollback procedures</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The upcoming exercise will provide hands-on experience with:</p></div>
<div class="paragraph"><p>Key Activities:
1. Configuring cluster auto-finalization options and verifying the setting:
   [source,sql]
   ----
   SHOW CLUSTER SETTING upgrade.auto_finalize_enabled;
   ----
2. Performing detailed health checks to ensure the cluster is ready for an upgrade.
3. Executing a rolling upgrade across multiple nodes and validating each step (including node draining and binary replacement).
4. Monitoring key metrics (under-replicated ranges, CPU usage, memory usage) to assess stability.
5. Practicing rollback procedures before finalization to confirm you can safely revert if unexpected problems appear.</p></div>
<div class="paragraph"><p>Learning Objectives:
- Understand the impact of version management on cluster stability.
- Implement robust pre-upgrade checks and backups.
- Perform rolling upgrades with minimal downtime.
- Validate success and manage potential rollbacks for safe operations.</p></div></aside></div></section></section>
<section><section id="_application_behavior_settings"><h2>Application Behavior Settings</h2></section><section id="_why_application_settings_matter"><h2>Why Application Settings Matter</h2><div class="slide-content"><div class="ulist"><ul><li><p>Protect cluster stability and performance</p></li><li><p>Balance application needs with system resources</p></li><li><p>Prevent problematic workload patterns</p></li><li><p>Enable effective troubleshooting</p></li><li><p>Foster collaboration between ops and dev teams</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Application behavior settings are crucial because they:
- Act as guardrails for system stability by limiting or shaping how applications can interact with the database. For example, restricting excessively large transactions can protect other workloads from performance degradation.
- Help prevent resource exhaustion: By imposing per-session or per-transaction limits, you avoid scenarios where a single misbehaving query or application can overwhelm the cluster.
- Enable identification of problematic patterns: Logging or restricting certain SQL behaviors can reveal inefficiencies (e.g., excessive writes, large in-flight transactions, or repeated full-table scans).
- Support proper capacity planning: Knowing the boundary conditions (like maximum row size or max transactions in flight) helps plan resource scaling.
- Create clear boundaries for applications: Developers and operators benefit from well-documented limits around transaction sizes, row sizes, and concurrency.</p></div>
<div class="paragraph"><p>Additionally, these settings help both operators and developers work together. Ops can enforce high-level constraints while dev teams can fine-tune session-specific behaviors to best suit their application requirements.</p></div></aside></div></section><section id="_sql_defaults_vs_session_settings"><h2>SQL Defaults vs Session Settings</h2><div class="slide-content"><div class="ulist"><ul><li><p>Cluster-wide defaults provide baseline behavior</p></li><li><p>Session settings enable application-specific overrides</p></li><li><p>Role-based defaults target specific use cases</p></li><li><p>Changes affect only new sessions</p></li><li><p>Clear communication prevents conflicts</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Two layers of settings control how queries and transactions are run:</p></div>
<div class="paragraph"><p>Cluster Defaults:
- Set the baseline or "global" behavior for all new connections. For instance, you can enforce a default transaction isolation level, or a particular date style across the cluster.
- Modified with <code>CLUSTER SETTING</code> commands. For example:
  [source,sql]
  ----
  SET CLUSTER SETTING sql.defaults.transaction_isolation = 'serializable';
  ----
- Do not affect existing sessions already established.
- Provide a safety net when multiple applications share the same cluster.</p></div>
<div class="paragraph"><p>Session Variables:
- Allow fine-grained customization for specific use cases or applications. For example:
  [source,sql]
  ----
  SET application_name = 'MyAppV1';
  SET search_path = 'public,myschema';
  ----
- Only affect the current connection; the settings revert to defaults when the session ends.
- Useful when different applications (or modules within the same application) have distinct needs for concurrency, isolation, or timeouts.
- Role-based or user-based defaults can be applied to help specific groups or services adopt a certain baseline without affecting the entire cluster.</p></div>
<div class="paragraph"><p>Clear communication with dev teams is critical. They need to know the cluster defaults so they can decide whether session-level overrides are necessary, and to avoid conflicts that could degrade performance or lead to unexpected behavior.</p></div></aside></div></section><section id="_write_path_protection"><h2>Write Path Protection</h2><div class="slide-content"><div class="ulist"><ul><li><p>Transaction row limits prevent resource exhaustion</p></li><li><p>Separate thresholds for logging and errors</p></li><li><p>Batching strategies for large operations</p></li><li><p>Role-specific limits for different workloads</p></li><li><p>Monitoring helps identify trends</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Write path protection is a safety mechanism to guard against overly large or runaway write operations. Common ways CockroachDB addresses this:</p></div>
<div class="paragraph"><p>Key Settings:
- <code>sql.guardrails.transaction_rows_written_err</code>: A hard limit on the total number of rows a single transaction can write. Exceeding it will result in an error.
- <code>sql.guardrails.transaction_rows_written_log</code>: A warning threshold that, if crossed, logs an event but does not abort the transaction. This helps detect potentially excessive write workloads early.</p></div>
<div class="paragraph"><p>Batching Strategies:
- When you anticipate large data loads—such as bulk inserts or heavy ETL processes—implement batching to write data in segments (e.g., 1,000 rows per transaction) to prevent hitting hard limits.
- Use facilities like the IMPORT statement or built-in CSV import features for large-scale data ingestion, which are optimized to handle large datasets more safely.</p></div>
<div class="paragraph"><p>Role-Specific Limits:
- By using role-based settings (if supported by your CockroachDB version), different teams or microservices can have distinct thresholds for warnings and errors. For instance, a data ingestion role might need higher logging thresholds than an OLTP application role.</p></div>
<div class="paragraph"><p>Monitoring:
- Keep an eye on logs and metrics in the Admin UI or your logging stack to identify frequent threshold breaches. Frequent logs may indicate a need to adjust thresholds or optimize application-side batching.</p></div></aside></div></section><section id="_cluster_guardrails"><h2>Cluster Guardrails</h2><div class="slide-content"><div class="ulist"><ul><li><p>Provide hard limits that cannot be overridden</p></li><li><p>Control maximum row sizes</p></li><li><p>Separate logging and error thresholds</p></li><li><p>Protect against resource exhaustion</p></li><li><p>Help identify problematic access patterns</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Cluster guardrails extend beyond transactional row limits to safeguard the overall data model:</p></div>
<div class="paragraph"><p>Key Settings:
- <code>sql.guardrails.max_row_size_err</code>: Defines the maximum allowed row size in bytes. If a write exceeds this limit, it triggers an error.
- <code>sql.guardrails.max_row_size_log</code>: A warning threshold that logs whenever a row exceeds the specified size but does not block the write.</p></div>
<div class="paragraph"><p>Implementation Strategy:
- Start with values that align with typical row sizes in your schema design. For example, if your largest row is around 8 KB, setting a log threshold at 16 KB and an error threshold at 32 KB might be a reasonable starting point.
- Monitor the logs for warnings indicating row sizes nearing the log threshold. This can reveal unforeseen usage patterns or suboptimal schema designs (like storing large BLOBs without partitioning).
- Communicate these guardrails to developers, so they don’t accidentally exceed them with new features or unexpected usage patterns.
- If needed, make measured adjustments, balancing performance and resource usage. Very large row sizes may slow down queries, backups, and rebalancing operations.</p></div></aside></div></section><section id="_transaction_size_management"><h2>Transaction Size Management</h2><div class="slide-content"><div class="ulist"><ul><li><p>Controls maximum pending write intents</p></li><li><p>Affects large transaction handling</p></li><li><p>Query optimization before limit adjustment</p></li><li><p>Batching strategies for large operations</p></li><li><p>Monitoring guides tuning decisions</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Transaction size management in CockroachDB ensures that large or complex transactions do not overwhelm the system:</p></div>
<div class="paragraph"><p>Key Concepts:
- Write intents are placeholders for pending changes within an open transaction. If a transaction accumulates too many write intents, it can slow down the system or risk running out of memory.
- CockroachDB offers cluster settings (e.g., <code>kv.transaction.max_intents</code>) to cap how many write intents a single transaction can hold before the database rejects or logs it.</p></div>
<div class="paragraph"><p>Best Practices:
1. <strong>Analyze and optimize queries first</strong>: Identify if queries can be split or if the data model can be reorganized to reduce the size of each transaction.
2. <strong>Batching</strong>: Similar to write path protection, break large updates/inserts into multiple smaller transactions to reduce the risk of exceeding intent limits.
3. <strong>Monitor transaction sizes</strong>: Tools like the Admin UI, system ranges, and logs help pinpoint if large transactions are frequently approaching limits.
4. <strong>Adjust limits carefully</strong>: Use incremental changes, then reassess performance. Overly generous limits can put the cluster at risk of memory pressure or performance degradation.
5. <strong>Document</strong>: Any time you deviate from defaults, record the justification and keep application teams informed so they can adapt queries or workloads accordingly.</p></div></aside></div></section><section id="_best_practices_for_setting_changes"><h2>Best Practices for Setting Changes</h2><div class="slide-content"><div class="ulist"><ul><li><p>Document all cluster-level changes</p></li><li><p>Start with logging before implementing errors</p></li><li><p>Monitor impact of changes</p></li><li><p>Communicate with application teams</p></li><li><p>Plan for existing session handling</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Effective setting management requires:</p></div>
<div class="paragraph"><p>Documentation:
- Log every change in a centralized location (e.g., wiki, version control, or an internal knowledge base).
- Include the rationale for each setting adjustment, the exact command used, and any observed or expected impacts.
- Track historical patterns: If a setting is changed repeatedly, it may indicate an underlying architectural issue.</p></div>
<div class="paragraph"><p>Communication:
- When raising thresholds from warnings to errors (or lowering them for more restrictive behavior), let application teams know the timeline, so they can adjust queries or code if needed.
- Clarify how changes affect existing sessions. Some changes only apply to new connections, so applications might need restarts or new connections to see updated behaviors.</p></div>
<div class="paragraph"><p>Planning:
- Always test new settings in a non-production or staging environment, where you can safely replicate production-like workloads.
- Roll out changes gradually if possible, especially in large clusters. Rapid, cluster-wide changes can create unexpected performance spikes.</p></div></aside></div></section><section id="_summary_7"><h2>Summary</h2><div class="slide-content"><div class="ulist"><ul><li><p>Settings protect cluster stability</p></li><li><p>Balancing flexibility with protection</p></li><li><p>Clear communication is essential</p></li><li><p>Monitoring guides adjustments</p></li><li><p>Documentation supports maintenance</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Key takeaways:
1. Settings protect cluster health by preventing runaway processes or resource misuse.
2. Multiple layers of protection ensure each environment can have different thresholds, from cluster defaults to session overrides.
3. Communication prevents issues: Dev and Ops alignment on limits is crucial to avoid breaking workloads or overloading the cluster.
4. Monitoring informs decisions: Adjust thresholds based on actual metrics and logs, not just guesswork.
5. Documentation ensures consistency: Proper records of setting changes prevent confusion and guide future maintenance.</p></div>
<div class="paragraph"><p>Remember:
- Start with monitoring to identify normal usage and detect anomalies.
- Communicate changes early and frequently to avoid surprises.
- Document all changes so you can roll them back if necessary or revisit them later.
- Plan for growth, as workload patterns often evolve over time.</p></div></aside></div></section><section id="_exercise_preview_5"><h2>Exercise Preview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Configure SQL defaults and session variables</p></li><li><p>Implement write path protection</p></li><li><p>Set up cluster guardrails</p></li><li><p>Manage large transactions</p></li><li><p>Monitor setting impacts</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The upcoming exercise provides hands-on experience with:</p></div>
<div class="paragraph"><p>Learning Objectives:
- Configure cluster-level defaults (e.g., transaction limits, row size guardrails) and per-session overrides for specialized workloads.
- Implement protection mechanisms around write paths, such as logging and error thresholds, to safeguard cluster performance.
- Monitor the effectiveness of these settings, using logs, the Admin UI, or external observability tools, to detect potential issues early.
- Manage large transactions by splitting or batching operations to stay within configured intent limits or row thresholds.
- Communicate changes effectively and document them thoroughly, ensuring a shared understanding with development teams.</p></div>
<div class="paragraph"><p>You will:
1. Work with cluster settings (e.g., <code>SET CLUSTER SETTING sql.guardrails.transaction_rows_written_log = 50000;</code>).
2. Configure write protection, using both logging and error thresholds.
3. Set up guardrails for row sizes and transaction sizes.
4. Handle large transactions via optimized queries or batching strategies.
5. Practice monitoring and validation techniques to confirm that your changes have the intended effect without harming performance.</p></div></aside></div></section></section>
<section><section id="_ttl_management_in_cockroachdb"><h2>TTL Management in CockroachDB</h2></section><section id="_why_row_level_ttl_matters"><h2>Why Row-Level TTL Matters</h2><div class="slide-content"><div class="ulist"><ul><li><p>Automated data lifecycle management is crucial for modern applications</p></li><li><p>Manual cleanup processes are error-prone and resource-intensive</p></li><li><p>Regulatory compliance often requires systematic data removal</p></li><li><p>TTL provides a reliable, performant solution for data expiration</p></li><li><p>Reduces operational overhead and ensures consistent data management</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Row-Level TTL is a critical feature for modern database management. It automates
the process of removing expired data, which is essential for maintaining
database performance, ensuring compliance with data retention policies, and
implementing efficient data lifecycle management. Without TTL, organizations
often rely on complex application-level code or manual processes, which can be
unreliable and resource-intensive.</p></div>
<div class="paragraph"><p>CockroachDB&#8217;s TTL mechanism ensures that expired data is removed efficiently
without requiring manual intervention, reducing the risk of orphaned or outdated
records. This feature is especially useful in applications dealing with
temporary or compliance-sensitive data, such as session logs, transaction
histories, and time-limited offers.</p></div></aside></div></section><section id="_understanding_row_level_ttl"><h2>Understanding Row-Level TTL</h2><div class="slide-content"><div class="ulist"><ul><li><p>TTL automatically removes expired data based on time conditions</p></li><li><p>Works at the storage layer for optimal performance</p></li><li><p>Supports both absolute timestamps and relative time expressions</p></li><li><p>Configurable scheduling with cron-style expressions</p></li><li><p>Balances cleanup operations with system performance</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Row-Level TTL in CockroachDB operates directly at the <strong>storage layer</strong>,
ensuring high efficiency and minimal impact on query performance. Unlike
application-driven deletions, TTL does not require explicit <code>DELETE</code> statements,
as expiration policies are automatically enforced.</p></div>
<div class="paragraph"><p>TTL can be configured using:</p></div>
<div class="ulist"><ul><li><p><strong>Absolute timestamps</strong>: A column that explicitly defines when a row should be deleted.</p></li><li><p><strong>Relative time expressions</strong>: Defines a time-based retention period (e.g., <code>ttl_expire_after = '30d'</code>).</p></li></ul></div>
<div class="paragraph"><p>CockroachDB leverages <strong>background jobs</strong> to manage TTL cleanup, and these jobs can be fine-tuned using cron-style scheduling (<code>ttl_job_cron</code>) to minimize performance impact during peak hours.</p></div></aside></div></section><section id="_implementing_ttl"><h2>Implementing TTL</h2><div class="slide-content"><div class="ulist"><ul><li><p>TTL configuration requires either <code>ttl_expiration_expression</code> or <code>ttl_expire_after</code></p></li><li><p>Expiration can be based on any timestamp column</p></li><li><p>TTL jobs run on a configurable schedule using cron syntax</p></li><li><p>Supports both <code>timestamp</code> and <code>timestamptz</code> data types</p></li><li><p>Can be modified or paused without table restructuring</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>When implementing TTL, you must specify either <code>ttl_expiration_expression</code> or
<code>ttl_expire_after</code>. The <code>ttl_expiration_expression</code> can reference any timestamp
column in the table. The default schedule for TTL jobs is <strong>daily</strong> (<code>@daily</code>),
but it can be customized using cron syntax.</p></div>
<div class="paragraph"><p><strong>Example</strong>: Defining TTL Based on an Expiration Column</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">CREATE TABLE user_sessions (
    session_id UUID PRIMARY KEY,
    created_at TIMESTAMPTZ DEFAULT now(),
    expires_at TIMESTAMPTZ,
    user_id UUID REFERENCES users(id)
) WITH (
    ttl_expiration_expression = 'expires_at',
    ttl_job_cron = '@hourly'
);</code></pre></div></div>
<div class="paragraph"><p>In this example:</p></div>
<div class="ulist"><ul><li><p>The <code>expires_at</code> column defines when each session expires.</p></li><li><p>The TTL job runs <strong>every hour</strong> (<code>@hourly</code>), ensuring timely cleanup.</p></li></ul></div>
<div class="paragraph"><p>TTL configurations can be modified dynamically:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_expire_after = '7d');
ALTER TABLE user_sessions SET (ttl_job_cron = '0 3 * * *'); -- Run cleanup daily at 3 AM UTC</code></pre></div></div>
<div class="paragraph"><p>Pausing TTL jobs is also supported:</p></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_pause = 'on');</code></pre></div></div>
<div class="paragraph"><p>This allows administrators to temporarily disable TTL cleanup without altering table schema.</p></div></aside></div></section><section id="_ttl_management_and_monitoring"><h2>TTL Management and Monitoring</h2><div class="slide-content"><div class="ulist"><ul><li><p>TTL jobs can be monitored through the DB Console</p></li><li><p>Status and metrics available via system tables</p></li><li><p>Supports pause/resume functionality for maintenance</p></li><li><p>Configurable batch sizes for performance tuning</p></li><li><p>Integration with CockroachDB&#8217;s job framework</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Monitoring TTL execution is crucial to ensure that cleanup processes do not
interfere with production workloads. CockroachDB provides multiple tools for
observing TTL jobs:</p></div>
<div class="paragraph"><p><strong>Monitoring TTL Jobs</strong></p></div>
<div class="olist arabic"><ol class="arabic"><li><p><strong>DB Console</strong>:</p><div class="ulist"><ul><li><p>The CockroachDB Web UI displays active TTL jobs, their status, and execution history.</p></li></ul></div></li><li><p><strong>Querying System Tables</strong>:</p><div class="ulist"><ul><li><p>View TTL jobs using:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">SELECT * FROM [SHOW JOBS] WHERE job_type = 'ROW LEVEL TTL';</code></pre></div></div></li><li><p>Check TTL schedules:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">SELECT * FROM [SHOW SCHEDULES] WHERE schedule_name LIKE '%TTL%';</code></pre></div></div></li></ul></div></li></ol></div>
<div class="paragraph"><p><strong>Managing TTL Jobs</strong></p></div>
<div class="ulist"><ul><li><p><strong>Pause TTL Cleanup</strong>:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_pause = 'on');</code></pre></div></div></li><li><p><strong>Resume TTL Cleanup</strong>:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_pause = 'off');</code></pre></div></div></li><li><p><strong>Modify TTL Execution Schedule</strong>:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_job_cron = '@daily');</code></pre></div></div>
<div class="paragraph"><p>This level of control allows for precise tuning of TTL operations, ensuring that
they run during optimal time windows and avoid impacting transactional
workloads.</p></div></li></ul></div></aside></div></section><section id="_best_practices_and_considerations"><h2>Best Practices and Considerations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Monitor TTL job performance impact</p></li><li><p>Consider scheduling during off-peak hours</p></li><li><p>Index design affects TTL cleanup performance</p></li><li><p>Batch size tuning may be necessary for large tables</p></li><li><p>Test TTL configuration in non-production first</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Key Best Practices for TTL Management</strong></p></div>
<div class="olist arabic"><ol class="arabic"><li><p><strong>Monitor the Impact on Performance</strong></p><div class="ulist"><ul><li><p>TTL jobs can generate a high volume of deletions, which may trigger additional write amplification due to MVCC garbage collection.</p></li><li><p>Monitor job execution time and resource utilization via:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">SELECT * FROM [SHOW JOBS] WHERE job_type = 'ROW LEVEL TTL';</code></pre></div></div></li></ul></div></li><li><p><strong>Schedule TTL Cleanup During Off-Peak Hours</strong></p><div class="ulist"><ul><li><p>By default, TTL jobs run daily (<code>@daily</code>), but modifying this to <strong>off-peak hours</strong> minimizes the impact on active transactions:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_job_cron = '0 2 * * *'); -- Run at 2 AM UTC</code></pre></div></div></li></ul></div></li><li><p><strong>Optimize Indexing for TTL Queries</strong></p><div class="ulist"><ul><li><p>TTL deletions involve scans, so having an appropriate <strong>index strategy</strong> reduces unnecessary table scans.</p></li><li><p>Ensure that <strong>the expiration column is indexed</strong> when TTL is configured based on <code>ttl_expiration_expression</code>:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">CREATE INDEX ON user_sessions (expires_at);</code></pre></div></div></li></ul></div></li><li><p><strong>Batch Size Tuning for Large Tables</strong></p><div class="ulist"><ul><li><p>The default batch size for TTL deletions may not be optimal for large datasets. Adjust this using:</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">ALTER TABLE user_sessions SET (ttl_batch_size = 5000);</code></pre></div></div></li></ul></div></li><li><p><strong>Test TTL Configurations in a Staging Environment</strong></p><div class="ulist"><ul><li><p>Before enabling TTL in production, verify behavior in a controlled environment.</p></li><li><p>Use CockroachDB&#8217;s <strong>EXPLAIN ANALYZE</strong> feature to observe execution plans before applying TTL policies.</p></li></ul></div></li></ol></div>
<div class="paragraph"><p>Following these best practices ensures that TTL jobs execute efficiently without negatively impacting database performance.</p></div></aside></div></section><section id="_summary_8"><h2>Summary</h2><div class="slide-content"><div class="ulist"><ul><li><p>TTL provides automated data lifecycle management</p></li><li><p>Configuration options support various use cases</p></li><li><p>Monitoring tools ensure visibility into TTL operations</p></li><li><p>Management features enable operational control</p></li><li><p>Exercise will provide hands-on experience</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Key takeaways:
- TTL automates data cleanup, reducing manual intervention.
- It provides configurable options for different expiration strategies.
- Multiple monitoring tools help track TTL execution and performance.
- Tuning job schedules and batch sizes optimizes efficiency.
- The exercise will reinforce these concepts with hands-on implementation.</p></div>
<div class="paragraph"><p>With TTL, CockroachDB provides a powerful mechanism for managing data retention efficiently.</p></div></aside></div></section><section id="_exercise_preview_ttl_implementation"><h2>Exercise Preview: TTL Implementation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Create a table with TTL configuration</p></li><li><p>Implement a book promotion system</p></li><li><p>Monitor TTL job execution</p></li><li><p>Manage TTL operations and scheduling</p></li><li><p>Observe automatic data cleanup</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In the upcoming exercise, you will:</p></div>
<div class="olist arabic"><ol class="arabic"><li><p><strong>Define TTL for a table</strong> that automatically removes expired book promotions.</p></li><li><p><strong>Configure custom TTL expiration rules</strong> and job schedules.</p></li><li><p><strong>Monitor TTL execution</strong> using CockroachDB system tables and logs.</p></li><li><p><strong>Pause and resume TTL operations</strong> dynamically.</p></li><li><p><strong>Observe data cleanup</strong> as expired rows are deleted.</p></li></ol></div>
<div class="paragraph"><p>This hands-on experience will reinforce key TTL concepts and provide practical exposure to managing TTL in CockroachDB.</p></div></aside></div></section></section></div></div><script src="assets/reveal.js/dist/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
});

// More info about config & dependencies:
// - https://github.com/hakimel/reveal.js#configuration
// - https://github.com/hakimel/reveal.js#dependencies
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: false,
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Add the current slide number to the URL hash so that reloading the
  // page/copying the URL will return you to the same slide
  hash: true,
  // Push each slide change to the browser history. Implies `hash: true`
  history: false,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Disables the default reveal.js slide layout so that you can use custom CSS layout
  disableLayout: false,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // See https://github.com/hakimel/reveal.js/#navigation-mode
  navigationMode: 'default',
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Global override for preloading lazy-loaded iframes
  // - null: Iframes with data-src AND data-preload will be loaded when within
  //   the viewDistance, iframes with only data-src will be loaded when visible
  // - true: All iframes with data-src will be loaded when within the viewDistance
  // - false: All iframes with data-src will be loaded only when visible
  preloadIframes: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Specify the total time in seconds that is available to
  // present.  If this is set to a nonzero value, the pacing
  // timer will work out the time available for each slide,
  // instead of using the defaultTiming value
  totalTime: 0,
  // Specify the minimum amount of time you want to allot to
  // each slide, if using the totalTime calculation method.  If
  // the automated time allocation causes slide pacing to fall
  // below this threshold, then you will see an alert in the
  // speaker notes window
  minimumTimePerSlide: 0,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hide cursor if inactive
  hideInactiveCursor: true,
  // Time before the cursor is hidden (in ms)
  hideCursorTime: 5000,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Number of slides away from the current that are visible on mobile
  // devices. It is advisable to set this to a lower number than
  // viewDistance in order to save resources.
  mobileViewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.04,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 2,

  // PDF Export Options
  // Put each fragment on a separate page
  pdfSeparateFragments: true,
  // For slides that do not fit on a page, max number of pages
  pdfMaxPagesPerSlide: 1,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'assets/reveal.js/plugin/zoom/zoom.js', async: true, callback: function () { Reveal.registerPlugin(RevealZoom) } },
      { src: 'assets/reveal.js/plugin/notes/notes.js', async: true, callback: function () { Reveal.registerPlugin(RevealNotes) } }
  ],
});</script><script>var dom = {};
dom.slides = document.querySelector('.reveal .slides');

function getRemainingHeight(element, slideElement, height) {
  height = height || 0;
  if (element) {
    var newHeight, oldHeight = element.style.height;
    // Change the .stretch element height to 0 in order find the height of all
    // the other elements
    element.style.height = '0px';
    // In Overview mode, the parent (.slide) height is set of 700px.
    // Restore it temporarily to its natural height.
    slideElement.style.height = 'auto';
    newHeight = height - slideElement.offsetHeight;
    // Restore the old height, just in case
    element.style.height = oldHeight + 'px';
    // Clear the parent (.slide) height. .removeProperty works in IE9+
    slideElement.style.removeProperty('height');
    return newHeight;
  }
  return height;
}

function layoutSlideContents(width, height) {
  // Handle sizing of elements with the 'stretch' class
  toArray(dom.slides.querySelectorAll('section .stretch')).forEach(function (element) {
    // Determine how much vertical space we can use
    var limit = 5; // hard limit
    var parent = element.parentNode;
    while (parent.nodeName !== 'SECTION' && limit > 0) {
      parent = parent.parentNode;
      limit--;
    }
    if (limit === 0) {
      // unable to find parent, aborting!
      return;
    }
    var remainingHeight = getRemainingHeight(element, parent, height);
    // Consider the aspect ratio of media elements
    if (/(img|video)/gi.test(element.nodeName)) {
      var nw = element.naturalWidth || element.videoWidth, nh = element.naturalHeight || element.videoHeight;
      var es = Math.min(width / nw, remainingHeight / nh);
      element.style.width = (nw * es) + 'px';
      element.style.height = (nh * es) + 'px';
    } else {
      element.style.width = width + 'px';
      element.style.height = remainingHeight + 'px';
    }
  });
}

function toArray(o) {
  return Array.prototype.slice.call(o);
}

Reveal.addEventListener('slidechanged', function () {
  layoutSlideContents(960, 700)
});
Reveal.addEventListener('ready', function () {
  layoutSlideContents(960, 700)
});
Reveal.addEventListener('resize', function () {
  layoutSlideContents(960, 700)
});</script><link rel="stylesheet" href="assets/theme/github.css"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.3/highlight.min.js"></script>

<script>

/* highlightjs-line-numbers.js 2.6.0 | (C) 2018 Yauheni Pakala | MIT License | github.com/wcoder/highlightjs-line-numbers.js */
/* Edited by Hakim for reveal.js; removed async timeout */
!function(n,e){"use strict";function t(){var n=e.createElement("style");n.type="text/css",n.innerHTML=g(".{0}{border-collapse:collapse}.{0} td{padding:0}.{1}:before{content:attr({2})}",[v,L,b]),e.getElementsByTagName("head")[0].appendChild(n)}function r(t){"interactive"===e.readyState||"complete"===e.readyState?i(t):n.addEventListener("DOMContentLoaded",function(){i(t)})}function i(t){try{var r=e.querySelectorAll("code.hljs,code.nohighlight");for(var i in r)r.hasOwnProperty(i)&&l(r[i],t)}catch(o){n.console.error("LineNumbers error: ",o)}}function l(n,e){"object"==typeof n&&f(function(){n.innerHTML=s(n,e)})}function o(n,e){if("string"==typeof n){var t=document.createElement("code");return t.innerHTML=n,s(t,e)}}function s(n,e){e=e||{singleLine:!1};var t=e.singleLine?0:1;return c(n),a(n.innerHTML,t)}function a(n,e){var t=u(n);if(""===t[t.length-1].trim()&&t.pop(),t.length>e){for(var r="",i=0,l=t.length;i<l;i++)r+=g('<tr><td class="{0}"><div class="{1} {2}" {3}="{5}"></div></td><td class="{4}"><div class="{1}">{6}</div></td></tr>',[j,m,L,b,p,i+1,t[i].length>0?t[i]:" "]);return g('<table class="{0}">{1}</table>',[v,r])}return n}function c(n){var e=n.childNodes;for(var t in e)if(e.hasOwnProperty(t)){var r=e[t];h(r.textContent)>0&&(r.childNodes.length>0?c(r):d(r.parentNode))}}function d(n){var e=n.className;if(/hljs-/.test(e)){for(var t=u(n.innerHTML),r=0,i="";r<t.length;r++){var l=t[r].length>0?t[r]:" ";i+=g('<span class="{0}">{1}</span>\n',[e,l])}n.innerHTML=i.trim()}}function u(n){return 0===n.length?[]:n.split(y)}function h(n){return(n.trim().match(y)||[]).length}function f(e){e()}function g(n,e){return n.replace(/{(\d+)}/g,function(n,t){return e[t]?e[t]:n})}var v="hljs-ln",m="hljs-ln-line",p="hljs-ln-code",j="hljs-ln-numbers",L="hljs-ln-n",b="data-line-number",y=/\r\n|\r|\n/g;n.hljs?(n.hljs.initLineNumbersOnLoad=r,n.hljs.lineNumbersBlock=l,n.hljs.lineNumbersValue=o,t()):n.console.error("highlight.js not detected!")}(window,document);

/**
 * This reveal.js plugin is wrapper around the highlight.js
 * syntax highlighting library.
 */
(function( root, factory ) {
  if (typeof define === 'function' && define.amd) {
    root.RevealHighlight = factory();
  } else if( typeof exports === 'object' ) {
    module.exports = factory();
  } else {
    // Browser globals (root is window)
    root.RevealHighlight = factory();
  }
}( this, function() {

  // Function to perform a better "data-trim" on code snippets
  // Will slice an indentation amount on each line of the snippet (amount based on the line having the lowest indentation length)
  function betterTrim(snippetEl) {
    // Helper functions
    function trimLeft(val) {
      // Adapted from https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/Trim#Polyfill
      return val.replace(/^[\s\uFEFF\xA0]+/g, '');
    }
    function trimLineBreaks(input) {
      var lines = input.split('\n');

      // Trim line-breaks from the beginning
      for (var i = 0; i < lines.length; i++) {
        if (lines[i].trim() === '') {
          lines.splice(i--, 1);
        } else break;
      }

      // Trim line-breaks from the end
      for (var i = lines.length-1; i >= 0; i--) {
        if (lines[i].trim() === '') {
          lines.splice(i, 1);
        } else break;
      }

      return lines.join('\n');
    }

    // Main function for betterTrim()
    return (function(snippetEl) {
      var content = trimLineBreaks(snippetEl.innerHTML);
      var lines = content.split('\n');
      // Calculate the minimum amount to remove on each line start of the snippet (can be 0)
      var pad = lines.reduce(function(acc, line) {
        if (line.length > 0 && trimLeft(line).length > 0 && acc > line.length - trimLeft(line).length) {
          return line.length - trimLeft(line).length;
        }
        return acc;
      }, Number.POSITIVE_INFINITY);
      // Slice each line with this amount
      return lines.map(function(line, index) {
        return line.slice(pad);
      })
        .join('\n');
    })(snippetEl);
  }

  var RevealHighlight = {

    HIGHLIGHT_STEP_DELIMITER: '|',
    HIGHLIGHT_LINE_DELIMITER: ',',
    HIGHLIGHT_LINE_RANGE_DELIMITER: '-',

    init: function( reveal ) {

      // Read the plugin config options and provide fallbacks
      var config = Reveal.getConfig().highlight || {};
      config.highlightOnLoad = typeof config.highlightOnLoad === 'boolean' ? config.highlightOnLoad : true;
      config.escapeHTML = typeof config.escapeHTML === 'boolean' ? config.escapeHTML : true;

      [].slice.call( reveal.getRevealElement().querySelectorAll( 'pre code' ) ).forEach( function( block ) {

        block.parentNode.className = 'code-wrapper';

        // Code can optionally be wrapped in script template to avoid
        // HTML being parsed by the browser (i.e. when you need to
        // include <, > or & in your code).
        let substitute = block.querySelector( 'script[type="text/template"]' );
        if( substitute ) {
          // textContent handles the HTML entity escapes for us
          block.textContent = substitute.innerHTML;
        }

        // Trim whitespace if the "data-trim" attribute is present
        if( block.hasAttribute( 'data-trim' ) && typeof block.innerHTML.trim === 'function' ) {
          block.innerHTML = betterTrim( block );
        }

        // Escape HTML tags unless the "data-noescape" attrbute is present
        if( config.escapeHTML && !block.hasAttribute( 'data-noescape' )) {
          block.innerHTML = block.innerHTML.replace( /</g,"&lt;").replace(/>/g, '&gt;' );
        }

        // Re-highlight when focus is lost (for contenteditable code)
        block.addEventListener( 'focusout', function( event ) {
          hljs.highlightElement( event.currentTarget );
        }, false );

        if( config.highlightOnLoad ) {
          RevealHighlight.highlightBlock( block );
        }
      } );

      // If we're printing to PDF, scroll the code highlights of
      // all blocks in the deck into view at once
      reveal.on( 'pdf-ready', function() {
        [].slice.call( reveal.getRevealElement().querySelectorAll( 'pre code[data-line-numbers].current-fragment' ) ).forEach( function( block ) {
          RevealHighlight.scrollHighlightedLineIntoView( block, {}, true );
        } );
      } );
    },

    /**
     * Highlights a code block. If the <code> node has the
     * 'data-line-numbers' attribute we also generate slide
     * numbers.
     *
     * If the block contains multiple line highlight steps,
     * we clone the block and create a fragment for each step.
     */
    highlightBlock: function( block ) {

      hljs.highlightElement( block );

      // Don't generate line numbers for empty code blocks
      if( block.innerHTML.trim().length === 0 ) return;

      if( block.hasAttribute( 'data-line-numbers' ) ) {
        hljs.lineNumbersBlock( block, { singleLine: true } );

        var scrollState = { currentBlock: block };

        // If there is at least one highlight step, generate
        // fragments
        var highlightSteps = RevealHighlight.deserializeHighlightSteps( block.getAttribute( 'data-line-numbers' ) );
        if( highlightSteps.length > 1 ) {

          // If the original code block has a fragment-index,
          // each clone should follow in an incremental sequence
          var fragmentIndex = parseInt( block.getAttribute( 'data-fragment-index' ), 10 );

          if( typeof fragmentIndex !== 'number' || isNaN( fragmentIndex ) ) {
            fragmentIndex = null;
          }

          // Generate fragments for all steps except the original block
          highlightSteps.slice(1).forEach( function( highlight ) {

            var fragmentBlock = block.cloneNode( true );
            fragmentBlock.setAttribute( 'data-line-numbers', RevealHighlight.serializeHighlightSteps( [ highlight ] ) );
            fragmentBlock.classList.add( 'fragment' );
            block.parentNode.appendChild( fragmentBlock );
            RevealHighlight.highlightLines( fragmentBlock );

            if( typeof fragmentIndex === 'number' ) {
              fragmentBlock.setAttribute( 'data-fragment-index', fragmentIndex );
              fragmentIndex += 1;
            }
            else {
              fragmentBlock.removeAttribute( 'data-fragment-index' );
            }

            // Scroll highlights into view as we step through them
            fragmentBlock.addEventListener( 'visible', RevealHighlight.scrollHighlightedLineIntoView.bind( Plugin, fragmentBlock, scrollState ) );
            fragmentBlock.addEventListener( 'hidden', RevealHighlight.scrollHighlightedLineIntoView.bind( Plugin, fragmentBlock.previousSibling, scrollState ) );

          } );

          block.removeAttribute( 'data-fragment-index' )
          block.setAttribute( 'data-line-numbers', RevealHighlight.serializeHighlightSteps( [ highlightSteps[0] ] ) );

        }

        // Scroll the first highlight into view when the slide
        // becomes visible. Note supported in IE11 since it lacks
        // support for Element.closest.
        var slide = typeof block.closest === 'function' ? block.closest( 'section:not(.stack)' ) : null;
        if( slide ) {
          var scrollFirstHighlightIntoView = function() {
            RevealHighlight.scrollHighlightedLineIntoView( block, scrollState, true );
            slide.removeEventListener( 'visible', scrollFirstHighlightIntoView );
          }
          slide.addEventListener( 'visible', scrollFirstHighlightIntoView );
        }

        RevealHighlight.highlightLines( block );

      }

    },

    /**
     * Animates scrolling to the first highlighted line
     * in the given code block.
     */
    scrollHighlightedLineIntoView: function( block, scrollState, skipAnimation ) {

      cancelAnimationFrame( scrollState.animationFrameID );

      // Match the scroll position of the currently visible
      // code block
      if( scrollState.currentBlock ) {
        block.scrollTop = scrollState.currentBlock.scrollTop;
      }

      // Remember the current code block so that we can match
      // its scroll position when showing/hiding fragments
      scrollState.currentBlock = block;

      var highlightBounds = RevealHighlight.getHighlightedLineBounds( block )
      var viewportHeight = block.offsetHeight;

      // Subtract padding from the viewport height
      var blockStyles = getComputedStyle( block );
      viewportHeight -= parseInt( blockStyles.paddingTop ) + parseInt( blockStyles.paddingBottom );

      // Scroll position which centers all highlights
      var startTop = block.scrollTop;
      var targetTop = highlightBounds.top + ( Math.min( highlightBounds.bottom - highlightBounds.top, viewportHeight ) - viewportHeight ) / 2;

      // Account for offsets in position applied to the
      // <table> that holds our lines of code
      var lineTable = block.querySelector( '.hljs-ln' );
      if( lineTable ) targetTop += lineTable.offsetTop - parseInt( blockStyles.paddingTop );

      // Make sure the scroll target is within bounds
      targetTop = Math.max( Math.min( targetTop, block.scrollHeight - viewportHeight ), 0 );

      if( skipAnimation === true || startTop === targetTop ) {
        block.scrollTop = targetTop;
      }
      else {

        // Don't attempt to scroll if there is no overflow
        if( block.scrollHeight <= viewportHeight ) return;

        var time = 0;
        var animate = function() {
          time = Math.min( time + 0.02, 1 );

          // Update our eased scroll position
          block.scrollTop = startTop + ( targetTop - startTop ) * RevealHighlight.easeInOutQuart( time );

          // Keep animating unless we've reached the end
          if( time < 1 ) {
            scrollState.animationFrameID = requestAnimationFrame( animate );
          }
        };

        animate();

      }

    },

    /**
     * The easing function used when scrolling.
     */
    easeInOutQuart: function( t ) {

      // easeInOutQuart
      return t<.5 ? 8*t*t*t*t : 1-8*(--t)*t*t*t;

    },

    getHighlightedLineBounds: function( block ) {

      var highlightedLines = block.querySelectorAll( '.highlight-line' );
      if( highlightedLines.length === 0 ) {
        return { top: 0, bottom: 0 };
      }
      else {
        var firstHighlight = highlightedLines[0];
        var lastHighlight = highlightedLines[ highlightedLines.length -1 ];

        return {
          top: firstHighlight.offsetTop,
          bottom: lastHighlight.offsetTop + lastHighlight.offsetHeight
        }
      }

    },

    /**
     * Visually emphasize specific lines within a code block.
     * This only works on blocks with line numbering turned on.
     *
     * @param {HTMLElement} block a <code> block
     * @param {String} [linesToHighlight] The lines that should be
     * highlighted in this format:
     * "1" 		= highlights line 1
     * "2,5"	= highlights lines 2 & 5
     * "2,5-7"	= highlights lines 2, 5, 6 & 7
     */
    highlightLines: function( block, linesToHighlight ) {

      var highlightSteps = RevealHighlight.deserializeHighlightSteps( linesToHighlight || block.getAttribute( 'data-line-numbers' ) );

      if( highlightSteps.length ) {

        highlightSteps[0].forEach( function( highlight ) {

          var elementsToHighlight = [];

          // Highlight a range
          if( typeof highlight.end === 'number' ) {
            elementsToHighlight = [].slice.call( block.querySelectorAll( 'table tr:nth-child(n+'+highlight.start+'):nth-child(-n+'+highlight.end+')' ) );
          }
          // Highlight a single line
          else if( typeof highlight.start === 'number' ) {
            elementsToHighlight = [].slice.call( block.querySelectorAll( 'table tr:nth-child('+highlight.start+')' ) );
          }

          if( elementsToHighlight.length ) {
            elementsToHighlight.forEach( function( lineElement ) {
              lineElement.classList.add( 'highlight-line' );
            } );

            block.classList.add( 'has-highlights' );
          }

        } );

      }

    },

    /**
     * Parses and formats a user-defined string of line
     * numbers to highlight.
     *
     * @example
     * RevealHighlight.deserializeHighlightSteps( '1,2|3,5-10' )
     * // [
     * //   [ { start: 1 }, { start: 2 } ],
     * //   [ { start: 3 }, { start: 5, end: 10 } ]
     * // ]
     */
    deserializeHighlightSteps: function( highlightSteps ) {

      // Remove whitespace
      highlightSteps = highlightSteps.replace( /\s/g, '' );

      // Divide up our line number groups
      highlightSteps = highlightSteps.split( RevealHighlight.HIGHLIGHT_STEP_DELIMITER );

      return highlightSteps.map( function( highlights ) {

        return highlights.split( RevealHighlight.HIGHLIGHT_LINE_DELIMITER ).map( function( highlight ) {

          // Parse valid line numbers
          if( /^[\d-]+$/.test( highlight ) ) {

            highlight = highlight.split( RevealHighlight.HIGHLIGHT_LINE_RANGE_DELIMITER );

            var lineStart = parseInt( highlight[0], 10 ),
              lineEnd = parseInt( highlight[1], 10 );

            if( isNaN( lineEnd ) ) {
              return {
                start: lineStart
              };
            }
            else {
              return {
                start: lineStart,
                end: lineEnd
              };
            }

          }
          // If no line numbers are provided, no code will be highlighted
          else {

            return {};

          }

        } );

      } );

    },

    /**
     * Serializes parsed line number data into a string so
     * that we can store it in the DOM.
     */
    serializeHighlightSteps: function( highlightSteps ) {

      return highlightSteps.map( function( highlights ) {

        return highlights.map( function( highlight ) {

          // Line range
          if( typeof highlight.end === 'number' ) {
            return highlight.start + RevealHighlight.HIGHLIGHT_LINE_RANGE_DELIMITER + highlight.end;
          }
          // Single line
          else if( typeof highlight.start === 'number' ) {
            return highlight.start;
          }
          // All lines
          else {
            return '';
          }

        } ).join( RevealHighlight.HIGHLIGHT_LINE_DELIMITER );

      } ).join( RevealHighlight.HIGHLIGHT_STEP_DELIMITER );

    }

  }

  Reveal.registerPlugin( 'highlight', RevealHighlight );

  return RevealHighlight;

}));
        
hljs.configure({
  ignoreUnescapedHTML: true,
});
hljs.highlightAll();
</script></body></html>