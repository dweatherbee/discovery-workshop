<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"><title>Day 1: Chapt 2</title><link rel="icon" type="image/x-icon" href="favicon.ico"><link rel="stylesheet" href="../assets/reveal.js/dist/reset.css"><link rel="stylesheet" href="../assets/reveal.js/dist/reveal.css"><link rel="stylesheet" href="../assets/theme/cockroachlabs-light.css" id="theme"><!--This CSS is generated by the Asciidoctor reveal.js converter to further integrate AsciiDoc's existing semantic with reveal.js--><style type="text/css">.reveal div.right {
  float: right
}

/* source blocks */
.reveal .listingblock.stretch > .content {
  height: 100%
}

.reveal .listingblock.stretch > .content > pre {
  height: 100%
}

.reveal .listingblock.stretch > .content > pre > code {
  height: 100%;
  max-height: 100%
}

/* auto-animate feature */
/* hide the scrollbar when auto-animating source blocks */
.reveal pre[data-auto-animate-target] {
  overflow: hidden;
}

.reveal pre[data-auto-animate-target] code {
  overflow: hidden;
}

/* add a min width to avoid horizontal shift on line numbers */
code.hljs .hljs-ln-line.hljs-ln-n {
  min-width: 1.25em;
}

/* tables */
table {
  border-collapse: collapse;
  border-spacing: 0
}

table {
  margin-bottom: 1.25em;
  border: solid 1px #dedede
}

table thead tr th, table thead tr td, table tfoot tr th, table tfoot tr td {
  padding: .5em .625em .625em;
  font-size: inherit;
  text-align: left
}

table tr th, table tr td {
  padding: .5625em .625em;
  font-size: inherit
}

table thead tr th, table tfoot tr th, table tbody tr td, table tr td, table tfoot tr td {
  display: table-cell;
  line-height: 1.6
}

td.tableblock > .content {
  margin-bottom: 1.25em
}

td.tableblock > .content > :last-child {
  margin-bottom: -1.25em
}

table.tableblock, th.tableblock, td.tableblock {
  border: 0 solid #dedede
}

table.grid-all > thead > tr > .tableblock, table.grid-all > tbody > tr > .tableblock {
  border-width: 0 1px 1px 0
}

table.grid-all > tfoot > tr > .tableblock {
  border-width: 1px 1px 0 0
}

table.grid-cols > * > tr > .tableblock {
  border-width: 0 1px 0 0
}

table.grid-rows > thead > tr > .tableblock, table.grid-rows > tbody > tr > .tableblock {
  border-width: 0 0 1px
}

table.grid-rows > tfoot > tr > .tableblock {
  border-width: 1px 0 0
}

table.grid-all > * > tr > .tableblock:last-child, table.grid-cols > * > tr > .tableblock:last-child {
  border-right-width: 0
}

table.grid-all > tbody > tr:last-child > .tableblock, table.grid-all > thead:last-child > tr > .tableblock, table.grid-rows > tbody > tr:last-child > .tableblock, table.grid-rows > thead:last-child > tr > .tableblock {
  border-bottom-width: 0
}

table.frame-all {
  border-width: 1px
}

table.frame-sides {
  border-width: 0 1px
}

table.frame-topbot, table.frame-ends {
  border-width: 1px 0
}

.reveal table th.halign-left, .reveal table td.halign-left {
  text-align: left
}

.reveal table th.halign-right, .reveal table td.halign-right {
  text-align: right
}

.reveal table th.halign-center, .reveal table td.halign-center {
  text-align: center
}

.reveal table th.valign-top, .reveal table td.valign-top {
  vertical-align: top
}

.reveal table th.valign-bottom, .reveal table td.valign-bottom {
  vertical-align: bottom
}

.reveal table th.valign-middle, .reveal table td.valign-middle {
  vertical-align: middle
}

table thead th, table tfoot th {
  font-weight: bold
}

tbody tr th {
  display: table-cell;
  line-height: 1.6
}

tbody tr th, tbody tr th p, tfoot tr th, tfoot tr th p {
  font-weight: bold
}

thead {
  display: table-header-group
}

.reveal table.grid-none th, .reveal table.grid-none td {
  border-bottom: 0 !important
}

/* kbd macro */
kbd {
  font-family: "Droid Sans Mono", "DejaVu Sans Mono", monospace;
  display: inline-block;
  color: rgba(0, 0, 0, .8);
  font-size: .65em;
  line-height: 1.45;
  background: #f7f7f7;
  border: 1px solid #ccc;
  -webkit-border-radius: 3px;
  border-radius: 3px;
  -webkit-box-shadow: 0 1px 0 rgba(0, 0, 0, .2), 0 0 0 .1em white inset;
  box-shadow: 0 1px 0 rgba(0, 0, 0, .2), 0 0 0 .1em #fff inset;
  margin: 0 .15em;
  padding: .2em .5em;
  vertical-align: middle;
  position: relative;
  top: -.1em;
  white-space: nowrap
}

.keyseq kbd:first-child {
  margin-left: 0
}

.keyseq kbd:last-child {
  margin-right: 0
}

/* callouts */
.conum[data-value] {
  display: inline-block;
  color: #fff !important;
  background: rgba(0, 0, 0, .8);
  -webkit-border-radius: 50%;
  border-radius: 50%;
  text-align: center;
  font-size: .75em;
  width: 1.67em;
  height: 1.67em;
  line-height: 1.67em;
  font-family: "Open Sans", "DejaVu Sans", sans-serif;
  font-style: normal;
  font-weight: bold
}

.conum[data-value] * {
  color: #fff !important
}

.conum[data-value] + b {
  display: none
}

.conum[data-value]:after {
  content: attr(data-value)
}

pre .conum[data-value] {
  position: relative;
  top: -.125em
}

b.conum * {
  color: inherit !important
}

.conum:not([data-value]):empty {
  display: none
}

/* Callout list */
.hdlist > table, .colist > table {
  border: 0;
  background: none
}

.hdlist > table > tbody > tr, .colist > table > tbody > tr {
  background: none
}

td.hdlist1, td.hdlist2 {
  vertical-align: top;
  padding: 0 .625em
}

td.hdlist1 {
  font-weight: bold;
  padding-bottom: 1.25em
}

/* Disabled from Asciidoctor CSS because it caused callout list to go under the
 * source listing when .stretch is applied (see #335)
 * .literalblock+.colist,.listingblock+.colist{margin-top:-.5em} */
.colist td:not([class]):first-child {
  padding: .4em .75em 0;
  line-height: 1;
  vertical-align: top
}

.colist td:not([class]):first-child img {
  max-width: none
}

.colist td:not([class]):last-child {
  padding: .25em 0
}

/* Override Asciidoctor CSS that causes issues with reveal.js features */
.reveal .hljs table {
  border: 0
}

/* Callout list rows would have a bottom border with some reveal.js themes (see #335) */
.reveal .colist > table th, .reveal .colist > table td {
  border-bottom: 0
}

/* Fixes line height with Highlight.js source listing when linenums enabled (see #331) */
.reveal .hljs table thead tr th, .reveal .hljs table tfoot tr th, .reveal .hljs table tbody tr td, .reveal .hljs table tr td, .reveal .hljs table tfoot tr td {
  line-height: inherit
}

/* Columns layout */
.columns .slide-content {
  display: flex;
}

.columns.wrap .slide-content {
  flex-wrap: wrap;
}

.columns.is-vcentered .slide-content {
  align-items: center;
}

.columns .slide-content > .column {
  display: block;
  flex-basis: 0;
  flex-grow: 1;
  flex-shrink: 1;
}

.columns .slide-content > .column > * {
  padding: .75rem;
}

/* See #353 */
.columns.wrap .slide-content > .column {
  flex-basis: auto;
}

.columns .slide-content > .column.is-full {
  flex: none;
  width: 100%;
}

.columns .slide-content > .column.is-four-fifths {
  flex: none;
  width: 80%;
}

.columns .slide-content > .column.is-three-quarters {
  flex: none;
  width: 75%;
}

.columns .slide-content > .column.is-two-thirds {
  flex: none;
  width: 66.6666%;
}

.columns .slide-content > .column.is-three-fifths {
  flex: none;
  width: 60%;
}

.columns .slide-content > .column.is-half {
  flex: none;
  width: 50%;
}

.columns .slide-content > .column.is-two-fifths {
  flex: none;
  width: 40%;
}

.columns .slide-content > .column.is-one-third {
  flex: none;
  width: 33.3333%;
}

.columns .slide-content > .column.is-one-quarter {
  flex: none;
  width: 25%;
}

.columns .slide-content > .column.is-one-fifth {
  flex: none;
  width: 20%;
}

.columns .slide-content > .column.has-text-left {
  text-align: left;
}

.columns .slide-content > .column.has-text-justified {
  text-align: justify;
}

.columns .slide-content > .column.has-text-right {
  text-align: right;
}

.columns .slide-content > .column.has-text-left {
  text-align: left;
}

.columns .slide-content > .column.has-text-justified {
  text-align: justify;
}

.columns .slide-content > .column.has-text-right {
  text-align: right;
}

.text-left {
  text-align: left !important
}

.text-right {
  text-align: right !important
}

.text-center {
  text-align: center !important
}

.text-justify {
  text-align: justify !important
}

.footnotes {
  border-top: 1px solid rgba(0, 0, 0, 0.2);
  padding: 0.5em 0 0 0;
  font-size: 0.65em;
  margin-top: 4em;
}

.byline {
  font-size:.8em
}
ul.byline {
  list-style-type: none;
}
ul.byline li + li {
  margin-top: 0.25em;
}
</style><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/v4-shims.min.css">
        <a href="../index.html" id="cockroachDBLogo" style="background: url(../assets/images/App_icon.svg);
                            position: absolute;
                            background-repeat: no-repeat;
                            z-index: 1000;
                            bottom: 10px;
                            left: 10px;
                            width: 50px;
                            height: 60px;">
        </a></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Day 1</h1><h2>Chapt 2</h2></section><section><section id="_ai_fundamentals_for_business_professionals"><h2>AI Fundamentals for Business Professionals</h2><div class="slide-content"><div class="paragraph"><p>Understanding the Value of AI in Business</p></div></div></section><section id="_overview"><h2>Overview</h2><div class="slide-content"><div class="ulist"><ul><li><p>Focus on AI fundamentals with emphasis on large language models (LLMs)</p></li><li><p>Provide conceptual understanding of LLMs without requiring programming knowledge</p><div class="ulist"><ul><li><p>LLMs are a foundational part of Agentic AI</p></li></ul></div></li><li><p>Prepare participants to identify Agentic AI opportunities in their business processes</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Welcome to our training module on AI Fundamentals for Business Professionals. This session is designed specifically for business professionals without technical backgrounds who want to understand how AI, particularly large language models, can create value in business contexts.</p></div>
<div class="paragraph"><p>We&#8217;ll explore the fundamentals of AI with a focus on practical business applications rather than technical implementation details. You don&#8217;t need any programming knowledge to benefit from this training - we&#8217;ll focus on concepts and applications rather than code.</p></div>
<div class="paragraph"><p>This module serves as the foundation for the remainder of our workshop, where you&#8217;ll identify business processes that could benefit from AI agent automation. The concepts we cover today will directly inform your ability to recognize valuable AI opportunities in your specific business context.</p></div>
<div class="paragraph"><p>By the end of this session, you&#8217;ll have a solid conceptual understanding of modern AI systems, particularly large language models, and be able to evaluate their potential applications in your work. This knowledge will prepare you for the more applied sessions in the coming days.</p></div></aside></div></section><section id="_learning_objectives"><h2>Learning Objectives</h2><div class="slide-content"><div class="ulist"><ul><li><p>Explain the evolution and key concepts of modern AI systems</p></li><li><p>Differentiate between types of LLMs and their capabilities</p></li><li><p>Evaluate potential business applications for AI technologies</p></li><li><p>Formulate effective prompts for different types of LLMs</p></li><li><p>Assess when to use different AI enhancement techniques like RAG and tools</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s review what we aim to accomplish in this session. By the end of this training, you&#8217;ll be able to:</p></div>
<div class="paragraph"><p>First, explain the evolution and key concepts of modern AI systems. You&#8217;ll understand how AI has developed over time and grasp the fundamental concepts that drive today&#8217;s AI technologies, particularly large language models.</p></div>
<div class="paragraph"><p>Second, differentiate between types of LLMs and their capabilities. You&#8217;ll learn about different approaches to language models, including probabilistic models like GPT-4o and reasoning-based models like Claude 3 Opus, and understand when each might be most appropriate for different business needs.</p></div>
<div class="paragraph"><p>Third, evaluate potential business applications for AI technologies. You&#8217;ll develop the ability to identify opportunities where AI can create value in your specific business context, focusing on practical applications rather than theoretical possibilities.</p></div>
<div class="paragraph"><p>Fourth, formulate effective prompts for different types of LLMs. You&#8217;ll learn the basics of prompt engineering - how to effectively communicate with AI systems to get the results you need for business tasks.</p></div>
<div class="paragraph"><p>Finally, assess when to use different AI enhancement techniques. You&#8217;ll understand approaches like Retrieval-Augmented Generation (RAG) and tools that extend AI capabilities, and be able to determine when these techniques might add value to your AI implementations.</p></div>
<div class="paragraph"><p>These objectives align with Bloom&#8217;s Taxonomy of learning, progressing from basic understanding to more complex application and evaluation skills. This structured approach ensures you&#8217;ll develop practical knowledge you can apply immediately in your business context.</p></div></aside></div></section></section>
<section><section id="_the_evolution_of_ai"><h2>The Evolution of AI</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>We&#8217;ll begin our exploration of AI fundamentals by looking at how artificial intelligence has evolved over time. Understanding this history provides important context for appreciating the capabilities and limitations of today&#8217;s AI systems, particularly large language models.</p></div>
<div class="paragraph"><p>In this module, we&#8217;ll trace the development of AI from early rule-based systems through the AI winters and into the current era of machine learning and deep learning. We&#8217;ll pay particular attention to the emergence of large language models, which represent a significant advancement in AI capabilities with broad business applications.</p></div>
<div class="paragraph"><p>This historical perspective will help you understand why today&#8217;s AI systems are fundamentally different from previous generations of technology and why they&#8217;re creating unprecedented opportunities for business transformation.</p></div></aside></div></section><section id="_brief_history_of_ai"><h2>Brief History of AI</h2><div class="slide-content"><div class="ulist"><ul><li><p>Early AI (1950s-1980s): Rule-based systems with explicit programming</p></li><li><p>AI Winters (Late 1970s, Late 1980s-Early 1990s): Periods of reduced funding and interest due to unmet expectations</p></li><li><p>Machine Learning Revolution (1990s-2000s): Systems that learn from data rather than explicit rules</p></li><li><p>Deep Learning Breakthrough (2012-2018): Neural networks with multiple layers enabling complex pattern recognition</p></li><li><p>Current Era (2019-Present): Foundation models trained on massive datasets with broad capabilities</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>IMAGE TODO - Timeline with Milestones: Create a horizontal timeline showing the progression of AI development with key milestones marked. Use different colors for each era and include small icons representing the dominant technology of each period (e.g., flowcharts for rule-based systems, neural network diagrams for deep learning).  This slide may need to be split into two.</p></div>
<div class="paragraph"><p>The history of artificial intelligence spans several decades, with distinct phases that have shaped today&#8217;s landscape:</p></div>
<div class="paragraph"><p>Early AI, from roughly the 1950s through the 1980s, focused primarily on rule-based systems that relied on explicit programming. These systems followed logical rules created by human experts to solve specific problems. Examples included expert systems for medical diagnosis or chess-playing programs that used predefined strategies. While impressive for their time, these systems were limited by their inability to handle exceptions or learn from experience.</p></div>
<div class="paragraph"><p>The field experienced several "AI Winters" - periods of reduced funding and interest when the technology failed to meet inflated expectations. These occurred notably in the late 1970s and again in the late 1980s/early 1990s. During these periods, many believed AI had fundamental limitations that would prevent it from achieving its promised potential.</p></div>
<div class="paragraph"><p>The Machine Learning Revolution marked a significant shift in approach. Rather than programming explicit rules, systems were designed to learn patterns from data. This approach proved more flexible and scalable, enabling AI to tackle problems that were too complex for rule-based approaches. Statistical methods and algorithms like support vector machines and random forests demonstrated impressive capabilities in specific domains.</p></div>
<div class="paragraph"><p>The Deep Learning Breakthrough, beginning around 2012, represented another fundamental advancement. Neural networks with multiple layers (hence "deep" learning) demonstrated unprecedented abilities in pattern recognition tasks. The key innovation was the ability to automatically learn hierarchical features from data, eliminating the need for human feature engineering. This approach dramatically improved performance in image recognition, speech processing, and eventually language understanding.</p></div>
<div class="paragraph"><p>The Current Era is characterized by foundation models - large AI systems trained on massive datasets that can be adapted to a wide range of tasks. These models, particularly large language models like GPT-4, Claude, and Gemini, demonstrate broad capabilities across domains without task-specific training. They represent a shift from specialized AI systems to general-purpose technologies with applications across virtually every business function.</p></div>
<div class="paragraph"><p>This evolution from narrow, rule-based systems to flexible, learning-based approaches has dramatically expanded the potential applications of AI in business contexts. Understanding this trajectory helps explain why today&#8217;s AI capabilities represent such a significant opportunity for business transformation.</p></div></aside></div></section><section id="_the_rise_of_large_language_models"><h2>The Rise of Large Language Models</h2><div class="slide-content"><div class="ulist"><ul><li><p>Word embeddings: Representing words as mathematical vectors</p></li><li><p>Transformer architecture (2017): Revolutionary approach enabling parallel processing of text</p></li><li><p>Key breakthroughs: BERT (2018), GPT series (2018-present), Claude, Gemini</p></li><li><p>Scaling laws (2020-2023): Performance improvements correlate with model size and training data</p></li><li><p>Emergent capabilities (2022-present): Advanced reasoning and problem-solving appearing at scale</p></li><li><p>Test Time Compute scaling (2023-present): Performance improvements through increased inference computation</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>IMAGE TODO - Timeline with Milestones: may also use a timeline plus Split Into two.</p></div>
<div class="paragraph"><p>Large Language Models (LLMs) represent one of the most significant developments in artificial intelligence, with a trajectory of rapid advancement over the past decade:</p></div>
<div class="paragraph"><p>Word embeddings marked an important early step in natural language processing. These techniques, like Word2Vec (2013) and GloVe (2014), represented words as mathematical vectors in a way that captured semantic relationships. Words with similar meanings would be positioned close together in this mathematical space. This approach allowed algorithms to understand relationships between words, but still had limitations in understanding context.</p></div>
<div class="paragraph"><p>The Transformer architecture, introduced in the 2017 paper "Attention is All You Need," represented a revolutionary approach to processing text. Unlike previous sequential models, transformers could process all words in a text simultaneously, using a mechanism called "attention" to weigh the importance of different words in relation to each other. This parallel processing capability enabled much more efficient training on larger datasets.</p></div>
<div class="paragraph"><p>Key breakthroughs followed rapidly. BERT (Bidirectional Encoder Representations from Transformers), released by Google in 2018, demonstrated unprecedented performance on language understanding tasks. The GPT (Generative Pre-trained Transformer) series from OpenAI, beginning in 2018 and continuing through GPT-4 and beyond, showed increasingly impressive text generation capabilities. Other models like Claude from Anthropic and Gemini from Google have further advanced the field.</p></div>
<div class="paragraph"><p>Scaling laws have emerged as a crucial insight in LLM development. Researchers discovered that performance improvements correlate predictably with increases in model size (number of parameters) and training data volume. This finding led to a race to build ever-larger models, with sizes increasing from millions to billions and now trillions of parameters.</p></div>
<div class="paragraph"><p>Perhaps most surprisingly, emergent capabilities have appeared as models reached certain scale thresholds. Advanced reasoning, problem-solving, and even coding abilities weren&#8217;t explicitly programmed but emerged as models grew larger and were trained on more diverse data. These emergent capabilities have dramatically expanded the potential business applications of LLMs.</p></div>
<div class="paragraph"><p>Test Time Compute scaling represents one of the newest frontiers in LLM advancement. Research from 2023 onward has demonstrated that model performance can be significantly improved not just by increasing model size or training data, but by allocating more computational resources during inference (when the model is actually generating responses). Techniques like speculative decoding, tree-of-thought reasoning, and self-consistency sampling allow models to explore multiple reasoning paths or potential responses before selecting the best one. This approach effectively trades inference speed for quality, enabling even existing models to achieve better performance on complex reasoning tasks without retraining. For businesses, this means that model capabilities can continue to improve through algorithmic innovations even without building larger models, potentially offering more cost-effective paths to enhanced AI performance.</p></div>
<div class="paragraph"><p>The rapid evolution of LLMs has transformed them from academic curiosities to powerful business tools in just a few years. Understanding this trajectory helps explain their current capabilities and limitations, as well as their potential future development.</p></div></aside></div></section><section id="_current_ai_landscape_major_platforms"><h2>Current AI Landscape: Major Platforms</h2><div class="slide-content"><div class="ulist"><ul><li><p>Major commercial platforms: OpenAI (GPT-4o), Anthropic (Claude), Google (Gemini), Mistral, Meta (Llama)</p></li><li><p>Open-source alternatives: Llama 3 (Meta), Mistral Large, Falcon (TII), Deepseek, Mixtral 8x7B, BLOOM, Pythia, Stable LM</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Today&#8217;s AI landscape is characterized by rapid innovation, increasing accessibility, and a growing focus on business applications:</p></div>
<div class="paragraph"><p>Major commercial platforms have emerged as leaders in the development and deployment of large language models. OpenAI&#8217;s GPT series, particularly GPT-4o, offers state-of-the-art capabilities across text, image, and audio modalities. Anthropic&#8217;s Claude models emphasize safety and helpfulness. Google&#8217;s Gemini combines language capabilities with multimodal understanding. Newer entrants like Mistral AI and Meta&#8217;s Llama models are also gaining significant traction. These platforms typically offer API access, allowing businesses to integrate their capabilities without managing the underlying infrastructure.</p></div>
<div class="paragraph"><p>Open-source alternatives have created a parallel ecosystem of freely available models that can be downloaded, modified, and deployed by organizations with the technical resources to do so. Meta&#8217;s Llama 3 series (ranging from 8B to 70B parameters) has become one of the most widely adopted open-source models, offering performance competitive with many commercial options. Mistral AI has released several high-quality open models, including Mistral Large and the innovative Mixtral 8x7B which uses a mixture-of-experts architecture. The Technology Innovation Institute&#8217;s Falcon models (7B, 40B, and 180B versions) have shown impressive capabilities for their size. Deepseek&#8217;s models, particularly Deepseek Coder, excel at programming tasks. Other notable open-source models include BLOOM (a multilingual model developed by over 1,000 researchers), Pythia (a family of models designed for interpretability research), and Stable LM from Stability AI. This open-source movement has accelerated innovation and reduced costs, though these models often require more technical expertise to implement effectively.</p></div></aside></div></section><section id="_current_ai_landscape_trends"><h2>Current AI Landscape: Trends</h2><div class="slide-content"><div class="ulist"><ul><li><p>Enterprise AI integration: Increasing focus on business-specific implementations</p></li><li><p>Democratization of access: API-based services making AI capabilities widely available</p></li><li><p>Specialized vs. general-purpose systems: Trend toward adaptable foundation models</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Enterprise AI integration has become a major focus, with organizations moving beyond experimentation to implement AI capabilities in core business processes. This shift is driving demand for industry-specific models, enterprise-grade security and compliance features, and seamless integration with existing business systems. Companies like Microsoft, Salesforce, and IBM are positioning themselves as enablers of this enterprise AI transformation.</p></div>
<div class="paragraph"><p>The democratization of access represents another key trend. API-based services have made sophisticated AI capabilities available to organizations of all sizes without requiring specialized AI expertise. This accessibility has dramatically expanded the potential user base and use cases for AI technologies. No-code and low-code platforms are further reducing barriers to entry.</p></div>
<div class="paragraph"><p>The industry is seeing a shift from specialized to general-purpose systems. Rather than building custom AI models for each specific task, organizations are increasingly leveraging foundation models that can be adapted to a wide range of applications through techniques like fine-tuning and prompt engineering. This approach reduces development time and cost while maintaining high performance.</p></div>
<div class="paragraph"><p>Understanding this landscape is crucial for business professionals seeking to leverage AI effectively. The rapid pace of innovation means new capabilities are constantly emerging, while increasing accessibility makes implementation more feasible than ever before. This combination creates unprecedented opportunities for business transformation across virtually every industry and function.</p></div></aside></div></section></section>
<section><section id="_understanding_large_language_models"><h2>Understanding Large Language Models</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this module, we&#8217;ll develop a deeper understanding of Large Language Models (LLMs) - what they are, how they work at a high level, and the different approaches to their development. This understanding is essential for evaluating their potential applications in your business.</p></div>
<div class="paragraph"><p>We&#8217;ll explore the fundamental capabilities of LLMs and how they process and generate language. We&#8217;ll then examine different types of LLMs, particularly the distinction between probabilistic models like GPT-4o and reasoning-based models like Claude 3 Opus or GPT-4o with specific prompting techniques.</p></div>
<div class="paragraph"><p>By understanding these different approaches and their respective strengths and limitations, you&#8217;ll be better equipped to determine which type of LLM might be most appropriate for different business applications. This knowledge will directly inform your ability to identify and evaluate AI opportunities in your organization.</p></div></aside></div></section><section id="_what_are_llms_core_mechanics"><h2>What are LLMs? Core Mechanics</h2><div class="slide-content"><div class="ulist"><ul><li><p>AI systems trained on vast text datasets to understand and generate human language</p></li><li><p>Process information by breaking text into tokens (word parts) and analyzing patterns</p></li><li><p>Predict likely next words/tokens based on patterns learned during training</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Large Language Models (LLMs) are a type of artificial intelligence system specifically designed to understand and generate human language. Let&#8217;s explore their fundamental characteristics:</p></div>
<div class="paragraph"><p>LLMs are trained on vast text datasets, often comprising hundreds of billions of words from sources like books, articles, websites, code repositories, and other text-based content. This extensive training allows them to learn the patterns, structures, and relationships in human language across diverse topics and domains. The largest models have effectively "read" more text than any human could in multiple lifetimes.</p></div>
<div class="paragraph"><p>These models process information by breaking text into tokens, which are essentially word parts or complete words. For example, the word "understanding" might be broken into tokens like "under" and "standing." The model analyzes patterns in how these tokens appear in relation to each other across its training data. This tokenization approach allows the model to handle words it hasn&#8217;t explicitly seen before by recognizing their component parts.</p></div>
<div class="paragraph"><p>At their core, LLMs predict likely next words or tokens based on the patterns they&#8217;ve learned. When given a prompt or partial text, they calculate probabilities for what might come next based on similar patterns in their training data. This predictive capability is what enables them to generate coherent and contextually appropriate text that continues from any starting point.</p></div></aside></div></section><section id="_understanding_tokenization"><h2>Understanding Tokenization</h2><div class="slide-content"><div class="ulist"><ul><li><p>Tokens are the basic units LLMs process - can be words, parts of words, or punctuation</p></li><li><p>Examples:</p><div class="ulist"><ul><li><p>"Artificial" → "Art" + "ificial"</p></li><li><p>"intelligence" → "intel" + "ligence"</p></li><li><p>"doesn&#8217;t" → "doesn" + "'t"</p></li></ul></div></li><li><p>Most models use 1,000-100,000 unique tokens in their vocabulary</p></li><li><p>Efficient compression of language into machine-readable units</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Tokenization is a fundamental process that converts human text into a format that LLMs can process. Understanding how tokenization works helps explain both the capabilities and limitations of these models:</p></div>
<div class="paragraph"><p>Tokens represent the basic units that LLMs process. Unlike traditional NLP systems that might work with whole words, LLMs break text down into subword units. These tokens can be complete words, parts of words, or even individual characters and punctuation marks. This approach allows the model to handle a virtually unlimited vocabulary by combining token pieces.</p></div>
<div class="paragraph"><p>The tokenization process follows specific patterns based on the frequency of character combinations in the training data. Common words like "the" or "and" typically get their own tokens, while less common words are split into multiple tokens. For example, "tokenization" might be broken into "token" + "ization" because these parts appear frequently in other words.</p></div>
<div class="paragraph"><p>Different LLM systems use different tokenization approaches. GPT models use a method called Byte-Pair Encoding (BPE), while some other models use WordPiece or SentencePiece tokenizers. Regardless of the specific method, all modern LLMs use some form of subword tokenization.</p></div>
<div class="paragraph"><p>The size of a model&#8217;s token vocabulary typically ranges from about 1,000 to 100,000 unique tokens. This vocabulary represents the building blocks the model uses to understand and generate all text. The specific tokens in this vocabulary are determined during the pre-training process based on the frequency of character patterns in the training data.</p></div>
<div class="paragraph"><p>Tokenization has important practical implications. When using LLMs, inputs are counted in tokens, not words or characters. This affects usage costs for commercial APIs and context window limitations. As a rule of thumb, one word typically corresponds to about 1.3-1.5 tokens in English, though this varies widely depending on the specific text.</p></div>
<div class="paragraph"><p>Understanding tokenization helps explain why LLMs sometimes struggle with very rare words, made-up terms, or specialized technical vocabulary. If a word must be broken into many small token pieces, the model may have difficulty maintaining coherence across those pieces during processing.</p></div></aside></div></section><section id="_prediction_mechanism_probability_distribution"><h2>Prediction Mechanism: Probability Distribution</h2><div class="slide-content"><div class="ulist"><ul><li><p>LLMs function as next-token prediction engines</p></li><li><p>For input: "The capital of France is&#8230;&#8203;"</p></li><li><p>Model calculates probability distribution across entire vocabulary:</p><div class="ulist"><ul><li><p>"Paris": 92%</p></li><li><p>"Lyon": 2%</p></li><li><p>"located": 1%</p></li><li><p>[thousands of other possibilities with lower probabilities]</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>At their core, LLMs operate through a surprisingly simple mechanism: they predict the next token in a sequence based on all the tokens that came before it. This fundamental prediction capability is what enables all their more complex behaviors:</p></div>
<div class="paragraph"><p>When an LLM receives input text, it processes this text token by token, building an internal representation of the context. This representation captures patterns, relationships, and semantic meanings from the input.</p></div>
<div class="paragraph"><p>For each position in the sequence, the model calculates a probability distribution across its entire vocabulary of tokens. This distribution represents the model&#8217;s prediction of how likely each possible token is to appear next in the sequence. For example, given the prompt "The capital of France is," the model might assign a 92% probability to "Paris," a 2% probability to "Lyon," a 1% probability to "located," and distribute the remaining 5% across thousands of other tokens.</p></div>
<div class="paragraph"><p>The model then selects a token from this probability distribution. In the simplest case, it selects the highest probability token (a process called "greedy decoding"). However, most implementations use more sophisticated sampling methods that introduce controlled randomness to generate more diverse and interesting outputs.</p></div></aside></div></section><section id="_prediction_mechanism_temperature_generation"><h2>Prediction Mechanism: Temperature &amp; Generation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Temperature setting controls randomness in token selection:</p><div class="ulist"><ul><li><p>Low temp (0.1-0.5): More predictable, focused outputs</p></li><li><p>Medium temp (0.6-0.8): Balanced creativity &amp; accuracy</p></li><li><p>High temp (0.9-1.0+): More creative, diverse outputs</p></li></ul></div></li><li><p>Each selected token becomes part of context for next prediction</p><div class="ulist"><ul><li><p>This iterative process continues until completion</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The "temperature" setting that many LLM interfaces provide controls the randomness in the token selection process. This parameter fundamentally alters how the model generates text:</p></div>
<div class="paragraph"><p>At low temperatures (typically 0.1-0.5), the model strongly favors high-probability tokens. This results in more predictable, focused, and often more factually accurate outputs. Low temperatures are ideal for tasks requiring precision, such as answering factual questions or generating code.</p></div>
<div class="paragraph"><p>At medium temperatures (around 0.6-0.8), the model strikes a balance between selecting high-probability tokens and occasionally choosing less likely options. This creates outputs with a good balance of coherence and creativity, making it suitable for many general-purpose applications.</p></div>
<div class="paragraph"><p>At high temperatures (0.9 and above), the model is much more likely to select lower-probability tokens. This produces more diverse, creative, and sometimes surprising outputs, but with increased risk of incoherence or factual errors. High temperatures work well for creative writing, brainstorming, or generating varied alternatives.</p></div>
<div class="paragraph"><p>Once a token is selected, it&#8217;s added to the sequence, and the process repeats. The model now calculates a new probability distribution for the next position, taking into account the newly added token. This iterative process continues until the model generates a stopping token or reaches a predefined length limit.</p></div>
<div class="paragraph"><p>What&#8217;s remarkable is that this relatively simple prediction mechanism, when scaled up with billions of parameters and trained on vast datasets, enables the complex capabilities we observe in modern LLMs. The model isn&#8217;t explicitly programmed to answer questions, write essays, or solve problems—it&#8217;s simply predicting what tokens are likely to come next in a given context. The emergent behaviors we value arise from this fundamental prediction capability.</p></div></aside></div></section><section id="_llm_capabilities_and_applications"><h2>LLM Capabilities and Applications</h2><div class="slide-content"><div class="ulist"><ul><li><p>LLMs demonstrate capabilities in writing, summarizing, answering questions, and reasoning</p></li><li><p>Represent a general-purpose technology with applications across business functions</p></li><li><p>All complex behaviors emerge from the simple next-token prediction mechanism</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Modern LLMs demonstrate remarkable capabilities across a range of language tasks. They can write essays, stories, or business documents; summarize lengthy content; answer questions based on their training data; translate between languages; and even perform reasoning tasks that require multiple steps of logical thinking. These capabilities emerge from their statistical understanding of language patterns rather than explicit programming.</p></div>
<div class="paragraph"><p>From a business perspective, LLMs represent a general-purpose technology with applications across virtually every function and industry. Unlike previous AI systems that were designed for specific narrow tasks, LLMs can be applied to a wide range of language-related challenges through appropriate prompting or fine-tuning. This flexibility makes them particularly valuable as a business tool.</p></div>
<div class="paragraph"><p>Understanding LLMs as pattern-recognition systems trained on language data helps explain both their impressive capabilities and their limitations. They don&#8217;t "understand" text in the human sense but have learned statistical patterns that allow them to mimic understanding in ways that are increasingly useful for business applications.</p></div>
<div class="paragraph"><p>What makes LLMs particularly remarkable is that all these diverse capabilities—from writing marketing copy to analyzing financial data to generating computer code—emerge from the same fundamental next-token prediction mechanism. The model&#8217;s ability to perform such varied tasks comes from the patterns it learned during training rather than from task-specific programming. This explains why the same model can switch between different types of tasks simply based on how it&#8217;s prompted.</p></div></aside></div></section><section><div class="slide-content"><div class="paragraph h4-style"><p>There are two types of LLMs: Probabilistic and Chain-of-Thought Reasoning</p></div></div></section><section id="_probabilistic_llms"><h2>Probabilistic LLMs</h2><div class="slide-content"><div class="ulist"><ul><li><p>Trained to predict the next token based on statistical patterns in training data</p></li><li><p>Generate text by repeatedly predicting the most likely next word/token</p></li><li><p>Process is purely statistical - no explicit rules about grammar, facts, or reasoning</p></li><li><p>Each prediction influenced by the entire context provided so far</p></li><li><p>Examples: OpenAI&#8217;s GPT-4o, Anthropic&#8217;s Claude 3.5, Meta&#8217;s Llama2</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Probabilistic LLMs like GPT-4o represent the most common approach to language model development. Let&#8217;s explore how they work and their business implications:</p></div>
<div class="paragraph"><p>These models are trained through a process called "next token prediction." During training, the model is shown vast amounts of text and learns to predict what word or token is likely to come next in any given sequence. This training objective is purely statistical - the model learns patterns of word co-occurrence across billions of examples without explicit rules about grammar, facts, or reasoning.</p></div>
<div class="paragraph"><p>When generating text, probabilistic LLMs work by repeatedly predicting the most likely next word or token based on what they&#8217;ve already generated. Each prediction is influenced by the entire context provided so far. The model calculates probability distributions across its entire vocabulary (often 100,000+ tokens) and selects from these possibilities. This process continues word by word until the response is complete.</p></div></aside></div></section><section id="_probabilistic_strengths_limitations"><h2>Probabilistic: Strengths &amp; Limitations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Strengths:</p><div class="ulist"><ul><li><p>Remarkably fluent, natural-sounding writing</p></li><li><p>Broad knowledge across diverse domains</p></li><li><p>Creative content generation capabilities</p></li></ul></div></li><li><p>Limitations:</p><div class="ulist"><ul><li><p>May "hallucinate" facts that sound plausible but are incorrect</p></li><li><p>Can struggle with complex multi-step reasoning</p></li><li><p>Limited by training data cutoff and potential biases</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The strengths of probabilistic LLMs include remarkably fluent writing that mimics human language patterns, broad knowledge across diverse domains absorbed from their training data, and creative capabilities in generating novel content like stories, marketing copy, or business ideas. They excel at tasks requiring language fluency and general knowledge.</p></div>
<div class="paragraph"><p>However, these models have important limitations. They may "hallucinate" facts that sound plausible but are incorrect, as they&#8217;re optimizing for plausible-sounding text rather than factual accuracy. They can struggle with complex multi-step reasoning, particularly for problems requiring precise logical or mathematical thinking. They&#8217;re also limited by their training data - they don&#8217;t have real-time information beyond their training cutoff and may reflect biases present in that data.</p></div>
<div class="paragraph"><p>Understanding the probabilistic nature of these models helps explain both their impressive capabilities and their limitations. They don&#8217;t "know" facts in the human sense but have learned statistical patterns that allow them to generate text that often contains accurate information. This distinction is important when evaluating their potential applications in business contexts where factual accuracy or reliable reasoning may be critical.</p></div></aside></div></section><section id="_probabilistic_business_applications"><h2>Probabilistic: Business Applications</h2><div class="slide-content"><div class="ulist"><ul><li><p>Content creation: marketing materials, reports, communications</p></li><li><p>Document summarization and information extraction</p></li><li><p>Creative ideation and brainstorming</p></li><li><p>Customer support automation and chatbots</p></li><li><p>Knowledge management and information retrieval</p></li><li><p>Draft generation with human review and refinement</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>From a business perspective, probabilistic LLMs are particularly valuable for a wide range of applications:</p></div>
<div class="paragraph"><p>Content creation is one of the most common use cases, with LLMs helping to generate marketing materials, reports, emails, and other business communications. The models can produce drafts quickly that humans can then review and refine.</p></div>
<div class="paragraph"><p>Document summarization is another powerful application, allowing these models to condense lengthy reports, articles, or transcripts into concise summaries that capture key points. This can dramatically improve information processing efficiency.</p></div>
<div class="paragraph"><p>For creative tasks, LLMs excel at ideation and brainstorming, generating diverse perspectives and approaches that can spark innovation. They can suggest multiple angles on a problem or help develop creative concepts for marketing campaigns.</p></div>
<div class="paragraph"><p>Customer support automation has been revolutionized by these models, which can handle a wide range of customer inquiries with natural-sounding responses. They can be deployed as chatbots or used to assist human agents with response suggestions.</p></div>
<div class="paragraph"><p>Knowledge management applications leverage LLMs' ability to process and retrieve information from large document collections, making organizational knowledge more accessible and useful.</p></div>
<div class="paragraph"><p>The most effective business implementations typically combine LLM capabilities with human oversight, using the models to generate initial drafts or suggestions that humans then review, edit, and approve. This human-in-the-loop approach mitigates the risk of hallucinations or errors while still capturing the efficiency benefits.</p></div></aside></div></section><section id="_chain_of_thought_reasoning_llms"><h2>Chain-of-Thought Reasoning LLMs</h2><div class="slide-content"><div class="ulist"><ul><li><p>Specifically trained or prompted to show explicit reasoning steps</p></li><li><p>Process complex problems by breaking them into logical sequences</p></li><li><p>Examples: OpenAI&#8217;s o1/o3, DeepSeek-R1</p></li><li><p>Example approach for a math problem:</p><div class="ulist"><ul><li><p>Identify relevant variables and formulas</p></li><li><p>Work through calculations step-by-step</p></li><li><p>Verify results before providing final answer</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Chain-of-Thought (CoT) reasoning represents an important advancement in LLM capabilities, particularly evident in models like OpenAI&#8217;s o1 and o3 or when using specific prompting techniques with models like GPT-4:</p></div>
<div class="paragraph"><p>These models are either specifically trained or prompted to show explicit reasoning steps rather than jumping directly to conclusions. When faced with a complex problem, they break it down into a logical sequence of steps, working through the problem methodically much like a human would. This approach dramatically improves performance on tasks requiring multi-step reasoning.</p></div>
<div class="paragraph"><p>The key innovation in CoT models is their ability to process complex problems by breaking them into manageable components. For example, when solving a math problem, the model might first identify the relevant variables, then determine the appropriate formula, perform the calculation step by step, and finally verify the result. This step-by-step approach significantly reduces errors compared to attempting to solve problems in a single step.</p></div>
<div class="paragraph"><p>Chain-of-thought reasoning can be elicited in two primary ways: through specific model training that rewards step-by-step reasoning, or through prompting techniques that explicitly instruct the model to "think step by step" before answering. Both approaches have proven effective at improving performance on complex reasoning tasks.</p></div></aside></div></section><section id="_chain_of_thought_reasoning_strengths_limitations"><h2>Chain-of-Thought Reasoning: Strengths &amp; Limitations</h2><div class="slide-content"><div class="ulist"><ul><li><p>Strengths:</p><div class="ulist"><ul><li><p>Superior mathematical and logical reasoning</p></li><li><p>Transparent decision-making process</p></li><li><p>Reduced error rates on complex problems</p></li><li><p>Self-correction capabilities</p></li></ul></div></li><li><p>Limitations:</p><div class="ulist"><ul><li><p>Higher computational requirements</p></li><li><p>Potentially slower response times</p></li><li><p>Still probabilistic at core - can make reasoning errors</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The strengths of CoT models include superior performance on mathematical reasoning tasks, logical problem-solving that requires multiple steps, and transparent decision-making where the reasoning process is visible and can be verified. This transparency is particularly valuable in business contexts where understanding how a conclusion was reached may be as important as the conclusion itself.</p></div>
<div class="paragraph"><p>A key advantage of chain-of-thought reasoning is the ability to self-correct. By working through problems step-by-step, these models can often identify errors in their own reasoning and revise their approach before arriving at a final answer. This significantly reduces error rates compared to models that attempt to solve problems in a single step.</p></div>
<div class="paragraph"><p>However, these models have limitations. They typically require more computational resources, which can result in higher costs and potentially slower responses compared to standard LLMs. They&#8217;re also still fundamentally probabilistic systems at their core, meaning they can make reasoning errors despite their step-by-step approach. Their performance depends significantly on how problems are presented to them.</p></div>
<div class="paragraph"><p>A key development in this area is the concept of "test-time compute" as a scaling law. Research has shown that allowing models more computation time to think through problems step by step can significantly improve performance, even without increasing model size. This insight suggests that future models may become increasingly capable of complex reasoning tasks simply by allocating more computational resources at inference time.</p></div></aside></div></section><section id="_chain_of_thought_reasoning_business_applications"><h2>Chain-of-Thought Reasoning: Business Applications</h2><div class="slide-content"><div class="ulist"><ul><li><p>Financial analysis and modeling</p></li><li><p>Complex decision support with transparent rationale</p></li><li><p>Process optimization and troubleshooting</p></li><li><p>Risk assessment and scenario planning</p></li><li><p>Educational applications and training</p></li><li><p>Regulatory compliance with documented reasoning</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>From a business perspective, CoT models are particularly valuable for applications requiring complex reasoning and transparency:</p></div>
<div class="paragraph"><p>Financial analysis and modeling benefit greatly from chain-of-thought reasoning, as these models can work through complex calculations while showing their work. This is especially valuable for investment decisions, financial forecasting, and budget planning where stakeholders need to understand the reasoning behind recommendations.</p></div>
<div class="paragraph"><p>For complex decision support, these models can evaluate multiple factors, weigh trade-offs, and provide recommendations with clear rationales. The transparency of their reasoning process builds trust and allows decision-makers to evaluate the quality of the analysis.</p></div>
<div class="paragraph"><p>Process optimization and troubleshooting are enhanced by the models' ability to systematically analyze workflows, identify bottlenecks, and suggest improvements with detailed explanations. This applies to manufacturing processes, supply chain optimization, and service delivery improvements.</p></div>
<div class="paragraph"><p>Risk assessment and scenario planning benefit from the models' ability to methodically work through different scenarios and their implications, helping organizations prepare for various contingencies with well-reasoned strategies.</p></div>
<div class="paragraph"><p>Educational applications leverage these models' ability to explain complex concepts step-by-step, making them valuable tools for training, knowledge transfer, and skill development within organizations.</p></div>
<div class="paragraph"><p>Regulatory compliance is another area where documented reasoning is particularly valuable. When decisions need to be justified to regulators or auditors, having a clear record of the reasoning process provides necessary transparency and accountability.</p></div>
<div class="paragraph"><p>Understanding the capabilities and limitations of CoT reasoning is crucial for identifying business problems where this approach might add significant value compared to standard probabilistic LLMs.</p></div></aside></div></section><section id="_comparing_llm_approaches"><h2>Comparing LLM Approaches</h2><div class="slide-content"><div class="ulist"><ul><li><p>Probabilistic models excel at fluent generation and broad knowledge tasks</p></li><li><p>Reasoning models perform better on complex problem-solving requiring logical steps</p></li><li><p>Cost considerations: Reasoning approaches may require more computational resources</p></li><li><p>Response time: Step-by-step reasoning typically takes longer than direct generation</p></li><li><p>Hybrid approaches often provide the best results for complex business applications</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Understanding the relative strengths and appropriate applications of different LLM approaches is crucial for effective business implementation:</p></div>
<div class="paragraph"><p>Probabilistic models excel at tasks requiring fluent language generation and broad knowledge. They&#8217;re particularly effective for content creation, summarization, creative writing, general question answering, and conversational interfaces. Their strength lies in their ability to generate natural-sounding text across a wide range of topics based on patterns learned from their training data. For many business applications where approximate answers or creative content are sufficient, probabilistic models offer an excellent balance of performance and efficiency.</p></div>
<div class="paragraph"><p>Reasoning models perform significantly better on tasks requiring complex problem-solving with logical steps. They&#8217;re particularly valuable for mathematical calculations, logical deductions, step-by-step analyses, and situations where the reasoning process itself needs to be transparent. Their explicit reasoning approach reduces errors on complex tasks and provides visibility into how conclusions are reached. For business applications where accuracy and verifiability are critical, reasoning-based approaches often justify their additional resource requirements.</p></div>
<div class="paragraph"><p>Cost considerations play an important role in choosing between approaches. Reasoning-based methods typically require more computational resources, which translates to higher costs in cloud-based API implementations. The explicit generation of intermediate reasoning steps means more tokens are generated, directly affecting usage-based pricing models. Organizations need to weigh these additional costs against the value of improved accuracy and transparency for specific use cases.</p></div>
<div class="paragraph"><p>Response time is another important factor. Step-by-step reasoning naturally takes longer than direct generation, as the model works through multiple intermediate steps before reaching a conclusion. For applications where immediate responses are critical, this additional latency may be problematic. However, for complex analytical tasks where accuracy is paramount, the additional time is often a worthwhile tradeoff.</p></div>
<div class="paragraph"><p>In practice, hybrid approaches often provide the best results for complex business applications. Many implementations use probabilistic models for initial content generation or simple queries, then switch to reasoning approaches for complex problems requiring verification or step-by-step analysis. Some systems also implement verification steps where outputs from probabilistic generation are checked using reasoning approaches before being presented to users.</p></div>
<div class="paragraph"><p>The field continues to evolve rapidly, with models increasingly incorporating reasoning capabilities as a standard feature rather than a separate approach. Understanding the appropriate application of different techniques allows organizations to optimize their AI implementations for specific business needs, balancing performance, cost, speed, and accuracy.</p></div></aside></div></section></section>
<section><section id="_enhancing_llm_capabilities"><h2>Enhancing LLM Capabilities</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this module, we&#8217;ll explore techniques that enhance the capabilities of Large Language Models, allowing them to overcome some of their inherent limitations. We&#8217;ll focus particularly on Retrieval-Augmented Generation (RAG) and tools integration, two approaches that significantly expand what LLMs can accomplish in business contexts.</p></div>
<div class="paragraph"><p>Understanding these enhancement techniques is crucial for identifying the full range of potential AI applications in your organization. Many valuable business use cases require capabilities beyond what a standalone LLM can provide, such as access to current information, proprietary data, or specialized functionality.</p></div>
<div class="paragraph"><p>By the end of this module, you&#8217;ll understand how these techniques work at a conceptual level and be able to identify business scenarios where they would add significant value. This knowledge will expand the range of processes you can consider for AI enhancement during the workshop.</p></div></aside></div></section><section id="_retrieval_augmented_generation_rag"><h2>Retrieval-Augmented Generation (RAG)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Combines LLMs with the ability to retrieve and reference specific information</p></li><li><p>Addresses the limitation of LLMs being restricted to their training data</p></li><li><p>Process: Query → Retrieve relevant documents → Incorporate into context → Generate response</p></li><li><p>Enables access to proprietary information, recent data, and specialized knowledge</p></li><li><p>Business applications: Knowledge management, customer support, compliance, research</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Retrieval-Augmented Generation (RAG) represents one of the most important enhancements to LLM capabilities, particularly for business applications:</p></div>
<div class="paragraph"><p>RAG combines the generative capabilities of LLMs with the ability to retrieve and reference specific information from external sources. This hybrid approach leverages the strengths of both technologies - the language understanding and generation abilities of LLMs and the precision and currency of information retrieval systems. The result is a system that can generate responses grounded in specific, retrievable information rather than relying solely on patterns learned during training.</p></div>
<div class="paragraph"><p>This approach directly addresses one of the most significant limitations of standalone LLMs: their restriction to information available in their training data. LLMs don&#8217;t have access to real-time information, proprietary company data, or specialized knowledge unless it was included in their training. RAG overcomes this limitation by allowing the model to access and incorporate external information at the time of response generation.</p></div>
<div class="paragraph"><p>The RAG process typically follows several steps: First, the system analyzes the user query to understand what information is needed. Next, it retrieves relevant documents or data from specified sources such as company databases, knowledge bases, or document repositories. These retrieved documents are then incorporated into the context provided to the LLM. Finally, the LLM generates a response that incorporates both its general language capabilities and the specific information retrieved.</p></div>
<div class="paragraph"><p>From a business perspective, RAG enables access to three critical categories of information: proprietary company data that wouldn&#8217;t be in public training sets (like internal policies, product specifications, or customer records); recent information that postdates the model&#8217;s training cutoff (like current market conditions or updated regulations); and specialized knowledge that might be too niche to be well-represented in general training data (like industry-specific terminology or procedures).</p></div>
<div class="paragraph"><p>RAG is particularly valuable for business applications like knowledge management (creating systems that can answer questions about company-specific information), customer support (providing accurate responses about products, services, and policies), compliance (ensuring responses reflect current regulations and company guidelines), and research (synthesizing information from multiple sources to answer complex questions).</p></div>
<div class="paragraph"><p>The implementation of RAG requires several components: a document storage system, a method for converting documents into a format suitable for retrieval (typically vector embeddings), a retrieval mechanism to find relevant information, and integration with an LLM for response generation. While these technical details are important for implementation, business professionals primarily need to understand the capabilities RAG enables and the types of information it can incorporate.</p></div>
<div class="paragraph"><p>By understanding RAG, you can identify business processes where access to specific information would significantly enhance the value of AI assistance - a crucial consideration when evaluating automation candidates during our workshop.</p></div></aside></div></section><section id="_tools_and_function_calling"><h2>Tools and Function Calling</h2><div class="slide-content"><div class="ulist"><ul><li><p>Enables LLMs to interact with external systems and perform specific actions</p></li><li><p>Examples: Calculators, web search, data analysis, calendar management, CRM updates</p></li><li><p>Process: LLM recognizes need for tool → Formats appropriate request → Tool executes → Results incorporated</p></li><li><p>Extends LLM capabilities beyond text generation to real-world actions</p></li><li><p>Business applications: Workflow automation, data analysis, scheduling, transaction processing</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Tools and function calling represent another crucial enhancement to LLM capabilities, particularly for business applications requiring interaction with external systems:</p></div>
<div class="paragraph"><p>This approach enables LLMs to interact with external systems and perform specific actions beyond text generation. Rather than being limited to producing text based on patterns in their training data, tool-augmented LLMs can recognize when a specific capability is needed and invoke the appropriate external system to perform that function. This dramatically extends what LLMs can accomplish in business contexts.</p></div>
<div class="paragraph"><p>Common examples of tools include calculators for precise mathematical operations, web search for current information, data analysis functions for processing structured data, calendar management for scheduling, CRM systems for customer information updates, and various API-connected services. Each tool provides specialized capabilities that complement the LLM&#8217;s language understanding and generation abilities.</p></div>
<div class="paragraph"><p>The process typically works as follows: The LLM recognizes from the user&#8217;s request or the context of the conversation that a specific tool is needed. It then formats an appropriate request to that tool, including necessary parameters or inputs. The external tool executes the requested function and returns results. Finally, the LLM incorporates these results into its response, often interpreting or explaining them for the user.</p></div>
<div class="paragraph"><p>This capability extends LLMs beyond text generation to enabling real-world actions and accessing real-time information. For example, an LLM with calendar access can not only discuss scheduling concepts but actually check availability and create appointments. One with calculation tools can perform precise financial analyses rather than approximating calculations based on training patterns.</p></div>
<div class="paragraph"><p>From a business perspective, tool integration is particularly valuable for applications like workflow automation (triggering actions across multiple systems), data analysis (performing calculations and generating insights from business data), scheduling and coordination (managing calendars and resources), and transaction processing (initiating and confirming business transactions in external systems).</p></div>
<div class="paragraph"><p>The implementation of tool integration requires defining the available tools, their parameters, and how they should be invoked. The LLM needs to be instructed or fine-tuned to recognize when tools are appropriate and how to format requests correctly. While these technical details are important for implementation, business professionals primarily need to understand what capabilities tool integration enables and what systems might be connected.</p></div>
<div class="paragraph"><p>By understanding tool integration, you can identify business processes where interaction with external systems would significantly enhance the value of AI assistance - another crucial consideration when evaluating automation candidates during our workshop.</p></div></aside></div></section><section id="_combining_approaches_for_business_solutions"><h2>Combining Approaches for Business Solutions</h2><div class="slide-content"><div class="ulist"><ul><li><p>Most effective business implementations combine multiple enhancement techniques</p></li><li><p>RAG provides knowledge grounding while tools enable actions</p></li><li><p>Integration considerations: Data security, system access, performance requirements</p></li><li><p>Cost-benefit analysis: Enhanced capabilities vs. implementation complexity</p></li><li><p>Example: Customer service agent with product knowledge (RAG) and order processing abilities (tools)</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>In practice, the most effective business implementations of AI often combine multiple enhancement techniques to create comprehensive solutions:</p></div>
<div class="paragraph"><p>Combining approaches allows organizations to address multiple limitations of standalone LLMs simultaneously. For example, RAG provides knowledge grounding by giving the LLM access to specific information, while tools enable actions by connecting the LLM to external systems. Together, these enhancements create AI systems that both know more and can do more than basic LLMs.</p></div>
<div class="paragraph"><p>A typical combined implementation might include: an LLM for natural language understanding and generation; RAG capabilities to incorporate company-specific information; tool connections to relevant business systems; and potentially chain-of-thought reasoning for complex decision processes. Each component addresses specific requirements of the business process being automated.</p></div>
<div class="paragraph"><p>When considering such implementations, several integration considerations become important. Data security is paramount when connecting AI systems to proprietary information or business systems. System access must be carefully managed to ensure the AI has appropriate permissions while maintaining security boundaries. Performance requirements must be evaluated, as each enhancement adds some latency to the overall system response time.</p></div>
<div class="paragraph"><p>Cost-benefit analysis becomes more complex with combined approaches. Enhanced capabilities must be weighed against increased implementation complexity and operational costs. Not every business process requires the full suite of enhancements - the appropriate combination should be determined based on specific requirements and expected value.</p></div>
<div class="paragraph"><p>As a concrete example, consider a customer service AI agent. It might use RAG to access product specifications, pricing information, and company policies; tool connections to check inventory, process orders, and update customer records; and chain-of-thought reasoning to troubleshoot complex customer issues. This combination creates a comprehensive solution that can handle a wide range of customer interactions with minimal human intervention.</p></div>
<div class="paragraph"><p>For business professionals evaluating AI opportunities, understanding these combined approaches is valuable even without technical implementation knowledge. It allows you to envision more comprehensive solutions and identify processes where multiple enhancements might create significant value. During our workshop, we&#8217;ll explore how different combinations of capabilities might address specific business needs in your organization.</p></div>
<div class="paragraph"><p>The field continues to evolve rapidly, with new enhancement techniques emerging regularly. The fundamental principle remains consistent: identifying the specific limitations of basic LLMs that affect your use case and applying the appropriate enhancements to address those limitations.</p></div></aside></div></section></section>
<section><section id="_effective_prompting_strategies"><h2>Effective Prompting Strategies</h2><div class="slide-content"><aside class="notes"><div class="paragraph"><p>In this module, we&#8217;ll explore the art and science of effective prompting - how to communicate with AI systems to get the results you need. Prompt engineering is a crucial skill for business professionals working with AI, as it directly affects the quality and usefulness of AI outputs.</p></div>
<div class="paragraph"><p>We&#8217;ll examine the fundamentals of prompt design and explore specific strategies for different types of LLMs. We&#8217;ll also compare approaches and discuss how to adapt your prompting techniques based on the specific task and model you&#8217;re working with.</p></div>
<div class="paragraph"><p>By the end of this module, you&#8217;ll understand how to construct effective prompts for different business scenarios and be able to optimize your interactions with AI systems to achieve better outcomes. This practical knowledge will be immediately applicable as you begin working with AI tools.</p></div></aside></div></section><section id="_prompting_probabilistic_llms_gpt_4o"><h2>Prompting Probabilistic LLMs (GPT-4o)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Be specific and explicit about desired outcomes and formats</p></li><li><p>Provide relevant context to overcome knowledge limitations</p></li><li><p>Use examples (few-shot learning) to demonstrate expected outputs</p></li><li><p>Manage token limitations by focusing on essential information</p></li><li><p>Business strategy: Start broad, then refine based on initial responses</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Probabilistic LLMs like GPT-4o require specific prompting strategies to maximize their effectiveness for business applications:</p></div>
<div class="paragraph"><p>Being specific and explicit about desired outcomes is particularly important with probabilistic models. Rather than assuming the model will intuit your needs, clearly state what you want, including the format, level of detail, tone, and any specific elements you require. For example, instead of asking "Tell me about our quarterly results," specify "Analyze our Q2 financial results in a 5-bullet executive summary highlighting year-over-year trends in revenue, expenses, and profit margins."</p></div>
<div class="paragraph"><p>Providing relevant context helps overcome the knowledge limitations inherent in these models. Remember that probabilistic LLMs don&#8217;t have access to information beyond their training data unless you provide it. For business applications, this often means including specific facts, figures, or background information in your prompt. For example, when asking for analysis of a business situation, include the key data points the model needs to consider.</p></div>
<div class="paragraph"><p>Using examples, often called few-shot learning, is a powerful technique for demonstrating expected outputs. By showing the model one or more examples of the type of response you want, you provide a pattern it can follow. This approach is particularly effective for specialized formats or when you need consistency across multiple outputs. For instance, if you need product descriptions in a specific format, provide an example or two in your prompt.</p></div>
<div class="paragraph"><p>Managing token limitations is important since all LLMs have context windows that limit how much text they can process at once. Focus on providing essential information rather than exhaustive details. For business applications, this might mean summarizing background information rather than including complete documents, or linking to reference materials rather than pasting their entire contents.</p></div>
<div class="paragraph"><p>From a business strategy perspective, an effective approach is to start with broader prompts and then refine based on initial responses. Begin with a general request to see what the model produces, then iterate with more specific guidance based on what&#8217;s missing or needs improvement. This iterative approach often yields better results than trying to craft the perfect prompt on the first attempt.</p></div>
<div class="paragraph"><p>Business-specific prompting strategies might include using industry terminology to improve relevance, specifying the intended audience for the output (e.g., "Write this for C-level executives"), and including company-specific context that might not be in the model&#8217;s training data. These adaptations help tailor generic LLM capabilities to your specific business needs.</p></div>
<div class="paragraph"><p>Remember that probabilistic LLMs are particularly good at generating creative content, summarizing information, and producing natural-sounding language. Your prompting strategy should leverage these strengths while providing sufficient guidance to overcome limitations in factual precision or complex reasoning.</p></div></aside></div></section><section id="_prompting_chain_of_thought_llms_o1o3"><h2>Prompting Chain-of-Thought LLMs (o1/o3)</h2><div class="slide-content"><div class="ulist"><ul><li><p>Explicitly request step-by-step reasoning in your prompts</p></li><li><p>Structure complex problems with clear intermediate steps</p></li><li><p>Encourage the model to "think aloud" before concluding</p></li><li><p>Implement verification steps to check reasoning validity</p></li><li><p>Business strategy: Break complex problems into logical sequences</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Chain-of-Thought (CoT) LLMs like OpenAI&#8217;s o1/o3 or Claude 3 Opus require different prompting strategies to fully leverage their reasoning capabilities:</p></div>
<div class="paragraph"><p>Explicitly requesting step-by-step reasoning is the foundation of effective CoT prompting. Unlike with standard LLMs where you might ask directly for a conclusion, with reasoning-focused models you should specifically ask the model to work through the problem methodically. For example, instead of "What&#8217;s the optimal inventory level?" try "Please think through the optimal inventory level step by step, considering our lead times, demand variability, and storage costs."</p></div>
<div class="paragraph"><p>Structuring complex problems with clear intermediate steps helps guide the model&#8217;s reasoning process. Break down multi-part problems into a logical sequence and ask the model to address each component in order. This approach is particularly effective for complex business analyses or decision-making scenarios. For instance, when evaluating a potential market entry, you might structure the prompt to first analyze market size, then competition, then regulatory considerations, and finally potential profitability.</p></div>
<div class="paragraph"><p>Encouraging the model to "think aloud" leverages the model&#8217;s ability to reason through problems verbally. Phrases like "Let&#8217;s think about this step by step" or "Let&#8217;s work through this methodically" signal to the model that you want to see its reasoning process, not just its conclusion. This approach is valuable when the reasoning itself provides insights or when you need to verify the model&#8217;s approach to a problem.</p></div>
<div class="paragraph"><p>Implementing verification steps improves accuracy by asking the model to check its own work. After the model provides a solution, prompt it to verify the answer by working backward, using a different method, or checking for common errors. For example, after a financial calculation, you might ask "Please verify this result by using an alternative calculation method and check for any potential errors in your reasoning."</p></div>
<div class="paragraph"><p>From a business strategy perspective, the key is breaking complex problems into logical sequences that the model can work through methodically. This approach is particularly valuable for financial analyses, strategic decisions, risk assessments, and other business scenarios where the reasoning process is as important as the conclusion.</p></div>
<div class="paragraph"><p>When working with CoT models in business contexts, it&#8217;s often valuable to combine reasoning requests with specific business frameworks or methodologies relevant to your industry. For example, you might ask the model to apply a specific strategic framework like Porter&#8217;s Five Forces or a standard financial analysis methodology to ensure the reasoning follows established business practices.</p></div>
<div class="paragraph"><p>The explicit reasoning capabilities of these models make them particularly valuable for explaining complex concepts to stakeholders, documenting decision processes for compliance purposes, and building confidence in AI-assisted business decisions through transparent reasoning.</p></div></aside></div></section></section></div></div><script src="../assets/reveal.js/dist/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
});

// More info about config & dependencies:
// - https://github.com/hakimel/reveal.js#configuration
// - https://github.com/hakimel/reveal.js#dependencies
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: false,
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Add the current slide number to the URL hash so that reloading the
  // page/copying the URL will return you to the same slide
  hash: true,
  // Push each slide change to the browser history. Implies `hash: true`
  history: false,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Disables the default reveal.js slide layout so that you can use custom CSS layout
  disableLayout: false,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // See https://github.com/hakimel/reveal.js/#navigation-mode
  navigationMode: 'default',
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Global override for preloading lazy-loaded iframes
  // - null: Iframes with data-src AND data-preload will be loaded when within
  //   the viewDistance, iframes with only data-src will be loaded when visible
  // - true: All iframes with data-src will be loaded when within the viewDistance
  // - false: All iframes with data-src will be loaded only when visible
  preloadIframes: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Specify the total time in seconds that is available to
  // present.  If this is set to a nonzero value, the pacing
  // timer will work out the time available for each slide,
  // instead of using the defaultTiming value
  totalTime: 0,
  // Specify the minimum amount of time you want to allot to
  // each slide, if using the totalTime calculation method.  If
  // the automated time allocation causes slide pacing to fall
  // below this threshold, then you will see an alert in the
  // speaker notes window
  minimumTimePerSlide: 0,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hide cursor if inactive
  hideInactiveCursor: true,
  // Time before the cursor is hidden (in ms)
  hideCursorTime: 5000,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Number of slides away from the current that are visible on mobile
  // devices. It is advisable to set this to a lower number than
  // viewDistance in order to save resources.
  mobileViewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.04,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 2,

  // PDF Export Options
  // Put each fragment on a separate page
  pdfSeparateFragments: true,
  // For slides that do not fit on a page, max number of pages
  pdfMaxPagesPerSlide: 1,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: '../assets/reveal.js/plugin/zoom/zoom.js', async: true, callback: function () { Reveal.registerPlugin(RevealZoom) } },
      { src: '../assets/reveal.js/plugin/notes/notes.js', async: true, callback: function () { Reveal.registerPlugin(RevealNotes) } }
  ],
});</script><script>var dom = {};
dom.slides = document.querySelector('.reveal .slides');

function getRemainingHeight(element, slideElement, height) {
  height = height || 0;
  if (element) {
    var newHeight, oldHeight = element.style.height;
    // Change the .stretch element height to 0 in order find the height of all
    // the other elements
    element.style.height = '0px';
    // In Overview mode, the parent (.slide) height is set of 700px.
    // Restore it temporarily to its natural height.
    slideElement.style.height = 'auto';
    newHeight = height - slideElement.offsetHeight;
    // Restore the old height, just in case
    element.style.height = oldHeight + 'px';
    // Clear the parent (.slide) height. .removeProperty works in IE9+
    slideElement.style.removeProperty('height');
    return newHeight;
  }
  return height;
}

function layoutSlideContents(width, height) {
  // Handle sizing of elements with the 'stretch' class
  toArray(dom.slides.querySelectorAll('section .stretch')).forEach(function (element) {
    // Determine how much vertical space we can use
    var limit = 5; // hard limit
    var parent = element.parentNode;
    while (parent.nodeName !== 'SECTION' && limit > 0) {
      parent = parent.parentNode;
      limit--;
    }
    if (limit === 0) {
      // unable to find parent, aborting!
      return;
    }
    var remainingHeight = getRemainingHeight(element, parent, height);
    // Consider the aspect ratio of media elements
    if (/(img|video)/gi.test(element.nodeName)) {
      var nw = element.naturalWidth || element.videoWidth, nh = element.naturalHeight || element.videoHeight;
      var es = Math.min(width / nw, remainingHeight / nh);
      element.style.width = (nw * es) + 'px';
      element.style.height = (nh * es) + 'px';
    } else {
      element.style.width = width + 'px';
      element.style.height = remainingHeight + 'px';
    }
  });
}

function toArray(o) {
  return Array.prototype.slice.call(o);
}

Reveal.addEventListener('slidechanged', function () {
  layoutSlideContents(960, 700)
});
Reveal.addEventListener('ready', function () {
  layoutSlideContents(960, 700)
});
Reveal.addEventListener('resize', function () {
  layoutSlideContents(960, 700)
});</script><link rel="stylesheet" href="../assets/theme/github.css"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.3/highlight.min.js"></script>

<script>

/* highlightjs-line-numbers.js 2.6.0 | (C) 2018 Yauheni Pakala | MIT License | github.com/wcoder/highlightjs-line-numbers.js */
/* Edited by Hakim for reveal.js; removed async timeout */
!function(n,e){"use strict";function t(){var n=e.createElement("style");n.type="text/css",n.innerHTML=g(".{0}{border-collapse:collapse}.{0} td{padding:0}.{1}:before{content:attr({2})}",[v,L,b]),e.getElementsByTagName("head")[0].appendChild(n)}function r(t){"interactive"===e.readyState||"complete"===e.readyState?i(t):n.addEventListener("DOMContentLoaded",function(){i(t)})}function i(t){try{var r=e.querySelectorAll("code.hljs,code.nohighlight");for(var i in r)r.hasOwnProperty(i)&&l(r[i],t)}catch(o){n.console.error("LineNumbers error: ",o)}}function l(n,e){"object"==typeof n&&f(function(){n.innerHTML=s(n,e)})}function o(n,e){if("string"==typeof n){var t=document.createElement("code");return t.innerHTML=n,s(t,e)}}function s(n,e){e=e||{singleLine:!1};var t=e.singleLine?0:1;return c(n),a(n.innerHTML,t)}function a(n,e){var t=u(n);if(""===t[t.length-1].trim()&&t.pop(),t.length>e){for(var r="",i=0,l=t.length;i<l;i++)r+=g('<tr><td class="{0}"><div class="{1} {2}" {3}="{5}"></div></td><td class="{4}"><div class="{1}">{6}</div></td></tr>',[j,m,L,b,p,i+1,t[i].length>0?t[i]:" "]);return g('<table class="{0}">{1}</table>',[v,r])}return n}function c(n){var e=n.childNodes;for(var t in e)if(e.hasOwnProperty(t)){var r=e[t];h(r.textContent)>0&&(r.childNodes.length>0?c(r):d(r.parentNode))}}function d(n){var e=n.className;if(/hljs-/.test(e)){for(var t=u(n.innerHTML),r=0,i="";r<t.length;r++){var l=t[r].length>0?t[r]:" ";i+=g('<span class="{0}">{1}</span>\n',[e,l])}n.innerHTML=i.trim()}}function u(n){return 0===n.length?[]:n.split(y)}function h(n){return(n.trim().match(y)||[]).length}function f(e){e()}function g(n,e){return n.replace(/{(\d+)}/g,function(n,t){return e[t]?e[t]:n})}var v="hljs-ln",m="hljs-ln-line",p="hljs-ln-code",j="hljs-ln-numbers",L="hljs-ln-n",b="data-line-number",y=/\r\n|\r|\n/g;n.hljs?(n.hljs.initLineNumbersOnLoad=r,n.hljs.lineNumbersBlock=l,n.hljs.lineNumbersValue=o,t()):n.console.error("highlight.js not detected!")}(window,document);

/**
 * This reveal.js plugin is wrapper around the highlight.js
 * syntax highlighting library.
 */
(function( root, factory ) {
  if (typeof define === 'function' && define.amd) {
    root.RevealHighlight = factory();
  } else if( typeof exports === 'object' ) {
    module.exports = factory();
  } else {
    // Browser globals (root is window)
    root.RevealHighlight = factory();
  }
}( this, function() {

  // Function to perform a better "data-trim" on code snippets
  // Will slice an indentation amount on each line of the snippet (amount based on the line having the lowest indentation length)
  function betterTrim(snippetEl) {
    // Helper functions
    function trimLeft(val) {
      // Adapted from https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/Trim#Polyfill
      return val.replace(/^[\s\uFEFF\xA0]+/g, '');
    }
    function trimLineBreaks(input) {
      var lines = input.split('\n');

      // Trim line-breaks from the beginning
      for (var i = 0; i < lines.length; i++) {
        if (lines[i].trim() === '') {
          lines.splice(i--, 1);
        } else break;
      }

      // Trim line-breaks from the end
      for (var i = lines.length-1; i >= 0; i--) {
        if (lines[i].trim() === '') {
          lines.splice(i, 1);
        } else break;
      }

      return lines.join('\n');
    }

    // Main function for betterTrim()
    return (function(snippetEl) {
      var content = trimLineBreaks(snippetEl.innerHTML);
      var lines = content.split('\n');
      // Calculate the minimum amount to remove on each line start of the snippet (can be 0)
      var pad = lines.reduce(function(acc, line) {
        if (line.length > 0 && trimLeft(line).length > 0 && acc > line.length - trimLeft(line).length) {
          return line.length - trimLeft(line).length;
        }
        return acc;
      }, Number.POSITIVE_INFINITY);
      // Slice each line with this amount
      return lines.map(function(line, index) {
        return line.slice(pad);
      })
        .join('\n');
    })(snippetEl);
  }

  var RevealHighlight = {

    HIGHLIGHT_STEP_DELIMITER: '|',
    HIGHLIGHT_LINE_DELIMITER: ',',
    HIGHLIGHT_LINE_RANGE_DELIMITER: '-',

    init: function( reveal ) {

      // Read the plugin config options and provide fallbacks
      var config = Reveal.getConfig().highlight || {};
      config.highlightOnLoad = typeof config.highlightOnLoad === 'boolean' ? config.highlightOnLoad : true;
      config.escapeHTML = typeof config.escapeHTML === 'boolean' ? config.escapeHTML : true;

      [].slice.call( reveal.getRevealElement().querySelectorAll( 'pre code' ) ).forEach( function( block ) {

        block.parentNode.className = 'code-wrapper';

        // Code can optionally be wrapped in script template to avoid
        // HTML being parsed by the browser (i.e. when you need to
        // include <, > or & in your code).
        let substitute = block.querySelector( 'script[type="text/template"]' );
        if( substitute ) {
          // textContent handles the HTML entity escapes for us
          block.textContent = substitute.innerHTML;
        }

        // Trim whitespace if the "data-trim" attribute is present
        if( block.hasAttribute( 'data-trim' ) && typeof block.innerHTML.trim === 'function' ) {
          block.innerHTML = betterTrim( block );
        }

        // Escape HTML tags unless the "data-noescape" attrbute is present
        if( config.escapeHTML && !block.hasAttribute( 'data-noescape' )) {
          block.innerHTML = block.innerHTML.replace( /</g,"&lt;").replace(/>/g, '&gt;' );
        }

        // Re-highlight when focus is lost (for contenteditable code)
        block.addEventListener( 'focusout', function( event ) {
          hljs.highlightElement( event.currentTarget );
        }, false );

        if( config.highlightOnLoad ) {
          RevealHighlight.highlightBlock( block );
        }
      } );

      // If we're printing to PDF, scroll the code highlights of
      // all blocks in the deck into view at once
      reveal.on( 'pdf-ready', function() {
        [].slice.call( reveal.getRevealElement().querySelectorAll( 'pre code[data-line-numbers].current-fragment' ) ).forEach( function( block ) {
          RevealHighlight.scrollHighlightedLineIntoView( block, {}, true );
        } );
      } );
    },

    /**
     * Highlights a code block. If the <code> node has the
     * 'data-line-numbers' attribute we also generate slide
     * numbers.
     *
     * If the block contains multiple line highlight steps,
     * we clone the block and create a fragment for each step.
     */
    highlightBlock: function( block ) {

      hljs.highlightElement( block );

      // Don't generate line numbers for empty code blocks
      if( block.innerHTML.trim().length === 0 ) return;

      if( block.hasAttribute( 'data-line-numbers' ) ) {
        hljs.lineNumbersBlock( block, { singleLine: true } );

        var scrollState = { currentBlock: block };

        // If there is at least one highlight step, generate
        // fragments
        var highlightSteps = RevealHighlight.deserializeHighlightSteps( block.getAttribute( 'data-line-numbers' ) );
        if( highlightSteps.length > 1 ) {

          // If the original code block has a fragment-index,
          // each clone should follow in an incremental sequence
          var fragmentIndex = parseInt( block.getAttribute( 'data-fragment-index' ), 10 );

          if( typeof fragmentIndex !== 'number' || isNaN( fragmentIndex ) ) {
            fragmentIndex = null;
          }

          // Generate fragments for all steps except the original block
          highlightSteps.slice(1).forEach( function( highlight ) {

            var fragmentBlock = block.cloneNode( true );
            fragmentBlock.setAttribute( 'data-line-numbers', RevealHighlight.serializeHighlightSteps( [ highlight ] ) );
            fragmentBlock.classList.add( 'fragment' );
            block.parentNode.appendChild( fragmentBlock );
            RevealHighlight.highlightLines( fragmentBlock );

            if( typeof fragmentIndex === 'number' ) {
              fragmentBlock.setAttribute( 'data-fragment-index', fragmentIndex );
              fragmentIndex += 1;
            }
            else {
              fragmentBlock.removeAttribute( 'data-fragment-index' );
            }

            // Scroll highlights into view as we step through them
            fragmentBlock.addEventListener( 'visible', RevealHighlight.scrollHighlightedLineIntoView.bind( Plugin, fragmentBlock, scrollState ) );
            fragmentBlock.addEventListener( 'hidden', RevealHighlight.scrollHighlightedLineIntoView.bind( Plugin, fragmentBlock.previousSibling, scrollState ) );

          } );

          block.removeAttribute( 'data-fragment-index' )
          block.setAttribute( 'data-line-numbers', RevealHighlight.serializeHighlightSteps( [ highlightSteps[0] ] ) );

        }

        // Scroll the first highlight into view when the slide
        // becomes visible. Note supported in IE11 since it lacks
        // support for Element.closest.
        var slide = typeof block.closest === 'function' ? block.closest( 'section:not(.stack)' ) : null;
        if( slide ) {
          var scrollFirstHighlightIntoView = function() {
            RevealHighlight.scrollHighlightedLineIntoView( block, scrollState, true );
            slide.removeEventListener( 'visible', scrollFirstHighlightIntoView );
          }
          slide.addEventListener( 'visible', scrollFirstHighlightIntoView );
        }

        RevealHighlight.highlightLines( block );

      }

    },

    /**
     * Animates scrolling to the first highlighted line
     * in the given code block.
     */
    scrollHighlightedLineIntoView: function( block, scrollState, skipAnimation ) {

      cancelAnimationFrame( scrollState.animationFrameID );

      // Match the scroll position of the currently visible
      // code block
      if( scrollState.currentBlock ) {
        block.scrollTop = scrollState.currentBlock.scrollTop;
      }

      // Remember the current code block so that we can match
      // its scroll position when showing/hiding fragments
      scrollState.currentBlock = block;

      var highlightBounds = RevealHighlight.getHighlightedLineBounds( block )
      var viewportHeight = block.offsetHeight;

      // Subtract padding from the viewport height
      var blockStyles = getComputedStyle( block );
      viewportHeight -= parseInt( blockStyles.paddingTop ) + parseInt( blockStyles.paddingBottom );

      // Scroll position which centers all highlights
      var startTop = block.scrollTop;
      var targetTop = highlightBounds.top + ( Math.min( highlightBounds.bottom - highlightBounds.top, viewportHeight ) - viewportHeight ) / 2;

      // Account for offsets in position applied to the
      // <table> that holds our lines of code
      var lineTable = block.querySelector( '.hljs-ln' );
      if( lineTable ) targetTop += lineTable.offsetTop - parseInt( blockStyles.paddingTop );

      // Make sure the scroll target is within bounds
      targetTop = Math.max( Math.min( targetTop, block.scrollHeight - viewportHeight ), 0 );

      if( skipAnimation === true || startTop === targetTop ) {
        block.scrollTop = targetTop;
      }
      else {

        // Don't attempt to scroll if there is no overflow
        if( block.scrollHeight <= viewportHeight ) return;

        var time = 0;
        var animate = function() {
          time = Math.min( time + 0.02, 1 );

          // Update our eased scroll position
          block.scrollTop = startTop + ( targetTop - startTop ) * RevealHighlight.easeInOutQuart( time );

          // Keep animating unless we've reached the end
          if( time < 1 ) {
            scrollState.animationFrameID = requestAnimationFrame( animate );
          }
        };

        animate();

      }

    },

    /**
     * The easing function used when scrolling.
     */
    easeInOutQuart: function( t ) {

      // easeInOutQuart
      return t<.5 ? 8*t*t*t*t : 1-8*(--t)*t*t*t;

    },

    getHighlightedLineBounds: function( block ) {

      var highlightedLines = block.querySelectorAll( '.highlight-line' );
      if( highlightedLines.length === 0 ) {
        return { top: 0, bottom: 0 };
      }
      else {
        var firstHighlight = highlightedLines[0];
        var lastHighlight = highlightedLines[ highlightedLines.length -1 ];

        return {
          top: firstHighlight.offsetTop,
          bottom: lastHighlight.offsetTop + lastHighlight.offsetHeight
        }
      }

    },

    /**
     * Visually emphasize specific lines within a code block.
     * This only works on blocks with line numbering turned on.
     *
     * @param {HTMLElement} block a <code> block
     * @param {String} [linesToHighlight] The lines that should be
     * highlighted in this format:
     * "1" 		= highlights line 1
     * "2,5"	= highlights lines 2 & 5
     * "2,5-7"	= highlights lines 2, 5, 6 & 7
     */
    highlightLines: function( block, linesToHighlight ) {

      var highlightSteps = RevealHighlight.deserializeHighlightSteps( linesToHighlight || block.getAttribute( 'data-line-numbers' ) );

      if( highlightSteps.length ) {

        highlightSteps[0].forEach( function( highlight ) {

          var elementsToHighlight = [];

          // Highlight a range
          if( typeof highlight.end === 'number' ) {
            elementsToHighlight = [].slice.call( block.querySelectorAll( 'table tr:nth-child(n+'+highlight.start+'):nth-child(-n+'+highlight.end+')' ) );
          }
          // Highlight a single line
          else if( typeof highlight.start === 'number' ) {
            elementsToHighlight = [].slice.call( block.querySelectorAll( 'table tr:nth-child('+highlight.start+')' ) );
          }

          if( elementsToHighlight.length ) {
            elementsToHighlight.forEach( function( lineElement ) {
              lineElement.classList.add( 'highlight-line' );
            } );

            block.classList.add( 'has-highlights' );
          }

        } );

      }

    },

    /**
     * Parses and formats a user-defined string of line
     * numbers to highlight.
     *
     * @example
     * RevealHighlight.deserializeHighlightSteps( '1,2|3,5-10' )
     * // [
     * //   [ { start: 1 }, { start: 2 } ],
     * //   [ { start: 3 }, { start: 5, end: 10 } ]
     * // ]
     */
    deserializeHighlightSteps: function( highlightSteps ) {

      // Remove whitespace
      highlightSteps = highlightSteps.replace( /\s/g, '' );

      // Divide up our line number groups
      highlightSteps = highlightSteps.split( RevealHighlight.HIGHLIGHT_STEP_DELIMITER );

      return highlightSteps.map( function( highlights ) {

        return highlights.split( RevealHighlight.HIGHLIGHT_LINE_DELIMITER ).map( function( highlight ) {

          // Parse valid line numbers
          if( /^[\d-]+$/.test( highlight ) ) {

            highlight = highlight.split( RevealHighlight.HIGHLIGHT_LINE_RANGE_DELIMITER );

            var lineStart = parseInt( highlight[0], 10 ),
              lineEnd = parseInt( highlight[1], 10 );

            if( isNaN( lineEnd ) ) {
              return {
                start: lineStart
              };
            }
            else {
              return {
                start: lineStart,
                end: lineEnd
              };
            }

          }
          // If no line numbers are provided, no code will be highlighted
          else {

            return {};

          }

        } );

      } );

    },

    /**
     * Serializes parsed line number data into a string so
     * that we can store it in the DOM.
     */
    serializeHighlightSteps: function( highlightSteps ) {

      return highlightSteps.map( function( highlights ) {

        return highlights.map( function( highlight ) {

          // Line range
          if( typeof highlight.end === 'number' ) {
            return highlight.start + RevealHighlight.HIGHLIGHT_LINE_RANGE_DELIMITER + highlight.end;
          }
          // Single line
          else if( typeof highlight.start === 'number' ) {
            return highlight.start;
          }
          // All lines
          else {
            return '';
          }

        } ).join( RevealHighlight.HIGHLIGHT_LINE_DELIMITER );

      } ).join( RevealHighlight.HIGHLIGHT_STEP_DELIMITER );

    }

  }

  Reveal.registerPlugin( 'highlight', RevealHighlight );

  return RevealHighlight;

}));
        
hljs.configure({
  ignoreUnescapedHTML: true,
});
hljs.highlightAll();
</script></body></html>