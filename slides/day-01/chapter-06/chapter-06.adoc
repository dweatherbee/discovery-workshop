== Ranges, Replicas, and Leaseholders
=== Why Data Distribution Matters
[.text-left]

* Enables horizontal scalability
* Ensures data availability
* Supports fault tolerance
* Optimizes read/write performance
* Facilitates cluster management

[.notes]
--
CockroachDB leverages a *range-based architecture* to distribute data evenly across nodes. By splitting large datasets into smaller “ranges,” the cluster can horizontally scale out to add capacity and handle increasing workloads. This design:

* **Improves Availability:** Replicated ranges reduce the impact of node failures.
* **Boosts Performance:** Spreading ranges across nodes balances read/write load.
* **Aids Maintenance:** Node additions or removals automatically trigger rebalancing to preserve performance and availability.
--

=== Understanding Ranges
[.text-left]

* Contiguous chunks of table data
* Unique range IDs for identification
* Automatic range splitting
* Default replication factor of 3
* Dynamic redistribution capability

[.notes]
--
In CockroachDB, a *range* is a contiguous chunk of data within a table or index. Each range:

* Has a **unique ID** for identification (found via `SHOW RANGES`).
* Splits automatically as data grows beyond a threshold (by default, 64 MiB).
* Maintains a **default replication factor** of 3 (or more, depending on zone configuration).

You can inspect range details with:
[source,sql]
----
SHOW RANGES FROM TABLE bank.accounts WITH DETAILS;
----

This command reveals information like start/end keys, replicas, and leaseholder locations. As data volume changes or new nodes join, CockroachDB *dynamically* splits and redistributes ranges to balance load.
--

=== Replica Distribution
[.text-left]

* Multiple copies of each range
* Even distribution across nodes
* Automatic rebalancing
* Configurable replication factor
* Location-aware placement

[.notes]
--
CockroachDB replicates each range across multiple nodes to ensure fault tolerance. Typically:

* **3 replicas** for user data (default).
* Even distribution ensures no node holds multiple replicas of the same range.
* Automatic rebalancing moves replicas if nodes are added/removed or become overloaded.
* Locality tags (e.g., `--locality=region=us-east`) can direct replicas to specific locations for *low-latency reads* or *regulatory compliance*.

To modify replication settings, use **zone configurations**:
[source,sql]
----
ALTER RANGE default CONFIGURE ZONE USING num_replicas = 5;
----
This increases replication factor, enhancing fault tolerance but potentially affecting write performance due to quorum requirements.
--

=== Leaseholders
[.text-left]

* Single leaseholder per range
* Coordinates read/write operations
* Automatic leader election
* Performance optimization
* Dynamic reassignment

[.notes]
--
A *leaseholder* is the replica handling all read requests (and coordinating writes) for its range:

* **Single Leader Model:** Only one replica (the leaseholder) orchestrates reads/writes.  
* **Automatic Election:** If a node fails, CockroachDB quickly assigns the leaseholder role to another replica.  
* **Load Balancing:** Leaseholders distribute themselves based on usage patterns and node capacity.  
* **Performance Benefit:** By serving strongly consistent reads from the leaseholder, CockroachDB reduces cross-replica coordination for reads.

Check current leaseholders with:
[source,sql]
----
SHOW RANGES FROM TABLE bank.accounts WITH DETAILS;
----
Look for the `LEASE HOLDER` column to see which node is serving each range.
--

=== Dynamic Distribution
[.text-left]

* Automatic failure detection
* Range redistribution
* Leaseholder rebalancing
* Cluster size adaptation
* Locality-based placement

[.notes]
--
CockroachDB continuously monitors node health and adapts to changes:

* **Automatic Failure Detection:** Uses heartbeat timeouts; if a node is unresponsive, the cluster marks it suspect.  
* **Range Redistribution:** If a node is lost or added, replicas are automatically moved to maintain the desired replication factor.  
* **Leaseholder Rebalancing:** Leaseholders shift to active and healthy nodes to preserve performance.  
* **Locality-Aware Placement:** Takes region/zone constraints into account, placing data near users or within compliance boundaries.  
* **Scalable Growth:** As you add nodes, CockroachDB rebalances ranges to utilize the new capacity without manual intervention.
--

=== Exercise Overview
[.text-left]

* Create sample data
* Inspect range distribution
* Simulate node failures
* Observe automatic healing
* Scale cluster size

[.notes]
--
In this exercise, you will:

* **Create a sample schema** (e.g., a `bank.accounts` table) and populate it with data.  
* **Inspect range distribution** via `SHOW RANGES` to learn which nodes hold replicas and leaseholders.  
* **Simulate node failures** (e.g., by stopping a node) and observe how CockroachDB reassigns leaseholders and re-replicates data to maintain redundancy.  
* **Explore scaling** by adding or removing nodes, then check how ranges rebalance to accommodate the cluster size.  
* **Monitor cluster performance** and confirm that read/write operations remain continuous and consistent throughout.  

This hands-on session will deepen your understanding of CockroachDB’s data distribution model and highlight how ranges, replicas, and leaseholders work together to deliver fault tolerance and scalability.
--