== Node Failure and Recovery in CockroachDB
=== Why Business Continuity Matters
[.text-left]

* Prevents service interruptions
* Maintains data consistency
* Preserves customer experience
* Supports SLA compliance
* Reduces operational risk

[.notes]
--
Maintaining *business continuity* ensures CockroachDB can serve client requests
without interruption, even if an entire node (or multiple nodes) fails. By
automatically detecting and handling node failures, CockroachDB keeps data
consistent and operations transparent to the end user. This reliability is vital
for meeting *SLAs* and sustaining a high-quality user experience. Organizations
can perform planned maintenance or deal with unexpected hardware/network issues
without incurring downtime or risking data integrity.
--

=== Node Health States
[.text-left]

* LIVE - Node functioning normally
* SUSPECT - Node unresponsive but not dead
* DEAD - Node offline beyond threshold
* Configurable detection timing
* Automatic state transitions

[.notes]
--
CockroachDB assigns health states to each node:

* *LIVE:* The node is actively responding to heartbeats.
* *SUSPECT:* The node has missed heartbeats but hasn’t surpassed the *time_until_store_dead* threshold.
* *DEAD:* The node has exceeded the threshold, at which point the cluster automatically re-replicates data as needed.

This threshold defaults to 5 minutes:

[source,sql]
----
SET CLUSTER SETTING server.time_until_store_dead = '5m';
----
Adjusting this value can tailor failover sensitivity to your environment. Too short can cause unnecessary failovers, while too long could delay recovery from real failures.
--

=== Workload Continuity
[.text-left]

* Automatic request redistribution
* No manual intervention required
* Minimal performance impact
* Continuous monitoring available
* Real-time health tracking

[.notes]
--
When a node becomes unavailable:

* *Request Redistribution:* Ongoing requests automatically route to other available nodes without manual load balancing.
* *Minimal Impact:* Transactions in progress may retry internally. CockroachDB’s consensus mechanism ensures data consistency with minimal performance reduction.
* *Continuous Monitoring:* The DB Console displays real-time health statuses, letting you confirm that queries are still being served.
* *No Manual Action:* Administrators don’t need to intervene, as CockroachDB handles re-leasing and data re-replication automatically.

Use the *Statements* and *Metrics* pages in DB Console to track throughput and confirm that queries remain uninterrupted.
--

=== Recovery Process
[.text-left]

* Automatic node reintegration
* Data synchronization
* Workload rebalancing
* State restoration
* Seamless client operation

[.notes]
--
When a node rejoins the cluster, CockroachDB:

. *Authenticates and Reintegration:* Validates that the node is still part of the cluster.  
. *Synchronizes Data:* Fetches any missed updates to ensure the node’s replicas are current.  
. *Rebalances Load:* Reassigns leaseholders or replicas if needed so the revived node can resume its share of the workload.  
. *Maintains Client Transparency:* Transactions continue without requiring client-side reconfiguration.

A typical restart command might look like:

[source,bash]
----
cockroach start \
  --locality=host=instruqtVM,node=nodeX \
  --advertise-addr=nodeX:26257 \
  --join=node1:26257,node2:26257
----
After the node is recognized as LIVE, it gradually reacquires replicas as part of CockroachDB’s automatic healing.
--

=== Monitoring and Management
[.text-left]

* DB Console visualization
* Real-time metrics tracking
* SQL statement monitoring
* Node status tracking
* Performance impact analysis

[.notes]
--
The DB Console offers extensive visibility into cluster state:

* *Overview Page:* Highlights node statuses (LIVE, SUSPECT, DEAD) at a glance.
* *Metrics Charts:* Show QPS, latency, memory usage, and more to spot anomalies during failures.
* *SQL Monitoring:* The *Statements* page shows ongoing queries and their performance metrics.  
* *Node Status Tab:* Offers a detailed look at each node’s health, replicas held, and store usage.

Administrators can use these dashboards to quickly identify issues, gauge recovery progress, and measure performance impacts throughout node failure and recovery events.
--

=== Exercise Overview
[.text-left]

* Run continuous workload
* Monitor cluster health
* Simulate node failure
* Observe automatic handling
* Test node recovery

[.notes]
--
In the exercise, you will:

* *Start a workload* (e.g., `cockroach workload`) so there’s continuous traffic to the cluster.
* *Monitor the DB Console* focusing on node status and transaction metrics.
* *Simulate a node failure* by stopping or killing a node process and watch how CockroachDB handles the event.
* *Recover the failed node* and observe how data is re-synchronized.
* *Confirm continuity* by verifying that queries remain consistent and successful, and that metrics reflect normal performance after recovery.

This experience illustrates CockroachDB’s fault-tolerance in real-world scenarios, helping you understand how to maintain continuous operations even under adverse conditions.
--